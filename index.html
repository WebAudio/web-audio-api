<!DOCTYPE html>
<html>
  <head>
    <title>
      Web Audio API
    </title>
    <meta charset="utf-8">
    <script src='https://www.w3.org/Tools/respec/respec-w3c-common' async
    class='remove'>
    </script>
    <script src=
    "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script class='remove'>
    var respecConfig = {
        specStatus: "ED",
        shortName:  "webaudio",
        edDraftURI: "https://webaudio.github.io/web-audio-api/",
        editors: [
              {   name:       "Paul Adenot",
                  company:    "Mozilla",
                  companyURL: "https://www.mozilla.org/",
                  mailto:     "padenot@mozilla.com" },
              {
                  name:       "Chris Wilson",
                  company:    "Google, Inc.",
                  companyURL: "https://www.google.com/",
                  mailto:     "cwilso@google.com" },
        ],
        previousMaturity: "WD",
        previousPublishDate:  "2012-12-13",
        previousURI:  "http://www.w3.org/TR/2012/WD-webaudio-20121213/",
        wg:           "Audio Working Group",
        wgURI:        "http://www.w3.org/2011/audio/",
        wgPublicList: "public-audio",
        wgPatentURI:  "http://www.w3.org/2004/01/pp-impl/46884/status",
        tocIntroductory: true,
        copyrightStart: 2013,
        otherLinks: [
          {
            key: "Previous editor",
            data : [{value: "Chris Rogers (Until August 2013)"}] },
          {
            key: "Repository",
            href: "https://github.com/WebAudio/web-audio-api" },
          {
            key: "Bug tracker",
            href: "https://github.com/WebAudio/web-audio-api/issues?state=open" },
        ]
    };
    </script>
    <script>
    function findBadLink () {
      var old = document.querySelectorAll(".badLink");
      for (var i = 0 ; i < old.length; i++) {
        nodes[i].style.backgroundColor = "";
        nodes[i].classList.remove("badLink");
      }
      var nodes = document.querySelectorAll('a');

      for (var i = 0; i < nodes.length; i++) {
          if (nodes[i].href) {
              var id =  nodes[i].href.split("/");
              id = id[id.length - 1];
              if (id.length != 0) {
                  if (id[0] == "#") {
                      if (document.querySelectorAll(id).length == 0) {
                        nodes[i].style.backgroundColor = "red";
                        nodes[i].classList.add("badLink");
                        console.log(nodes[i].textContent);
                    }
                 }
              }
          } else {
            nodes[i].style.backgroundColor = "red";
            nodes[i].classList.add("badLink");
          }
      }
    }

    function findMissingLink() {
      var codetags = document.querySelectorAll("code");

      for (var i = 0; i < codetags.length; i++) {
        if (!(codetags[i].parentNode instanceof HTMLAnchorElement) ||
            codetags[i].parentNode.href == "") {
          codetags[i].style.backgroundColor = 'hotpink';
          codetags[i].style.color  = 'yellow';

          console.log(codetags[i].innerHTML);
        }
      }
    }

    // This allow wrapping mathjax formulas in <pre> so the LaTeX code is not
    // auto wrapped and keeps a nice formatting.
    MathJax.Hub.Config({
      tex2jax: {
         skipTags: ["script","noscript","style","textarea","code"]
      }
    });
    </script>
    <style>
    .todo {
      border: 1px solid red;
      background-color: rgba(255, 0, 0, 0.3);
    }
    .todo:before {
      content: "TODO:";
    }
    .synchronous:before {
      content: "⌛ ";
    }
    </style>
  </head>
  <body>
    <section id="abstract">
      <p>
        This specification describes a high-level JavaScript <abbr title=
        "Application Programming Interface">API</abbr> for processing and
        synthesizing audio in web applications. The primary paradigm is of an
        audio routing graph, where a number of <a><code>AudioNode</code></a>
        objects are connected together to define the overall audio rendering.
        The actual processing will primarily take place in the underlying
        implementation (typically optimized Assembly / C / C++ code), but
        <a href="#the-audioworker-interface">direct JavaScript processing and
        synthesis</a> is also supported.
      </p>
      <p>
        The <a href="#introduction">introductory</a> section covers the
        motivation behind this specification.
      </p>
      <p>
        This API is designed to be used in conjunction with other APIs and
        elements on the web platform, notably: XMLHttpRequest [[XHR]] (using
        the <code>responseType</code> and <code>response</code> attributes).
        For games and interactive applications, it is anticipated to be used
        with the <code>canvas</code> 2D [[2dcontext]] and WebGL [[WEBGL]] 3D
        graphics APIs.
      </p>
    </section>
    <section id="sotd"></section>
    <section class="introductory">
      <h2>
        Introduction
      </h2>
      <section>
        <p>
          Audio on the web has been fairly primitive up to this point and until
          very recently has had to be delivered through plugins such as Flash
          and QuickTime. The introduction of the <code>audio</code> element in
          HTML5 is very important, allowing for basic streaming audio playback.
          But, it is not powerful enough to handle more complex audio
          applications. For sophisticated web-based games or interactive
          applications, another solution is required. It is a goal of this
          specification to include the capabilities found in modern game audio
          engines as well as some of the mixing, processing, and filtering
          tasks that are found in modern desktop audio production applications.
        </p>
        <p>
          The APIs have been designed with a wide variety of use cases
          [[webaudio-usecases]] in mind. Ideally, it should be able to support
          <i>any</i> use case which could reasonably be implemented with an
          optimized C++ engine controlled via JavaScript and run in a browser.
          That said, modern desktop audio software can have very advanced
          capabilities, some of which would be difficult or impossible to build
          with this system. Apple's Logic Audio is one such application which
          has support for external MIDI controllers, arbitrary plugin audio
          effects and synthesizers, highly optimized direct-to-disk audio file
          reading/writing, tightly integrated time-stretching, and so on.
          Nevertheless, the proposed system will be quite capable of supporting
          a large range of reasonably complex games and interactive
          applications, including musical ones. And it can be a very good
          complement to the more advanced graphics features offered by WebGL.
          The API has been designed so that more advanced capabilities can be
          added at a later time.
        </p>
      </section>
      <section>
        <h2 id="Features">
          Features
        </h2>
        <p>
          The API supports these primary features:
        </p>
        <ul>
          <li>
            <a href="#ModularRouting">Modular routing</a> for simple or complex
            mixing/effect architectures, including <a href=
            "#mixer-gain-structure">multiple sends and submixes</a>.
          </li>
          <li>High dynamic range, using 32bits floats for internal processing.
          </li>
          <li>
            <a href="#AudioParam">Sample-accurate scheduled sound playback</a>
            with low <a href="#latency">latency</a> for musical applications
            requiring a very high degree of rhythmic precision such as drum
            machines and sequencers. This also includes the possibility of
            <a href="#DynamicLifetime">dynamic creation</a> of effects.
          </li>
          <li>Automation of audio parameters for envelopes, fade-ins /
          fade-outs, granular effects, filter sweeps, LFOs etc.
          </li>
          <li>Flexible handling of channels in an audio stream, allowing them
          to be split and merged.
          </li>
          <li>Processing of audio sources from an <code>audio</code> or <code>
            video</code> <a href="#MediaElementAudioSourceNode">media
            element</a>.
          </li>
          <li>Processing live audio input using a <a href=
          "#MediaStreamAudioSourceNode">MediaStream</a> from getUserMedia().
          </li>
          <li>Integration with WebRTC
            <ul>
              <li>Processing audio received from a remote peer using a
              <a><code>MediaStreamAudioSourceNode</code></a> and [[!webrtc]].
              </li>
              <li>Sending a generated or processed audio stream to a remote
              peer using a <a><code>MediaStreamAudioDestinationNode</code></a>
              and [[!webrtc]].
              </li>
            </ul>
          </li>
          <li>Audio stream synthesis and processing <a href=
          "#the-audioworker-interface">directly in JavaScript</a>.
          </li>
          <li>
            <a href="#Spatialization">Spatialized audio</a> supporting a wide
            range of 3D games and immersive environments:
            <ul>
              <li>Panning models: equalpower, HRTF, pass-through
              </li>
              <li>Distance Attenuation
              </li>
              <li>Sound Cones
              </li>
              <li>Obstruction / Occlusion
              </li>
              <li>Doppler Shift
              </li>
              <li>Source / Listener based
              </li>
            </ul>
          </li>
          <li>A <a href="#Convolution">convolution engine</a> for a wide range
          of linear effects, especially very high-quality room effects. Here
          are some examples of possible effects:
            <ul>
              <li>Small / large room
              </li>
              <li>Cathedral
              </li>
              <li>Concert hall
              </li>
              <li>Cave
              </li>
              <li>Tunnel
              </li>
              <li>Hallway
              </li>
              <li>Forest
              </li>
              <li>Amphitheater
              </li>
              <li>Sound of a distant room through a doorway
              </li>
              <li>Extreme filters
              </li>
              <li>Strange backwards effects
              </li>
              <li>Extreme comb filter effects
              </li>
            </ul>
          </li>
          <li>Dynamics compression for overall control and sweetening of the
          mix
          </li>
          <li>Efficient <a href="#the-analysernode-interface">real-time
          time-domain and frequency analysis / music visualizer support</a>
          </li>
          <li>Efficient biquad filters for lowpass, highpass, and other common
          filters.
          </li>
          <li>A Waveshaping effect for distortion and other non-linear effects
          </li>
          <li>Oscillators
          </li>
        </ul>
        <section>
          <h2 id="ModularRouting">
            Modular Routing
          </h2>
          <p>
            Modular routing allows arbitrary connections between different
            <a><code>AudioNode</code></a> objects. Each node can have
            <dfn>inputs</dfn> and/or <dfn>outputs</dfn>. A <dfn>source
            node</dfn> has no inputs and a single output. A <dfn>destination
            node</dfn> has one input and no outputs, the most common example
            being <a href=
            "#AudioDestinationNode"><code>AudioDestinationNode</code></a> the
            final destination to the audio hardware. Other nodes such as
            filters can be placed between the source and destination nodes. The
            developer doesn't have to worry about low-level stream format
            details when two objects are connected together; <a href=
            "#channel-up-mixing-and-down-mixing">the right thing just
            happens</a>. For example, if a mono audio stream is connected to a
            stereo input it should just mix to left and right channels <a href=
            "#channel-up-mixing-and-down-mixing">appropriately</a>.
          </p>
          <p>
            In the simplest case, a single source can be routed directly to the
            output. All routing occurs within an <a href=
            "#AudioContext"><code>AudioContext</code></a> containing a single
            <a href=
            "#AudioDestinationNode"><code>AudioDestinationNode</code></a>:
          </p>
          <figure>
            <img alt="modular routing" src="images/modular-routing1.png">
            <figcaption>
              A simple example of modular routing.
            </figcaption>
          </figure>
          <p>
            Illustrating this simple routing, here's a simple example playing a
            single sound:
          </p>
          <pre class="highlight example">

var context = new AudioContext();

function playSound() {
    var source = context.createBufferSource();
    source.buffer = dogBarkingBuffer;
    source.connect(context.destination);
    source.start(0);
}
</pre>
          <p>
            Here's a more complex example with three sources and a convolution
            reverb send with a dynamics compressor at the final output stage:
          </p>
          <figure>
            <img alt="modular routing2" src="images/modular-routing2.png">
            <figcaption>
              A more complex example of modular routing.
            </figcaption>
          </figure>
          <pre class="highlight example">

var context = 0;
var compressor = 0;
var reverb = 0;

var source1 = 0;
var source2 = 0;
var source3 = 0;

var lowpassFilter = 0;
var waveShaper = 0;
var panner = 0;

var dry1 = 0;
var dry2 = 0;
var dry3 = 0;

var wet1 = 0;
var wet2 = 0;
var wet3 = 0;

var masterDry = 0;
var masterWet = 0;

function setupRoutingGraph () {
    context = new AudioContext();

    // Create the effects nodes.
    lowpassFilter = context.createBiquadFilter();
    waveShaper = context.createWaveShaper();
    panner = context.createPanner();
    compressor = context.createDynamicsCompressor();
    reverb = context.createConvolver();

    // Create master wet and dry.
    masterDry = context.createGain();
    masterWet = context.createGain();

    // Connect final compressor to final destination.
    compressor.connect(context.destination);

    // Connect master dry and wet to compressor.
    masterDry.connect(compressor);
    masterWet.connect(compressor);

    // Connect reverb to master wet.
    reverb.connect(masterWet);

    // Create a few sources.
    source1 = context.createBufferSource();
    source2 = context.createBufferSource();
    source3 = context.createOscillator();

    source1.buffer = manTalkingBuffer;
    source2.buffer = footstepsBuffer;
    source3.frequency.value = 440;

    // Connect source1
    dry1 = context.createGain();
    wet1 = context.createGain();
    source1.connect(lowpassFilter);
    lowpassFilter.connect(dry1);
    lowpassFilter.connect(wet1);
    dry1.connect(masterDry);
    wet1.connect(reverb);

    // Connect source2
    dry2 = context.createGain();
    wet2 = context.createGain();
    source2.connect(waveShaper);
    waveShaper.connect(dry2);
    waveShaper.connect(wet2);
    dry2.connect(masterDry);
    wet2.connect(reverb);

    // Connect source3
    dry3 = context.createGain();
    wet3 = context.createGain();
    source3.connect(panner);
    panner.connect(dry3);
    panner.connect(wet3);
    dry3.connect(masterDry);
    wet3.connect(reverb);

    // Start the sources now.
    source1.start(0);
    source2.start(0);
    source3.start(0);
}
</pre>
          <p>
            Modular routing also permits the output of
            <a><code>AudioNode</code></a>s to be routed to an
            <a><code>AudioParam</code></a> parameter that controls the behavior
            of a different <a><code>AudioNode</code></a>. In this scenario, the
            output of a node can act as a modulation signal rather than an
            input signal.
          </p>
          <figure>
            <img alt="modular routing3" src="images/modular-routing3.png">
            <figcaption>
              Modular routing illustrating one Oscillator modulating the
              frequency of another.
            </figcaption>
          </figure>
          <pre class="highlight example">
function setupRoutingGraph() {
  var context = new AudioContext();

  // Create the low frequency oscillator that supplies the modulation signal
  var lfo = context.createOscillator();
  lfo.frequency.value = 1.0;

  // Create the high frequency oscillator to be modulated
  var hfo = context.createOscillator();
  hfo.frequency.value = 440.0;

  // Create a gain node whose gain determines the amplitude of the modulation signal
  var modulationGain = context.createGain();
  modulationGain.gain.value = 50;

  // Configure the graph and start the oscillators
  lfo.connect(modulationGain);
  modulationGain.connect(hfo.detune);
  hfo.connect(context.destination);
  hfo.start(0);
  lfo.start(0);
}
</pre>
        </section>
      </section>
      <section>
        <h2 id="APIOverview">
          API Overview
        </h2>
        <p>
          The interfaces defined are:
        </p>
        <ul>
          <li>An <a class="dfnref" href="#AudioContext">AudioContext</a>
          interface, which contains an audio signal graph representing
          connections betweens <a><code>AudioNode</code></a>s.
          </li>
          <li>An <a><code>AudioNode</code></a> interface, which represents
          audio sources, audio outputs, and intermediate processing modules.
          <a><code>AudioNode</code></a>s can be dynamically connected together
          in a <a href="#ModularRouting">modular fashion</a>.
          <a><code>AudioNode</code></a>s exist in the context of an
          <a><code>AudioContext</code></a>
          </li>
          <li>An <a><code>AudioDestinationNode</code></a> interface, an
          <a><code>AudioNode</code></a> subclass representing the final
          destination for all rendered audio.
          </li>
          <li>An <a><code>AudioBuffer</code></a> interface, for working with
          memory-resident audio assets. These can represent one-shot sounds, or
          longer audio clips.
          </li>
          <li>An <a><code>AudioBufferSourceNode</code></a> interface, an
          <a><code>AudioNode</code></a> which generates audio from an
          AudioBuffer.
          </li>
          <li>A <a><code>MediaElementAudioSourceNode</code></a> interface, an
          <a><code>AudioNode</code></a> which is the audio source from an
          <code>audio</code>, <code>video</code>, or other media element.
          </li>
          <li>A <a><code>MediaStreamAudioSourceNode</code></a> interface, an
          <a><code>AudioNode</code></a> which is the audio source from a
          MediaStream such as live audio input, or from a remote peer.
          </li>
          <li>A <a><code>MediaStreamAudioDestinationNode</code></a> interface,
          an <a><code>AudioNode</code></a> which is the audio destination to a
          MediaStream sent to a remote peer.
          </li>
          <li>An <a><code>AudioWorker</code></a> interface representing a
          factory for creating <code>WebWorker</code>-based custom nodes that
          can process audio directly in JavaScript.
          </li>
          <li>An <a><code>AudioWorkerNode</code></a> interface, an
          <a><code>AudioNode</code></a> representing a node processed in an
          AudioWorker.
          </li>
          <li>An <a><code>AudioWorkerGlobalScope</code></a> interface, the
          context in which AudioWorker processing scripts run.
          </li>
          <li>An <a><code>AudioWorkerNodeProcessor</code></a> interface,
          representing a single node instance inside an audio worker.
          </li>
          <li>An <a><code>AudioParam</code></a> interface, for controlling an
          individual aspect of an <a><code>AudioNode</code></a>'s functioning,
          such as volume.
          </li>
          <li>An <a><code>GainNode</code></a> interface, an
          <a><code>AudioNode</code></a> for explicit gain control. Because
          inputs to <a><code>AudioNode</code></a>s support multiple connections
          (as a unity-gain summing junction), mixers can be <a href=
          "#mixer-gain-structure">easily built</a> with GainNodes.
          </li>
          <li>A <a><code>BiquadFilterNode</code></a> interface, an
          <a><code>AudioNode</code></a> for common low-order filters such as:
            <ul>
              <li>Low Pass
              </li>
              <li>High Pass
              </li>
              <li>Band Pass
              </li>
              <li>Low Shelf
              </li>
              <li>High Shelf
              </li>
              <li>Peaking
              </li>
              <li>Notch
              </li>
              <li>Allpass
              </li>
            </ul>
          </li>
          <li>A <a><code>IIRFilterNode</code></a> interface, an
          <a><code>AudioNode</code></a> for a general IIR filter.
          </li>
          <li>A <a><code>DelayNode</code></a> interface, an
          <a><code>AudioNode</code></a> which applies a dynamically adjustable
          variable delay.
          </li>
          <li>A <a><code>SpatialPannerNode</code></a> interface, an
          <a><code>AudioNode</code></a> for positioning audio in 3D space.
          </li>
          <li>A <a><code>SpatialListener</code></a> interface, which works with
          a <a>SpatialPannerNode</a> for spatialization.
          </li>
          <li>A <a><code>StereoPannerNode</code></a> interface, an
          <a><code>AudioNode</code></a> for equal-power positioning of audio
          input in a stereo stream.
          </li>
          <li>A <a><code>ConvolverNode</code></a> interface, an
          <a><code>AudioNode</code></a> for applying a <a href="#Convolution">
            real-time linear effect</a> (such as the sound of a concert hall).
          </li>
          <li>A <a><code>AnalyserNode</code></a> interface, an
          <a><code>AudioNode</code></a> for use with music visualizers, or
          other visualization applications.
          </li>
          <li>A <a><code>ChannelSplitterNode</code></a> interface, an <a><code>
            AudioNode</code></a> for accessing the individual channels of an
            audio stream in the routing graph.
          </li>
          <li>A <a><code>ChannelMergerNode</code></a> interface, an
          <a><code>AudioNode</code></a> for combining channels from multiple
          audio streams into a single audio stream.
          </li>
          <li>A <a><code>DynamicsCompressorNode</code></a> interface, an
          <a><code>AudioNode</code></a> for dynamics compression.
          </li>
          <li>A <a><code>WaveShaperNode</code></a> interface, an
          <a><code>AudioNode</code></a> which applies a non-linear waveshaping
          effect for distortion and other more subtle warming effects.
          </li>
          <li>A <a><code>OscillatorNode</code></a> interface, an
          <a><code>AudioNode</code></a> for generating a periodic waveform.
          </li>
        </ul>
        <p>
          There are also several features that have been deprecated from the
          Web Audio API but not yet removed, pending implementation experience
          of their replacements:
        </p>
        <ul>
          <li>A <a><code>PannerNode</code></a> interface, an
          <a><code>AudioNode</code></a> for spatializing / positioning audio in
          3D space. This has been replaced by
          <a><code>SpatialPannerNode</code></a>, and
          <a><code>StereoPanner</code></a> for simpler scenarios.
          </li>
          <li>An <a><code>AudioListener</code></a> interface, which works with
          a <a>PannerNode</a> for spatialization.
          </li>
          <li>A <a><code>ScriptProcessorNode</code></a> interface, an <a><code>
            AudioNode</code></a> for generating or processing audio directly in
            JavaScript.
          </li>
          <li>An <a><code>AudioProcessingEvent</code></a> interface, which is
          an event type used with <a><code>ScriptProcessorNode</code></a>
          objects.
          </li>
        </ul>
      </section>
    </section>
    <section id="conformance">
      <p>
        The following conformance classes are defined by this specification:
      </p>
      <dl>
        <dt>
          <dfn id="dfn-conforming-implementation">conforming
          implementation</dfn>
        </dt>
        <dd>
          <p>
            A user agent is considered to be a <a class="dfnref" href=
            "#dfn-conforming-implementation">conforming implementation</a> if
            it satisfies all of the MUST-, REQUIRED- and SHALL-level criteria
            in this specification that apply to implementations.
          </p>
        </dd>
      </dl>
      <p>
        User agents that use ECMAScript to implement the APIs defined in this
        specification must implement them in a manner consistent with the
        ECMAScript Bindings defined in the Web IDL specification [[!WEBIDL]] as
        this specification uses that specification and terminology.
      </p>
    </section>
    <section id="audioapi">
      <h2 id="API">
        The Audio API
      </h2>
      <section>
        <h2 id="AudioContext">
          The AudioContext Interface
        </h2>
        <p>
          This interface represents a set of <a><code>AudioNode</code></a>
          objects and their connections. It allows for arbitrary routing of
          signals to the <a><code>AudioDestinationNode</code></a> (what the
          user ultimately hears). Nodes are created from the context and are
          then <a href="#ModularRouting">connected</a> together. In most use
          cases, only a single <a><code>AudioContext</code></a> is used per
          document.
        </p>
        <p>
          <!-- ReSpec does not support documenting constructor -->
           <span class="synchronous">When creating an <a>AudioContext</a>,
          execute these steps:</span>
        </p>
        <ol>
          <li>Set a <code>control thread flag</code> to
          <code>"suspended"</code> on the <a>AudioContext</a>.
          </li>
          <li>Send a <a>control message</a> to start processing.
          </li>
        </ol>
        <p>
          Sending a <a>control message</a> to start processing means executing
          the following steps:
        </p>
        <ol>
          <li>Attempt to <a href="#acquiring">Acquire system resources</a>.
          </li>
          <li>In case of failure, abort these steps.
          </li>
          <li>Queue a task on the <a>control thread</a> event loop, to execute
          these steps:
            <ol>
              <li>Set the <a href="#widl-audiocontext-state">state</a>
              attribute of the <a>AudioContext</a> to <code>"running"</code>.
              </li>
              <li>Queue a task to fire a simple event named
              <code>"statechange"</code> at the <a>AudioContext</a>.
              </li>
            </ol>
          </li>
        </ol>
        <p class="note">
          It is unfortunately not possible to programatically notify authors
          that the creation of the <a>AudioContext</a> failed. User-Agents are
          encouraged to log an informative message if they have access to a
          logging mecanism, such an a developer tools console.
        </p>
        <dl title="enum AudioContextState" class="idl">
          <dt>
            suspended
          </dt>
          <dd>
            The AudioContext is currently suspended (context time is not
            proceeding, audio hardware may be powered down/released).
          </dd>
          <dt>
            running
          </dt>
          <dd>
            Audio is being processed.
          </dd>
          <dt>
            closed
          </dt>
          <dd>
            The AudioContext has been released, and can no longer be used to
            process audio. All system audio resources have been released.
            <span class=synchronous>Attempts to create new Nodes on the
              AudioContext will throw InvalidStateError</span>. (AudioBuffers
            may still be created, through
            <a href=
            "#widl-AudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate">
            createBuffer</a> or <a href=
            "#widl-AudioContext-decodeAudioData-Promise-AudioBuffer--ArrayBuffer-audioData-DecodeSuccessCallback-successCallback-DecodeErrorCallback-errorCallback">
            decodeAudioData</a>.)
          </dd>
        </dl>
        <dl title="[Constructor] interface AudioContext : EventTarget" class=
        "idl" data-merge="DecodeSuccessCallback DecodeErrorCallback">
          <dt>
            readonly attribute AudioDestinationNode destination
          </dt>
          <dd>
            <p>
              An <a href=
              "#AudioDestinationNode"><code>AudioDestinationNode</code></a>
              with a single input representing the final destination for all
              audio. Usually this will represent the actual audio hardware. All
              <a><code>AudioNode</code></a>s actively rendering audio will
              directly or indirectly connect to <a href=
              "#widl-AudioContext-destination"><code>destination</code></a>.
            </p>
          </dd>
          <dt>
            readonly attribute float sampleRate
          </dt>
          <dd>
            <p>
              The sample rate (in sample-frames per second) at which the
              <a><code>AudioContext</code></a> handles audio. It is assumed
              that all <a><code>AudioNode</code></a>s in the context run at
              this rate. In making this assumption, sample-rate converters or
              "varispeed" processors are not supported in real-time processing.
            </p>
          </dd>
          <dt>
            readonly attribute double currentTime
          </dt>
          <dd>
            <p>
              This is a time in seconds which starts at zero when the context
              is created and increases in real-time. All scheduled times are
              relative to it. When the <a><code>AudioContext</code></a> is in
              the <a href=
              "#idl-def-AudioContextState.running"><code>running</code></a>
              state, the value of this attribute is always moving forward,
              monotonically increasing.
            </p>
          </dd>
          <dt>
            readonly attribute AudioListener listener
          </dt>
          <dd>
            <p>
              An <a href="#AudioListener"><code>AudioListener</code></a> which
              is used for 3D <a href="#Spatialization">spatialization</a>.
            </p>
          </dd>
          <dt>
            readonly attribute AudioContextState state
          </dt>
          <dd>
            Describes the current state of the AudioContext. The context state
            MUST begin in "suspended", and transitions to "running" when system
            resources are acquired and audio has begun processing. For
            OfflineAudioContexts, the state will remain in "suspended" until
            <code>startRendering()</code> is called, at which point it will
            transition to "running", and then to "closed" once audio processing
            has completed and oncomplete has been fired.
            <p>
              When the state is "suspended", a call to <code>resume()</code>
              will cause a transition to "running", or a call to
              <code>close()</code> will cause a transition to "closed".
            </p>
            <p>
              When the state is "running", a call to <code>suspend()</code>
              will cause a transition to "suspended", or a call to
              <code>close()</code> will cause a transition to "closed".
            </p>
            <p>
              When the state is "closed", no further state transitions are
              possible.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; suspend()
          </dt>
          <dd>
            <p>
              Suspends the progression of <a><code>AudioContext</code></a>'s
              <a href="#widl-AudioContext-currentTime">currentTime</a>, allows
              any current context processing blocks that are already processed
              to be played to the destination, and then allows the system to
              release its claim on audio hardware. This is generally useful
              when the application knows it will not need the
              <a>AudioContext</a> for some time, and wishes to temporarily
              <a>release system resources</a> associated to the
              <a>AudioContext</a>.
            </p>
            <p>
              <span class="synchronous">When suspend is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>"closed"</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>"suspended"</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>"suspended"</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to suspend the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to suspend an
              <a>AudioContext</a> means running these steps on the <a>rendering
              thread</a>:
            </p>
            <ol>
              <li>Attempt to <a>release system resources</a>.
              </li>
              <li>Queue a task on the <a>control thread</a>'s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>"suspended"</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>"suspended"</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>"statechanged"</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
            <p>
              While an <a>AudioContext</a> is suspended,
              <code>MediaStream</code>s will have their output ignored; that
              is, data will be lost by the real time nature of media streams.
              <code>HTMLMediaElement</code>s will similarly have their output
              ignored until the system is resumed. <a>AudioWorker</a>s and
              <a>ScriptProcessorNode</a>s will simply not fire their
              <code>onaudioprocess</code> events while suspended, but will
              resume when resumed. For the purpose of <a>AnalyserNode</a>
              window functions, the data is considered as a continuous stream -
              i.e. the <code>resume()</code>/<code>suspend()</code> does not
              cause silence to appear in the <a>AnalyserNode</a>'s stream of
              data.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; resume()
          </dt>
          <dd>
            Resumes the progression of the <a><code>AudioContext</code></a>'s'
            <a href="#widl-AudioContext-currentTime">currentTime</a> when it
            has been suspended.
            <p>
              <span class="synchronous">When resume is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>"closed"</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>"running"</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>"running"</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to resume the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to resume an <a>AudioContext</a>
              means running these steps on the <a>rendering thread</a>:
            </p>
            <ol>
              <li>Attempt to <a href="#acquiring">acquire system resources</a>.
              </li>
              <li>In case of failure, queue a task on the <a>control thread</a>
              to execute the following, and abort these steps
                <ol>
                  <li>Reject <em>promise</em>
                  </li>
                </ol>
              </li>
              <li>Queue a task on the <a>control thread</a>'s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>"running"</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>"running"</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>"statechange"</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            Promise&lt;void&gt; close()
          </dt>
          <dd>
            Closes the <a>AudioContext</a>, <a>releasing system resources</a>
            it's using. This will not automatically release all
            AudioContext-created objects, but will suspend the progression of
            the <a><code>AudioContext</code></a>'s <a href=
            "#widl-AudioContext-currentTime">currentTime</a>, and stop
            processing audio data.
            <p>
              <span class="synchronous">When close is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If this <a>AudioContext</a> is an <a>OfflineAudioContext</a>,
              reject <em>promise</em> with <code>NotSupportedError</code>,
              return it, and abort these steps.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>"closed"</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>"closed"</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>"closed"</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to suspend an
              <a>AudioContext</a> means running these steps on the <a>rendering
              thread</a>:
            </p>
            <ol>
              <li>Attempt to <a>release system resources</a>.
              </li>
              <li>Queue a task on the <a>control thread</a>'s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>"closed"</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>"closed"</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>"statechange"</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
            <p class="note">
              When an <a>AudioContext</a> is has been closed, implementation
              can choose to aggressively release more resource than when
              suspending.
            </p>
          </dd>
          <dt>
            attribute EventHandler onstatechange
          </dt>
          <dd>
            A property used to set the <code>EventHandler</code> for an event
            that is dispatched to <a><code>AudioContext</code></a> when the
            state of the AudioContext has changed (i.e. when the corresponding
            promise would have resolved). An event of type
            <a><code>Event</code></a> will be dispatched to the event handler,
            which can query the AudioContext's state directly. A newly-created
            AudioContext will always begin in the "suspended" state, and a
            state change event will be fired whenever the state changes to a
            different state.
          </dd>
          <dt>
            AudioBuffer createBuffer()
          </dt>
          <dd>
            Creates an AudioBuffer of the given size. The audio data in the
            buffer will be zero-initialized (silent). A NotSupportedError
            exception MUST be thrown if any of the arguments is negative, zero,
            or outside its nominal range.
            <dl class="parameters">
              <dt>
                unsigned long numberOfChannels
              </dt>
              <dd>
                Determines how many channels the buffer will have. An
                implementation must support at least 32 channels.
              </dd>
              <dt>
                unsigned long length
              </dt>
              <dd>
                Determines the size of the buffer in sample-frames.
              </dd>
              <dt>
                float sampleRate
              </dt>
              <dd>
                Describes the sample-rate of the linear PCM audio data in the
                buffer in sample-frames per second. An implementation must
                support sample rates in at least the range 8192 to 96000.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;AudioBuffer&gt; decodeAudioData()
          </dt>
          <dd>
            Asynchronously decodes the audio file data contained in the
            ArrayBuffer. The ArrayBuffer can, for example, be loaded from an
            XMLHttpRequest's <code>response</code> attribute after setting the
            <code>responseType</code> to "arraybuffer". Audio file data can be
            in any of the formats supported by the <code>audio</code> element.
            The buffer passed to <a href=
            "#widl-AudioContext-decodeAudioData-Promise-AudioBuffer--ArrayBuffer-audioData-DecodeSuccessCallback-successCallback-DecodeErrorCallback-errorCallback">
            decodeAudioData</a> has its content-type determined by sniffing, as
            described in [[mimesniff]].
            <dl class="parameters">
              <dt>
                ArrayBuffer audioData
              </dt>
              <dd>
                An ArrayBuffer containing compressed audio data
              </dd>
              <dt>
                optional DecodeSuccessCallback successCallback
              </dt>
              <dd>
                A callback function which will be invoked when the decoding is
                finished. The single argument to this callback is an
                AudioBuffer representing the decoded PCM audio data.
              </dd>
              <dt>
                optional DecodeErrorCallback errorCallback
              </dt>
              <dd>
                A callback function which will be invoked if there is an error
                decoding the audio file.
              </dd>
            </dl>
            <p>
              Although the primary method of interfacing with this function is
              via its promise return value, the callback parameters are
              provided for legacy reasons.
            </p>
            <p>
              The following steps must be performed:
            </p>
            <ol>
              <li>Let <var>promise</var> be a new promise.
              </li>
              <li>If <a>audioData</a> is null or not a valid ArrayBuffer:
                <ol>
                  <li>Let <var>error</var> be a <code>DOMException</code> whose
                  name is <code>NotSupportedError</code>.
                  </li>
                  <li>Reject <var>promise</var> with <var>error</var>.
                  </li>
                  <li>If <a>errorCallback</a> is not missing, invoke
                  <a>errorCallback</a> with <var>error</var>.
                  </li>
                  <li>Terminate this algorithm.
                  </li>
                </ol>
              </li>
              <li>Neuter the <a>audioData</a> ArrayBuffer in such a way that
              JavaScript code may not access or modify the data anymore.
              </li>
              <li>Queue a decoding operation to be performed on another thread.
              </li>
              <li>Return <var>promise</var>.
              </li>
              <li>In the decoding thread:
                <ol>
                  <li>Attempt to decode the encoded <a>audioData</a> into
                  linear PCM.
                  </li>
                  <li>If a decoding error is encountered due to the audio
                  format not being recognized or supported, or because of
                  corrupted/unexpected/inconsistent data, then, on the main
                  thread's event loop:
                    <ol>
                      <li>Let <var>error</var> be a <code>DOMException</code>
                      whose name is <code>"EncodingError"</code>.
                      </li>
                      <li>Reject <var>promise</var> with <var>error</var>.
                      </li>
                      <li>If <dfn>errorCallback</dfn> is not missing, invoke
                      <a>errorCallback</a> with <var>error</var>.
                      </li>
                    </ol>
                  </li>
                  <li>Otherwise:
                    <ol>
                      <li>Take the result, representing the decoded linear PCM
                      audio data, and resample it to the sample-rate of the <a>
                        <code>AudioContext</code></a> if it is different from
                        the sample-rate of <a>audioData</a>.
                      </li>
                      <li>On the main thread's event loop:
                        <ol>
                          <li>Let <var>buffer</var> be an
                          <code>AudioBuffer</code> containing the final result
                          (after possibly sample-rate converting).
                          </li>
                          <li>Resolve <var>promise</var> with
                          <var>buffer</var>.
                          </li>
                          <li>If <dfn>successCallback</dfn> is not missing,
                          invoke <a>successCallback</a> with <var>buffer</var>.
                          </li>
                        </ol>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            AudioBufferSourceNode createBufferSource()
          </dt>
          <dd>
            Creates an <a><code>AudioBufferSourceNode</code></a>.
          </dd>
          <dt>
            MediaElementAudioSourceNode createMediaElementSource()
          </dt>
          <dd>
            Creates a <a href=
            "#MediaElementAudioSourceNode">MediaElementAudioSourceNode</a>
            given an HTMLMediaElement. As a consequence of calling this method,
            audio playback from the HTMLMediaElement will be re-routed into the
            processing graph of the <a><code>AudioContext</code></a>.
            <dl class="parameters">
              <dt>
                HTMLMediaElement mediaElement
              </dt>
              <dd>
                The media element that will be re-routed.
              </dd>
            </dl>
          </dd>
          <dt>
            MediaStreamAudioSourceNode createMediaStreamSource()
          </dt>
          <dd>
            <dl class="parameters">
              <dt>
                MediaStream mediaStream
              </dt>
              <dd>
                The media stream that will act as source.
              </dd>
            </dl>
          </dd>
          <dt>
            MediaStreamAudioDestinationNode createMediaStreamDestination()
          </dt>
          <dd>
            Creates a <a><code>MediaStreamAudioDestinationNode</code></a>
          </dd>
          <dt>
            Promise&lt;AudioWorker&gt; createAudioWorker()
          </dt>
          <dd>
            Creates an <a><code>AudioWorker</code></a> object and loads the
            associated script into an
            <a><code>AudioWorkerGlobalScope</code></a>, then resolves the
            returned Promise.
            <dl class="parameters">
              <dt>
                DOMString scriptURL
              </dt>
              <dd>
                This parameter represents the URL of the script to be loaded as
                an <a>AudioWorker</a> node factory. See <a>AudioWorker</a>
                section for more detail.
              </dd>
            </dl>
          </dd>
          <dt>
            ScriptProcessorNode createScriptProcessor()
          </dt>
          <dd>
            This method is DEPRECATED, as it is intended to be replaced by
            createAudioWorker. Creates a
            <a><code>ScriptProcessorNode</code></a> for direct audio processing
            using JavaScript. An IndexSizeError exception MUST be thrown if
            <a><code>bufferSize</code></a> or
            <a><code>numberOfInputChannels</code></a> or
            <a><code>numberOfOutputChannels</code></a> are outside the valid
            range.
            <dl class="parameters">
              <dt>
                optional unsigned long bufferSize = 0
              </dt>
              <dd>
                The <a><code>bufferSize</code></a> parameter determines the
                buffer size in units of sample-frames. If it's not passed in,
                or if the value is 0, then the implementation will choose the
                best buffer size for the given environment, which will be
                constant power of 2 throughout the lifetime of the node.
                Otherwise if the author explicitly specifies the bufferSize, it
                must be one of the following values: 256, 512, 1024, 2048,
                4096, 8192, 16384. This value controls how frequently the
                <code>audioprocess</code> event is dispatched and how many
                sample-frames need to be processed each call. Lower values for
                <a><code>bufferSize</code></a> will result in a lower (better)
                <a href="#latency">latency</a>. Higher values will be necessary
                to avoid audio breakup and <a href=
                "#audio-glitching">glitches</a>. It is recommended for authors
                to not specify this buffer size and allow the implementation to
                pick a good buffer size to balance between <a href=
                "#latency">latency</a> and audio quality. If the value of this
                parameter is not one of the allowed power-of-2 values listed
                above, an IndexSizeError MUST be thrown.
              </dd>
              <dt>
                optional unsigned long numberOfInputChannels = 2
              </dt>
              <dd>
                This parameter determines the number of channels for this
                node's input. Values of up to 32 must be supported.
              </dd>
              <dt>
                optional unsigned long numberOfOutputChannels = 2
              </dt>
              <dd>
                This parameter determines the number of channels for this
                node's output. Values of up to 32 must be supported.
              </dd>
            </dl>It is invalid for both
            <a><code>numberOfInputChannels</code></a> and
            <a><code>numberOfOutputChannels</code></a> to be zero. In this case
            an IndexSizeError MUST be thrown.
          </dd>
          <dt>
            AnalyserNode createAnalyser()
          </dt>
          <dd>
            Create an <a><code>AnalyserNode</code></a>.
          </dd>
          <dt>
            GainNode createGain()
          </dt>
          <dd>
            Create an <a><code>GainNode</code></a>.
          </dd>
          <dt>
            DelayNode createDelay()
          </dt>
          <dd>
            Creates a <a><code>DelayNode</code></a> representing a variable
            delay line. The initial default delay time will be 0 seconds.
            <dl class="parameters">
              <dt>
                optional double maxDelayTime = 1.0
              </dt>
              <dd>
                The <dfn>maxDelayTime</dfn> parameter is optional and specifies
                the maximum delay time in seconds allowed for the delay line.
                If specified, this value MUST be greater than zero and less
                than three minutes or a NotSupportedError exception MUST be
                thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            BiquadFilterNode createBiquadFilter()
          </dt>
          <dd>
            Creates a <a><code>BiquadFilterNode</code></a> representing a
            second order filter which can be configured as one of several
            common filter types.
          </dd>
          <dt>
            IIRFilterNode createIIRFilter(Float32Array b, Float32Array a)
          </dt>
          <dd>
            Creates an <a><code>IIRFilterNode</code></a> representing a general
            IIR Filter.
            <dl class="parameters">
              <dt>
                Float32Array feedforward
              </dt>
              <dd>
                An array of the feedforward (numerator) coefficients for the
                transfer function of the IIR filter. The maximum length of this
                array is 20. If all of the values are zero, an
                InvalidStateError MUST be thrown. A NotSupportedError MUST be
                thrown if the array length is 0 or greater than 20.
              </dd>
              <dt>
                Float32Array feedback
              </dt>
              <dd>
                An array of the feedback (denominator) coefficients for the
                tranfer function of the IIR filter. The maximum length of this
                array is 20. If the first element of the array is 0, an
                InvalidStateError MUST be thrown. A NotSupportedError MUST be
                thrown if the array length is 0 or greater than 20.
              </dd>
            </dl>
          </dd>
          <dt>
            WaveShaperNode createWaveShaper()
          </dt>
          <dd>
            Creates a <a><code>WaveShaperNode</code></a> representing a
            non-linear distortion.
          </dd>
          <dt>
            PannerNode createPanner()
          </dt>
          <dd>
            This method is DEPRECATED, as it is intended to be replaced by
            createSpatialPanner or createStereoPanner, depending on the
            scenario. Creates a <a><code>PannerNode</code></a>.
          </dd>
          <dt>
            SpatialPannerNode createSpatialPanner()
          </dt>
          <dd>
            Creates a <a><code>SpatialPannerNode</code></a>.
          </dd>
          <dt>
            StereoPannerNode createStereoPanner()
          </dt>
          <dd>
            Creates a <a><code>StereoPannerNode</code></a>.
          </dd>
          <dt>
            ConvolverNode createConvolver()
          </dt>
          <dd>
            Creates a <a><code>ConvolverNode</code></a>.
          </dd>
          <dt>
            ChannelSplitterNode createChannelSplitter()
          </dt>
          <dd>
            Creates an <a><code>ChannelSplitterNode</code></a> representing a
            channel splitter. An IndexSizeError exception MUST be thrown for
            invalid parameter values.
            <dl class="parameters">
              <dt>
                optional unsigned long numberOfOutputs = 6
              </dt>
              <dd>
                The number of outputs. Values of up to 32 must be supported. If
                not specified, then 6 will be used.
              </dd>
            </dl>
          </dd>
          <dt>
            ChannelMergerNode createChannelMerger()
          </dt>
          <dd>
            Creates a <a><code>ChannelMergerNode</code></a> representing a
            channel merger. An IndexSizeError exception MUST be thrown for
            invalid parameter values.
            <dl class="parameters">
              <dt>
                optional unsigned long numberOfInputs = 6
              </dt>
              <dd>
                The <dfn>numberOfInputs</dfn> parameter determines the number
                of inputs. Values of up to 32 must be supported. If not
                specified, then 6 will be used.
              </dd>
            </dl>
          </dd>
          <dt>
            DynamicsCompressorNode createDynamicsCompressor()
          </dt>
          <dd>
            Creates a <a><code>DynamicsCompressorNode</code></a>
          </dd>
          <dt>
            OscillatorNode createOscillator()
          </dt>
          <dd>
            Creates an <a><code>OscillatorNode</code></a>
          </dd>
          <dt>
            PeriodicWave createPeriodicWave()
          </dt>
          <dd>
            Creates a <a><code>PeriodicWave</code></a> representing a waveform
            containing arbitrary harmonic content. The <code>real</code> and
            <code>imag</code> parameters must be of type
            <code>Float32Array</code> (described in [[!TYPED-ARRAYS]]) of equal
            lengths greater than zero or an IndexSizeError exception MUST be
            thrown. All implementations must support arrays up to at least
            8192. These parameters specify the Fourier coefficients of a
            <a href="https://en.wikipedia.org/wiki/Fourier_series">Fourier
            series</a> representing the partials of a periodic waveform. The
            created <a><code>PeriodicWave</code></a> will be used with an
            <a><code>OscillatorNode</code></a> and, by default, will represent
            a <em>normalized</em> time-domain waveform having maximum absolute
            peak value of 1. Another way of saying this is that the generated
            waveform of an <a><code>OscillatorNode</code></a> will have maximum
            peak value at 0dBFS. Conveniently, this corresponds to the
            full-range of the signal values used by the Web Audio API. Because
            the PeriodicWave is normalized by default on creation, the
            <code>real</code> and <code>imag</code> parameters represent
            <em>relative</em> values. If normalization is disabled via the
            <code>disableNormalization</code> parameter, this normalization is
            disabled, and the time-domain waveform has the amplitudes as given
            by the Fourier coefficients.
            <p>
              As <a>PeriodicWave</a> objects maintain their own copies of these
              arrays, any modification of the arrays uses as the
              <code>real</code> and <code>imag</code> parameters after the call
              to <a href=
              "#widl-AudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag">
              <code>createPeriodicWave()</code></a> will have no effect on the
              <a>PeriodicWave</a> object.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array real
              </dt>
              <dd>
                The <dfn id="dfn-real">real</dfn> parameter represents an array
                of <code>cosine</code> terms (traditionally the A terms). In
                audio terminology, the first element (index 0) is the DC-offset
                of the periodic waveform. The second element (index 1)
                represents the fundamental frequency. The third element
                represents the first overtone, and so on. The first element is
                ignored and implementations must set it to zero internally.
              </dd>
              <dt>
                Float32Array imag
              </dt>
              <dd>
                The <dfn id="dfn-imag">imag</dfn> parameter represents an array
                of <code>sine</code> terms (traditionally the B terms). The
                first element (index 0) should be set to zero (and will be
                ignored) since this term does not exist in the Fourier series.
                The second element (index 1) represents the fundamental
                frequency. The third element represents the first overtone, and
                so on.
              </dd>
              <dt>
                optional PeriodicWaveConstraints constraints
              </dt>
              <dd>
                If not given, the waveform is normalized. Otherwise, the
                waveform is normalized according the value given by
                <code>constraints</code>.
              </dd>
            </dl>
          </dd>
        </dl>
        <dl title="callback DecodeSuccessCallback = void" class="idl">
          <dt>
            AudioBuffer decodedData
          </dt>
          <dd>
            The AudioBuffer containing the decoded audio data.
          </dd>
        </dl>
        <dl title="callback DecodeErrorCallback = void" class="idl">
          <dt>
            DOMException error
          </dt>
          <dd>
            The error that occurred while decoding.
          </dd>
        </dl>
        <section>
          <h3 id="lifetime-AudioContext" class="informative">
            Lifetime
          </h3>
          <p>
            Once created, an <code>AudioContext</code> will continue to play
            sound until it has no more sound to play, or the page goes away.
          </p>
        </section>
        <section class="informative">
          <h3>
            Lack of introspection or serialization primitives
          </h3>
          <p>
            The Web Audio API takes a <em>fire-and-forget</em> approach to
            audio source scheduling. That is, <a>source nodes</a> are created
            for each note during the lifetime of the <a>AudioContext</a>, and
            never explicitely removed from the graph. This is incompatible with
            a serialization API, since there is no stable set of nodes that
            could be serialized.
          </p>
          <p>
            Moreover, having an introspection API would allow content script to
            be able to observe garbage collections.
          </p>
        </section>
        <section>
          <h3>
            System resources associated with an AudioContext
          </h3>
          <p>
            <a>AudioContext</a>s should be considered expensive objects.
            Creating an <a>AudioContext</a> often involves creating an
            high-priority thread, and use a low-latency system audio stream,
            both having impact on energy consumption. Creating more than one
            <a>AudioContext</a> in a document is most of the time unnecessary.
          </p>
          <p>
            Additionaly, user-agent can have an implementation-defined maximum
            number of <a>AudioContext</a>, after which any attempt to create a
            new <a>AudioContext</a> will fail, <span class=
            "synchronous">throwing <code>NotSupportedError</code></span>.
          </p>
          <p>
            <a href="#widl-AudioContext-suspend-Promise-void">suspend</a> and
            <a href="#widl-AudioContext-resume-Promise-void">close</a> allow
            authors to <dfn id="releasing">release system resources</dfn>.
            <a href="#releasing">Releasing system resources</a> means releasing
            the system resources such as threads, processes, audio streams, but
            conserving the state of the <a>AudioContext</a> such that it can
            continue to operate later after resuming if needed.
          </p>
          <p>
            Constructing or resuming an <a>AudioContext</a> involves <dfn id=
            "acquiring">acquiring system resources</dfn>. This means opening an
            system audio stream. This operation returns when the the audios
            stream is ready.
          </p>
          <p class="note">
            For example, this can involve waiting for the audio callbacks to
            fire regularly, or to wait for the hardware to be ready for
            processing.
          </p>
        </section>
      </section>
      <section>
        <h2 id="OfflineAudioContext">
          The OfflineAudioContext Interface
        </h2>
        <p>
          <a><code>OfflineAudioContext</code></a> is a particular type of
          <a><code>AudioContext</code></a> for rendering/mixing-down
          (potentially) faster than real-time. It does not render to the audio
          hardware, but instead renders as quickly as possible, fulfilling the
          returned promise with the rendered result as an
          <code>AudioBuffer</code>.
        </p>The OfflineAudioContext is constructed with the same arguments as
        AudioContext.createBuffer. A NotSupportedException exception MUST be
        thrown if any of the arguments is negative, zero, or outside its
        nominal range.
        <dl class="parameters">
          <dt>
            unsigned long numberOfChannels
          </dt>
          <dd>
            Determines how many channels the buffer will have. See <a href=
            "#widl-AudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate">
            createBuffer</a> for the supported number of channels.
          </dd>
          <dt>
            unsigned long length
          </dt>
          <dd>
            Determines the size of the buffer in sample-frames.
          </dd>
          <dt>
            float sampleRate
          </dt>
          <dd>
            Describes the sample-rate of the linear PCM audio data in the
            buffer in sample-frames per second. See <a href=
            "#widl-AudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate">
            createBuffer</a> for valid sample rates.
          </dd>
        </dl>
        <dl title=
        '[Constructor(unsigned long numberOfChannels, unsigned long length, float sampleRate)] interface OfflineAudioContext : AudioContext'
        class='idl'>
          <dt>
            Promise&lt;AudioBuffer&gt; startRendering()
          </dt>
          <dd>
            <p>
              Given the current connections and scheduled changes, starts
              rendering audio.
            </p>
            <p>
              Although the primary method of getting the rendered audio data is
              via its promise return value, the instance will also fire an
              event named <code>complete</code> for legacy reasons.
            </p>
            <p>
              The following steps must be performed:
            </p>
            <ol>
              <li>If <code>startRendering</code> has already been called
              previously, then return a promise rejected with
              <code>InvalidStateError</code>.
              </li>
              <li>Let <var>promise</var> be a new promise.
              </li>
              <li>Asynchronously perform the following steps:
                <ol>
                  <li>Let <var>buffer</var> be a new <code>AudioBuffer</code>,
                  with a number of channels, length and sample rate equal
                  respectively to the <code>numberOfChannels</code>,
                  <code>length</code> and <code>sampleRate</code> parameters
                  used when this instance's constructor was called.
                  </li>
                  <li>Given the current connections and scheduled changes,
                  start rendering <code>length</code> sample-frames of audio
                  into <var>buffer</var>.
                  </li>
                  <li>For every render quantum, check and suspend the rendering
                  if necessary.
                  </li>
                  <li>If a suspended context is resumed, continue to render the
                  buffer.
                  </li>
                  <li>Once the rendering is complete,
                    <ol>
                      <li>Resolve <var>promise</var> with <var>buffer</var>.
                      </li>
                      <li>Queue a task to fire an event named
                      <code>complete</code> at this instance, using an instance
                      of <a><code>OfflineAudioCompletionEvent</code></a> whose
                      <code>renderedBuffer</code> property is set to
                      <var>buffer</var>.
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
              <li>Return <var>promise</var>.
              </li>
            </ol>
          </dd>
          <dt>
            Promise&lt;void&gt; resume()
          </dt>
          <dd>
            <p>
              Resumes the progression of time in an audio context that has been
              suspended. The promise resolves immediately because the
              <a><code>OfflineAudioContext</code></a> does not require the
              audio hardware. If the context is not currently suspended or the
              rendering has not started, the promise is rejected with
              <code>InvalidStateError</code>.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; suspend()
          </dt>
          <dd>
            <p>
              Schedules a suspension of the time progression in the audio
              context at the specified time and returns a promise. This is
              generally useful when manipulating the audio graph synchronously
              on <a><code>OfflineAudioContext</code></a>.
            </p>
            <p>
              Note that the maximum precision of suspension is the size of the
              render quantum and the specified suspension time will be rounded
              down to the nearest render quantum boundary. For this reason, it
              is not allowed to schedule multiple suspends at the same
              quantized frame. Also scheduling should be done while the context
              is not running to ensure the precise suspension.
            </p>
            <dl class="parameters">
              <dt>
                double suspendTime
              </dt>
              <dd>
                Schedules a suspension of the rendering at the specified time,
                which is quantized and rounded down to the render quantum size.
                If the quantized frame number
                <ol>
                  <li>is negative or
                  </li>
                  <li>is less than or equal to the current time or
                  </li>
                  <li>is greater than or equal to the total render duration or
                  </li>
                  <li>is scheduled by another suspend for the same time,
                  </li>
                </ol>then the promise is rejected with
                <code>InvalidStateError</code>.
              </dd>
            </dl>
          </dd>
          <dt>
            attribute EventHandler oncomplete
          </dt>
          <dd>
            <p>
              An EventHandler of type <a href=
              "#OfflineAudioCompletionEvent">OfflineAudioCompletionEvent</a>.
            </p>
          </dd>
        </dl>
        <section>
          <h2 id="OfflineAudioCompletionEvent">
            The OfflineAudioCompletionEvent Interface
          </h2>
          <p>
            This is an <code>Event</code> object which is dispatched to
            <a><code>OfflineAudioContext</code></a> for legacy reasons.
          </p>
          <dl title="interface OfflineAudioCompletionEvent : Event" class=
          "idl">
            <dt>
              readonly attribute AudioBuffer renderedBuffer
            </dt>
            <dd>
              <p>
                An <code>AudioBuffer</code> containing the rendered audio data.
              </p>
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The <dfn>AudioNode</dfn> Interface
        </h2>
        <p>
          <a>AudioNode</a>s are the building blocks of an <a href=
          "#AudioContext"><code>AudioContext</code></a>. This interface
          represents audio sources, the audio destination, and intermediate
          processing modules. These modules can be connected together to form
          <a href="#ModularRouting">processing graphs</a> for rendering audio
          to the audio hardware. Each node can have <a>inputs</a> and/or
          <a>outputs</a>. A <a>source node</a> has no inputs and a single
          output. An <a href=
          "#AudioDestinationNode"><code>AudioDestinationNode</code></a> has one
          input and no outputs and represents the final destination to the
          audio hardware. Most processing nodes such as filters will have one
          input and one output. Each type of <a><code>AudioNode</code></a>
          differs in the details of how it processes or synthesizes audio. But,
          in general, an <a><code>AudioNode</code></a> will process its inputs
          (if it has any), and generate audio for its outputs (if it has any).
        </p>
        <p>
          Each output has one or more channels. The exact number of channels
          depends on the details of the specific <a><code>AudioNode</code></a>.
        </p>
        <p>
          An output may connect to one or more <a><code>AudioNode</code></a>
          inputs, thus <em>fan-out</em> is supported. An input initially has no
          connections, but may be connected from one or more <a>AudioNode</a>
          outputs, thus <em>fan-in</em> is supported. When the
          <code>connect()</code> method is called to connect an output of an
          <a>AudioNode</a> to an input of an <a>AudioNode</a>, we call that a
          <dfn>connection</dfn> to the input.
        </p>
        <p>
          Each <a>AudioNode</a> <dfn>input</dfn> has a specific number of
          channels at any given time. This number can change depending on the
          <a>connection</a>(s) made to the input. If the input has no
          connections then it has one channel which is silent.
        </p>
        <p>
          For each <a>input</a>, an <a><code>AudioNode</code></a> performs a
          mixing (usually an up-mixing) of all connections to that input.
          Please see <a href="#mixer-gain-structure"></a> for more informative
          details, and the <a href="#channel-up-mixing-and-down-mixing"></a>
          section for normative requirements.
        </p>
        <p>
          The processing of inputs and the internal operations of an
          <a>AudioNode</a> take place continuously with respect to
          <a>AudioContext</a> time, regardless of whether the node has
          connected outputs, and regardless of whether these outputs ultimately
          reach an <a>AudioContext</a>'s <a>AudioDestinationNode</a>.
        </p>
        <p>
          For performance reasons, practical implementations will need to use
          block processing, with each <a><code>AudioNode</code></a> processing
          a fixed number of sample-frames of size <em>block-size</em>. In order
          to get uniform behavior across implementations, we will define this
          value explicitly. <em>block-size</em> is defined to be 128
          sample-frames which corresponds to roughly 3ms at a sample-rate of
          44.1KHz.
        </p>
        <p>
          <a>AudioNode</a>s are <em>EventTarget</em>s, as described in <cite><a
              href= "https://dom.spec.whatwg.org/">DOM</a></cite> [[!DOM]]. This
          means that it is possible to dispatch events to
          <a><code>AudioNode</code></a>s the same way that other EventTargets
          accept events.
        </p>
        <dl title="enum ChannelCountMode" class="idl">
          <dt>
            max
          </dt>
          <dd>
            <a><code>computedNumberOfChannels</code></a> is computed as the
            maximum of the number of channels of all connections. In this mode
            channelCount is ignored
          </dd>
          <dt>
            clamped-max
          </dt>
          <dd>
            Same as “max” up to a limit of the channelCount
          </dd>
          <dt>
            explicit
          </dt>
          <dd>
            <a><code>computedNumberOfChannels</code></a> is the exact value as
            specified in channelCount
          </dd>
        </dl>
        <dl title="enum ChannelInterpretation" class="idl">
          <dt>
            speakers
          </dt>
          <dd>
            use <a href="#ChannelLayouts">up-down-mix equations for
            mono/stereo/quad/5.1</a>. In cases where the number of channels do
            not match any of these basic speaker layouts, revert to "discrete".
          </dd>
          <dt>
            discrete
          </dt>
          <dd>
            Up-mix by filling channels until they run out then zero out
            remaining channels. down-mix by filling as many channels as
            possible, then dropping remaining channels.
          </dd>
        </dl>
        <dl title="interface AudioNode : EventTarget" class="idl">
          <dt>
            void connect()
          </dt>
          <dd>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to connect to. If the
                <code>destination</code> parameter is an
                <a><code>AudioNode</code></a> that has been created using
                another <a><code>AudioContext</code></a>, an InvalidAccessError
                MUST be thrown. That is, <a><code>AudioNode</code></a>s cannot
                be shared between <a><code>AudioContext</code></a>.
              </dd>
              <dt>
                optional unsigned long output = 0
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                connect. If this paremeter is out-of-bound, an IndexSizeError
                exception MUST be thrown. It is possible to connect an
                <a><code>AudioNode</code></a> output to more than one input
                with multiple calls to connect(). Thus, "fan-out" is supported.
              </dd>
              <dt>
                optional unsigned long input = 0
              </dt>
              <dd>
                The <code>input</code> parameter is an index describing which
                input of the destination <a><code>AudioNode</code></a> to
                connect to. If this parameter is out-of-bounds, an
                IndexSizeError exception MUST be thrown. It is possible to
                connect an <a><code>AudioNode</code></a> to another
                <a><code>AudioNode</code></a> which creates a <dfn>cycle</dfn>:
                an <a><code>AudioNode</code></a> may connect to another
                <a><code>AudioNode</code></a>, which in turn connects back to
                the first <a><code>AudioNode</code></a>. This is allowed only
                if there is at least one <a><code>DelayNode</code></a> in the
                <em>cycle</em> or a NotSupportedError exception MUST be thrown.
              </dd>
            </dl>
            <p>
              There can only be one connection between a given output of one
              specific node and a given input of another specific node.
              Multiple connections with the same termini are ignored. For
              example:
            </p>
            <pre class="highlight example">
    nodeA.connect(nodeB);
    nodeA.connect(nodeB);
    </pre>
            <p>
              will have the same effect as
            </p>
            <pre class="highlight example">
      nodeA.connect(nodeB);
    </pre>
          </dd>
          <dt>
            void connect()
          </dt>
          <dd>
            Connects the <a><code>AudioNode</code></a> to an
            <a><code>AudioParam</code></a>, controlling the parameter value
            with an audio-rate signal.
            <dl class="parameters">
              <dt>
                AudioParam destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioParam</code></a> to connect to.
              </dd>
              <dt>
                optional unsigned long output = 0
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                connect. If the <code>parameter</code> is out-of-bound, an
                IndexSizeError exception MUST be thrown.
              </dd>
            </dl>
            <p>
              It is possible to connect an <a><code>AudioNode</code></a> output
              to more than one <a><code>AudioParam</code></a> with multiple
              calls to connect(). Thus, "fan-out" is supported.
            </p>
            <p>
              It is possible to connect more than one
              <a><code>AudioNode</code></a> output to a single
              <a><code>AudioParam</code></a> with multiple calls to connect().
              Thus, "fan-in" is supported.
            </p>
            <p>
              An <a><code>AudioParam</code></a> will take the rendered audio
              data from any <a><code>AudioNode</code></a> output connected to
              it and <a href="#down-mix">convert it to mono</a> by down-mixing
              if it is not already mono, then mix it together with other such
              outputs and finally will mix with the <em>intrinsic</em>
              parameter value (the <code>value</code> the
              <a><code>AudioParam</code></a> would normally have without any
              audio connections), including any timeline changes scheduled for
              the parameter.
            </p>
            <p>
              There can only be one connection between a given output of one
              specific node and a specific <a><code>AudioParam</code></a>.
              Multiple connections with the same termini are ignored. For
              example:
            </p>
            <pre class="highlight">
      nodeA.connect(param);
      nodeA.connect(param);
    </pre>will have the same effect as
            <pre>
      nodeA.connect(param);
    </pre>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects all outgoing connections from the
              <a><code>AudioNode</code></a>.
            </p>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a single output of the <a><code>AudioNode</code></a>
              from any other <a><code>AudioNode</code></a> or
              <a><code>AudioParam</code></a> objects to which it is connected.
            </p>
            <dl class="parameters">
              <dt>
                unsigned long output
              </dt>
              <dd>
                This parameter is an index describing which output of the
                <a><code>AudioNode</code></a> to disconnect. It disconnects all
                outgoing connections from the given output. If this parameter
                is out-of-bounds, an IndexSizeError exception MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects all outputs of the <a><code>AudioNode</code></a> that
              go to a specific destination <a><code>AudioNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to disconnect. It disconnects all
                outgoing connections to the given <code>destination</code>. If
                there is no connection to <code>destination</code>, an
                InvalidAccessError exception MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a specific output of the
              <a><code>AudioNode</code></a> from a specific destination
              <a><code>AudioNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to disconnect. If there is no
                connection to the <code>destination</code> from the given
                output, an InvalidAccessError exception MUST be thrown.
              </dd>
              <dt>
                unsigned long output
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                disconnect. If this parameter is out-of-bound, an
                IndexSizeError exception MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a specific output of the
              <a><code>AudioNode</code></a> from a specific input of some
              destination <a><code>AudioNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to disconnect. If there is no
                connection to the <code>destination</code> from the given
                output to the given input, an InvalidAccessError exception MUST
                be thrown.
              </dd>
              <dt>
                unsigned long output
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                disconnect. If this parameter is out-of-bound, an
                IndexSizeError exception MUST be thrown.
              </dd>
              <dt>
                unsigned long input
              </dt>
              <dd>
                The <code>input</code> parameter is an index describing which
                input of the destination <a><code>AudioNode</code></a> to
                disconnect. If this parameter is out-of-bounds, an
                IndexSizeError exception MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects all outputs of the <a><code>AudioNode</code></a> that
              go to a specific destination <a><code>AudioParam</code></a>. The
              contribution of this <a><code>AudioNode</code></a> to the
              computed parameter value goes to 0 when this operation takes
              effect. The intrinsic parameter value is not affected by this
              operation.
            </p>
            <dl class="parameters">
              <dt>
                AudioParam destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioParam</code></a> to disconnect. If there is no
                connection to the <code>destination</code>, an
                InvalidAccessError exception MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a specific output of the
              <a><code>AudioNode</code></a> from a specific destination
              <a><code>AudioParam</code></a>. The contribution of this
              <a><code>AudioNode</code></a> to the computed parameter value
              goes to 0 when this operation takes effect. The intrinsic
              parameter value is not affected by this operation.
            </p>
            <dl class="parameters">
              <dt>
                AudioParam destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioParam</code></a> to disconnect. If there is no
                connection to the <code>destination</code>, an
                InvalidAccessError exception MUST be thrown.
              </dd>
              <dt>
                unsigned long output
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                disconnect. If the <code>parameter</code> is out-of-bound, an
                IndexSizeError exception MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            readonly attribute AudioContext context
          </dt>
          <dd>
            The <a><code>AudioContext</code></a> which owns this
            <a><code>AudioNode</code></a>.
          </dd>
          <dt>
            readonly attribute unsigned long numberOfInputs
          </dt>
          <dd>
            The number of inputs feeding into the
            <a><code>AudioNode</code></a>. For <dfn>source nodes</dfn>, this
            will be 0. This attribute is predetermined for many
            <a><code>AudioNode</code></a> types, but some
            <a><code>AudioNode</code></a>, like the
            <a><code>ChannelMergerNode</code></a> and the
            <a><code>AudioWorkerNode</code></a> have variable number of inputs.
          </dd>
          <dt>
            readonly attribute unsigned long numberOfOutputs
          </dt>
          <dd>
            The number of outputs coming out of the
            <a><code>AudioNode</code></a>. This attribute is predetermined for
            some <a><code>AudioNode</code></a> types, but can be variable, like
            for the <a><code>ChannelSplitterNode</code></a> and the
            <a><code>AudioWorkerNode</code></a>.
          </dd>
          <dt>
            attribute unsigned long <dfn>channelCount</dfn>
          </dt>
          <dd>
            <p>
              The number of channels used when up-mixing and down-mixing
              connections to any inputs to the node. The default value is 2
              except for specific nodes where its value is specially
              determined. This attribute has no effect for nodes with no
              inputs. If this value is set to zero or to a value greater than
              the implementation's maximum number of channels the
              implementation MUST throw a NotSupportedError exception.
            </p>
            <p>
              See the <a href="#channel-up-mixing-and-down-mixing"></a> section
              for more information on this attribute.
            </p>
          </dd>
          <dt>
            attribute ChannelCountMode channelCountMode
          </dt>
          <dd>
            <p>
              Determines how channels will be counted when up-mixing and
              down-mixing connections to any inputs to the node. This attribute
              has no effect for nodes with no inputs.
            </p>
            <p>
              See the <a href="#channel-up-mixing-and-down-mixing"></a> section
              for more information on this attribute.
            </p>
          </dd>
          <dt>
            attribute ChannelInterpretation channelInterpretation
          </dt>
          <dd>
            <p>
              Determines how individual channels will be treated when up-mixing
              and down-mixing connections to any inputs to the node. This
              attribute has no effect for nodes with no inputs.
            </p>
            <p>
              See the <a href="#channel-up-mixing-and-down-mixing"></a> section
              for more information on this attribute.
            </p>
          </dd>
        </dl>
        <section>
          <h3 id="lifetime-AudioNode" class="informative">
            Lifetime
          </h3>
          <p>
            <i>This section is informative.</i>
          </p>
          <p>
            An implementation may choose any method to avoid unnecessary
            resource usage and unbounded memory growth of unused/finished
            nodes. The following is a description to help guide the general
            expectation of how node lifetime would be managed.
          </p>
          <p>
            An <a><code>AudioNode</code></a> will live as long as there are any
            references to it. There are several types of references:
          </p>
          <ol>
            <li>A <em>normal</em> JavaScript reference obeying normal garbage
            collection rules.
            </li>
            <li>A <em>playing</em> reference for both
            <a><code>AudioBufferSourceNode</code></a>s and
            <a><code>OscillatorNode</code></a>s. These nodes maintain a
            <em>playing</em> reference to themselves while they are currently
            playing.
            </li>
            <li>A <em>connection</em> reference which occurs if another
            <a><code>AudioNode</code></a> is connected to it.
            </li>
            <li>A <em>tail-time</em> reference which an
            <a><code>AudioNode</code></a> maintains on itself as long as it has
            any internal processing state which has not yet been emitted. For
            example, a <a><code>ConvolverNode</code></a> has a tail which
            continues to play even after receiving silent input (think about
            clapping your hands in a large concert hall and continuing to hear
            the sound reverberate throughout the hall). Some
            <a><code>AudioNode</code></a>s have this property. Please see
            details for specific nodes.
            </li>
          </ol>
          <p>
            Any <a><code>AudioNode</code></a>s which are connected in a cycle
            <em>and</em> are directly or indirectly connected to the
            <a><code>AudioDestinationNode</code></a> of the
            <a><code>AudioContext</code></a> will stay alive as long as the
            <a><code>AudioContext</code></a> is alive.
          </p>
          <p class="note">
            The uninterrupted operation of <a>AudioNode</a>s implies that as
            long as live references exist to a node, the node will continue
            processing its inputs and evolving its internal state even if it is
            disconnected from the audio graph. Since this processing will
            consume CPU and power, developers should carefully consider the
            resource usage of disconnected nodes. In particular, it is a good
            idea to minimize resource consumption by explicitly putting
            disconnected nodes into a stopped state when possible.
          </p>
          <p>
            When an <a><code>AudioNode</code></a> has no references it will be
            deleted. Before it is deleted, it will disconnect itself from any
            other <a><code>AudioNode</code></a>s which it is connected to. In
            this way it releases all connection references (3) it has to other
            nodes.
          </p>
          <p>
            Regardless of any of the above references, it can be assumed that
            the <a><code>AudioNode</code></a> will be deleted when its
            <a><code>AudioContext</code></a> is deleted.
          </p>
        </section>
      </section>
      <section>
        <h2 id="AudioDestinationNode">
          The AudioDestinationNode Interface
        </h2>
        <p>
          This is an <a><code>AudioNode</code></a> representing the final audio
          destination and is what the user will ultimately hear. It can often
          be considered as an audio output device which is connected to
          speakers. All rendered audio to be heard will be routed to this node,
          a "terminal" node in the <a><code>AudioContext</code></a>'s routing
          graph. There is only a single AudioDestinationNode per
          <a><code>AudioContext</code></a>, provided through the
          <code>destination</code> attribute of
          <a><code>AudioContext</code></a>.
        </p>
        <pre>
      numberOfInputs  : 1
      numberOfOutputs : 0

      channelCount = 2;
      channelCountMode = "explicit";
      channelInterpretation = "speakers";
</pre>
        <dl title="interface AudioDestinationNode : AudioNode" class="idl">
          <dt>
            readonly attribute unsigned long maxChannelCount
          </dt>
          <dd>
            <p>
              The maximum number of channels that the <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a>
              attribute can be set to. An
              <a><code>AudioDestinationNode</code></a> representing the audio
              hardware end-point (the normal case) can potentially output more
              than 2 channels of audio if the audio hardware is multi-channel.
              <code>maxChannelCount</code> is the maximum number of channels
              that this hardware is capable of supporting. If this value is 0,
              then this indicates that <a href=
              "#widl-AudioNode-channelCount">channelCount</a> may not be
              changed. This will be the case for an
              <a><code>AudioDestinationNode</code></a> in an
              <a><code>OfflineAudioContext</code></a> and also for basic
              implementations with hardware support for stereo output only.
            </p>
            <p>
              <a href="#widl-AudioNode-channelCount">channelCount</a> defaults
              to 2 for a destination in a normal
              <a><code>AudioContext</code></a>, and may be set to any non-zero
              value less than or equal to <code>maxChannelCount</code>. An
              <a><code>IndexSizeError</code></a> exception MUST be thrown if
              this value is not within the valid range. Giving a concrete
              example, if the audio hardware supports 8-channel output, then we
              may set <a href="#widl-AudioNode-channelCount">channelCount</a>
              to 8, and render 8-channels of output.
            </p>
            <p>
              For an<a><code>AudioDestinationNode</code></a> in an
              <a><code>OfflineAudioContext</code></a>, the <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a> is
              determined when the offline context is created and this value may
              not be changed.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="AudioParam">
          The AudioParam Interface
        </h2>
        <p>
          <a><code>AudioParam</code></a> controls an individual aspect of an
          <a><code>AudioNode</code></a>'s functioning, such as volume. The
          parameter can be set immediately to a particular value using the
          <code>value</code> attribute. Or, value changes can be scheduled to
          happen at very precise times (in the coordinate system of
          <a><code>AudioContext</code></a>'s <a href=
          "#widl-AudioContext-currentTime">currentTime</a> attribute), for
          envelopes, volume fades, LFOs, filter sweeps, grain windows, etc. In
          this way, arbitrary timeline-based automation curves can be set on
          any <a><code>AudioParam</code></a>. Additionally, audio signals from
          the outputs of <a><code>AudioNode</code></a>s can be connected to an
          <a><code>AudioParam</code></a>, summing with the <em>intrinsic</em>
          parameter value.
        </p>
        <p>
          Some synthesis and processing <a><code>AudioNode</code></a>s have
          <code>AudioParams</code> as attributes whose values must be taken
          into account on a per-audio-sample basis. For other
          <code>AudioParams</code>, sample-accuracy is not important and the
          value changes can be sampled more coarsely. Each individual
          <code>AudioParam</code> will specify that it is either an
          <a>a-rate</a> parameter which means that its values must be taken
          into account on a per-audio-sample basis, or it is a <a>k-rate</a>
          parameter.
        </p>
        <p>
          Implementations must use block processing, with each
          <a><code>AudioNode</code></a> processing 128 sample-frames in each
          block.
        </p>
        <p>
          For each 128 sample-frame block, the value of a <dfn id=
          "k-rate">k-rate</dfn> parameter must be sampled at the time of the
          very first sample-frame, and that value must be used for the entire
          block. <dfn id="a-rate">a-rate</dfn> parameters must be sampled for
          each sample-frame of the block.
        </p>
        <p>
          An <code>AudioParam</code> maintains a time-ordered event list which
          is initially empty. The times are in the time coordinate system of
          the <a><code>AudioContext</code></a>'s <a href=
          "#widl-AudioContext-currentTime">currentTime</a> attribute. The
          events define a mapping from time to value. The following methods can
          change the event list by adding a new event into the list of a type
          specific to the method. Each event has a time associated with it, and
          the events will always be kept in time-order in the list. These
          methods will be called <em>automation</em> methods:
        </p>
        <ul>
          <li>
            <a href=
            "#widl-AudioParam-setValueAtTime-void-float-value-double-startTime">
            setValueAtTime()</a> - <em>SetValue</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-linearRampToValueAtTime-void-float-value-double-endTime">
            linearRampToValueAtTime()</a> - <em>LinearRampToValue</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-exponentialRampToValueAtTime-void-float-value-double-endTime">
            exponentialRampToValueAtTime()</a> -
            <em>ExponentialRampToValue</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-setTargetAtTime-void-float-target-double-startTime-float-timeConstant">
            setTargetAtTime()</a> - <em>SetTarget</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-setValueCurveAtTime-void-Float32Array-values-double-startTime-double-duration">
            setValueCurveAtTime()</a> - <em>SetValueCurve</em>
          </li>
        </ul>
        <p>
          The following rules will apply when calling these methods:
        </p>
        <ul>
          <li>If one of these events is added at a time where there is already
          an event of the exact same type, then the new event will replace the
          old one.
          </li>
          <li>If one of these events is added at a time where there is already
          one or more events of a different type, then it will be placed in the
          list after them, but before events whose times are after the event.
          </li>
          <li>If setValueCurveAtTime() is called for time \(T\) and duration
          \(D\) and there are any events having a time greater than \(T\), but
          less than \(T + D\), then a NotSupportedError exception MUST be
          thrown. In other words, it's not ok to schedule a value curve during
          a time period containing other events.
          </li>
          <li>Similarly a NotSupportedError exception MUST be thrown if any
          <em>automation</em> method is called at a time which is inside of the
          time interval of a <em>SetValueCurve</em> event at time T and
          duration D.
          </li>
        </ul>
        <dl title="interface AudioParam" class="idl">
          <dt>
            attribute float value
          </dt>
          <dd>
            <p>
              The parameter's floating-point value. This attribute is
              initialized to the <code>defaultValue</code>. If
              <code>value</code> is set during a time when there are any
              automation events scheduled then it will be ignored and no
              exception will be thrown.
            </p>
            <p>
              The effect of setting this attribute is equivalent to calling
              <code>setValueAtTime()</code> with the current
              <code>AudioContext</code>'s <code>currentTime</code> and the
              requested value. Subsequent accesses to this attribute's getter
              will return the same value.
            </p>
          </dd>
          <dt>
            readonly attribute float defaultValue
          </dt>
          <dd>
            Initial value for the <code>value</code> attribute.
          </dd>
          <dt>
            void setValueAtTime(float value, double startTime)
          </dt>
          <dd>
            <p>
              Schedules a parameter value change at the given time.
            </p>
            <dl class="parameters">
              <dt>
                float value
              </dt>
              <dd>
                The value the parameter will change to at the given time.
              </dd>
              <dt>
                double startTime
              </dt>
              <dd>
                The time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute at
                which the parameter changes to the given value. A TypeError
                exception MUST be thrown if <code>startTime</code> is negative
                or is not a finite number.
              </dd>
            </dl>
            <p>
              If there are no more events after this <em>SetValue</em> event,
              then for \(t \geq T_0\), \(v(t) = V\), where \(T_0\) is the
              <code>startTime</code> parameter and \(V\) is the
              <code>value</code> parameter. In other words, the value will
              remain constant.
            </p>
            <p>
              If the next event (having time \(T_1\)) after this
              <em>SetValue</em> event is not of type <em>LinearRampToValue</em>
              or <em>ExponentialRampToValue</em>, then, for \(T_0 \leq t &lt;
              T_1\):
            </p>
            <pre>
              $$
                v(t) = V
              $$
            </pre>
            <p>
              In other words, the value will remain constant during this time
              interval, allowing the creation of "step" functions.
            </p>
            <p>
              If the next event after this <em>SetValue</em> event is of type
              <em>LinearRampToValue</em> or <em>ExponentialRampToValue</em>
              then please see <code><a href=
              "#widl-AudioParam-linearRampToValueAtTime-void-float-value-double-endTime">
              linearRampToValueAtTime</a></code> or <code><a href=
              "#widl-AudioParam-exponentialRampToValueAtTime-void-float-value-double-endTime">
              exponentialRampToValueAtTime</a></code>, respectively.
            </p>
          </dd>
          <dt>
            void linearRampToValueAtTime(float value, double endTime)
          </dt>
          <dd>
            <p>
              Schedules a linear continuous change in parameter value from the
              previous scheduled parameter value to the given value.
            </p>
            <dl class="parameters">
              <dt>
                float value
              </dt>
              <dd>
                The value the parameter will linearly ramp to at the given
                time.
              </dd>
              <dt>
                double endTime
              </dt>
              <dd>
                The time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute at
                which the automation ends. A TypeError exception MUST be thrown
                if <code>endTime</code> is negative or is not a finite number.
              </dd>
            </dl>
            <p>
              The value during the time interval \(T_0 \leq t &lt; T_1\) (where
              \(T_0\) is the time of the previous event and \(T_1\) is the
              <code>endTime</code> parameter passed into this method) will be
              calculated as:
            </p>
            <pre>
              $$
                v(t) = V_0 + (V_1 - V_0) \frac{t - T_0}{T_1 - T_0}
              $$
            </pre>
            <p>
              Where \(V_0\) is the value at the time \(T_0\) and \(V_1\) is the
              <code>value</code> parameter passed into this method.
            </p>
            <p>
              If there are no more events after this LinearRampToValue event
              then for \(t \geq T_1\), \(v(t) = V_1\).
            </p>
          </dd>
          <dt>
            void exponentialRampToValueAtTime(float value, double endTime)
          </dt>
          <dd>
            <p>
              Schedules an exponential continuous change in parameter value
              from the previous scheduled parameter value to the given value.
              Parameters representing filter frequencies and playback rate are
              best changed exponentially because of the way humans perceive
              sound.
            </p>
            <p>
              The value during the time interval \(T_0 \leq t &lt; T_1\) (where
              \(T_0\) is the time of the previous event and \(T_1\) is the
              <code>endTime</code> parameter passed into this method) will be
              calculated as:
            </p>
            <pre>
              $$
                v(t) = V_0 \left(\frac{V_1}{V_0}\right)^\frac{t - T_0}{T_1 - T_0}
              $$
            </pre>
            <p>
              where \(V_0\) is the value at the time \(T_0\) and \(V_1\) is the
              <code>value</code> parameter passed into this method. It is an
              error if either \(V_0\) or \(V_1\) is not strictly positive.
            </p>
            <p>
              This also implies an exponential ramp to 0 is not possible. A
              good approximation can be achieved using <a href=
              "#widl-AudioParam-setTargetAtTime-void-float-target-double-startTime-float-timeConstant">
              setTargetAtTime</a> with an appropriately chosen time constant.
            </p>
            <p>
              If there are no more events after this ExponentialRampToValue
              event then for \(t \geq T_1\), \(v(t) = V_1\).
            </p>
            <dl class="parameters">
              <dt>
                float value
              </dt>
              <dd>
                The value the parameter will exponentially ramp to at the given
                time. A NotSupportedError exception MUST be thrown if this
                value is less than or equal to 0, or if the value at the time
                of the previous event is less than or equal to 0.
              </dd>
              <dt>
                double endTime
              </dt>
              <dd>
                The time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute
                where the exponential ramp ends. A TypeError exception MUST be
                thrown if <code>endTime</code> is negative or is not a finite
                number.
              </dd>
            </dl>
          </dd>
          <dt>
            void setTargetAtTime(float target, double startTime, float
            timeConstant)
          </dt>
          <dd>
            <p>
              Start exponentially approaching the target value at the given
              time with a rate having the given time constant. Among other
              uses, this is useful for implementing the "decay" and "release"
              portions of an ADSR envelope. Please note that the parameter
              value does not immediately change to the target value at the
              given time, but instead gradually changes to the target value.
            </p>
            <dl class="parameters">
              <dt>
                float target
              </dt>
              <dd>
                The value the parameter will <em>start</em> changing to at the
                given time.
              </dd>
              <dt>
                double startTime
              </dt>
              <dd>
                The time at which the exponential approach will begin, in the
                same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute. A
                TypeError exception MUST be thrown if <code>start</code> is
                negative or is not a finite number.
              </dd>
              <dt>
                float timeConstant
              </dt>
              <dd>
                The time-constant value of first-order filter (exponential)
                approach to the target value. The larger this value is, the
                slower the transition will be. The value must be strictly
                positive or a TypeError exception MUST be thrown.
                <p>
                  More precisely, <em>timeConstant</em> is the time it takes a
                  first-order linear continuous time-invariant system to reach
                  the value \(1 - 1/e\) (around 63.2%) given a step input
                  response (transition from 0 to 1 value).
                </p>
              </dd>
            </dl>
            <p>
              During the time interval: \(T_0 \leq t &lt; T_1\), where \(T_0\)
              is the <code>startTime</code> parameter and \(T_1\) represents
              the time of the event following this event (or \(\infty\) if
              there are no following events):
            </p>
            <pre>
              $$
                v(t) = V_1 + (V_0 - V_1)\, e^{-\left(\frac{t - T_0}{\tau}\right)}
              $$
            </pre>
            <p>
              where \(V_0\) is the initial value (the <code>.value</code>
              attribute) at \(T_0\) (the <code>startTime</code> parameter),
              \(V_1\) is equal to the <code>target</code> parameter, and
              \(\tau\) is the <code>timeConstant</code> parameter.
            </p>
          </dd>
          <dt>
            void setValueCurveAtTime(Float32Array values, double startTime,
            double duration)
          </dt>
          <dd>
            <p>
              Sets an array of arbitrary parameter values starting at the given
              time for the given duration. The number of values will be scaled
              to fit into the desired duration.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array values
              </dt>
              <dd>
                A Float32Array representing a parameter value curve. These
                values will apply starting at the given time and lasting for
                the given duration. When this method is called, an internal
                copy of the curve is created for automation purposes.
                Subsequent modifications of the contents of the passed-in array
                therefore have no effect on the the <a>AudioParam</a>.
              </dd>
              <dt>
                double startTime
              </dt>
              <dd>
                The start time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute at
                which the value curve will be applied. A TypeError exception
                MUST be thrown if <code>startTime</code> is negative or is not
                a finite number.
              </dd>
              <dt>
                double duration
              </dt>
              <dd>
                The amount of time in seconds (after the <em>time</em>
                parameter) where values will be calculated according to the
                <em>values</em> parameter.
              </dd>
            </dl>
            <p>
              Let \(T_0\) be <code>startTime</code>, \(T_D\) be
              <code>duration</code>, \(V\) be the <code>values</code> array,
              and \(N\) be the length of the <code>values</code> array. Then,
              during the time interval: \(T_0 \le t &lt; T_0 + T_D\), let
            </p>
            <pre>
              $$
                \begin{align*} k &amp;= \big\lfloor \frac{N}{T_D}(t-T_0) \big\rfloor \\
                \end{align*}
              $$
            </pre>
            <p>
              Then \(v(t)\) is computed by linearly interpolating between
              \(V[k]\) and \(V[k+1]\),
            </p>
            <p>
              After the end of the curve time interval (\(t \ge T_0 + T_D\)),
              the value will remain constant at the final curve value, until
              there is another automation event (if any).
            </p>
          </dd>
          <dt>
            void cancelScheduledValues(double startTime)
          </dt>
          <dd>
            <p>
              Cancels all scheduled parameter changes with times greater than
              or equal to <code>startTime</code>. Active
              <code><a>setTargetAtTime</a></code> automations (those with
              <code>startTime</code> less than the supplied time value) will
              also be cancelled.
            </p>
            <dl class="parameters">
              <dt>
                double startTime
              </dt>
              <dd>
                The starting time at and after which any previously scheduled
                parameter changes will be cancelled. It is a time in the same
                time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute. A
                TypeError exception MUST be thrown if <code>startTime</code> is
                negative or is not a finite number.
              </dd>
            </dl>
          </dd>
        </dl>
        <section>
          <h3>
            Computation of Value
          </h3>
          <p>
            <dfn>computedValue</dfn> is the final value controlling the audio
            DSP and is computed by the audio <a>rendering thread</a> during each
            rendering time quantum. It must be internally computed as follows:
          </p>
          <ol>
            <li>An <em>intrinsic</em> parameter value will be calculated at
            each time, which is either the value set directly to the
            <code>value</code> attribute, or, if there are any scheduled
            parameter changes (automation events) with times before or at this
            time, the value as calculated from these events. If the
            <code>value</code> attribute is set after any automation events
            have been scheduled, then these events will be removed. When read,
            the <code>value</code> attribute always returns the
            <em>intrinsic</em> value for the current time. If automation events
            are removed from a given time range, then the <em>intrinsic</em>
            value will remain unchanged and stay at its previous value until
            either the <code>value</code> attribute is directly set, or
            automation events are added for the time range.
            </li>
            <li>An <a><code>AudioParam</code></a> will take the rendered audio
            data from any <a><code>AudioNode</code></a> output connected to it
            and <a href="#down-mix">convert it to mono</a> by down-mixing if it
            is not already mono, then mix it together with other such outputs.
            If there are no <a><code>AudioNode</code></a>s connected to it,
            then this value is 0, having no effect on the
            <em>computedValue</em>.
            </li>
            <li>The <em>computedValue</em> is the sum of the <em>intrinsic</em>
            value and the value calculated from (2).
            </li>
          </ol>
        </section>
        <section>
          <h3 id="example1-AudioParam">
            AudioParam Automation Example
          </h3>
          <figure>
            <!-- The image here was created from
  http://googlechrome.github.io/web-audio-samples/samples/audio/timeline.html -->
            <img alt="AudioParam automation" src=
            "images/audioparam-automation1.png">
            <figcaption>
              An example of parameter automation.
            </figcaption>
          </figure>
          <pre class="code example"><code class="es-code highlight">
var curveLength = 44100;
var curve = new Float32Array(curveLength);
for (var i = 0; i &lt; curveLength; ++i)
    curve[i] = Math.sin(Math.PI * i / curveLength);

var t0 = 0;
var t1 = 0.1;
var t2 = 0.2;
var t3 = 0.3;
var t4 = 0.325;
var t5 = 0.5;
var t6 = 0.6;
var t7 = 0.7;
var t8 = 1.0;
var timeConstant = 0.1;

param.setValueAtTime(0.2, t0);
param.setValueAtTime(0.3, t1);
param.setValueAtTime(0.4, t2);
param.linearRampToValueAtTime(1, t3);
param.linearRampToValueAtTime(0.8, t4);
param.setTargetAtTime(.5, t4, timeConstant);
// Compute where the setTargetAtTime will be at time t5 so we can make
// the following exponential start at the right point so there's no
// jump discontinuity.  From the spec, we have
//   v(t) = 0.5 + (0.8 - 0.5)*exp(-(t-t4)/timeConstant)
// Thus v(t5) = 0.5 + (0.8 - 0.5)*exp(-(t5-t4)/timeConstant)
param.setValueAtTime(0.5 + (0.8 - 0.5)*Math.exp(-(t5 - t4)/timeConstant), t5);
param.exponentialRampToValueAtTime(0.75, t6);
param.exponentialRampToValueAtTime(0.05, t7);
param.setValueCurveAtTime(curve, t7, t8 - t7);
</code></pre>
        </section>
      </section>
      <section>
        <h2 id="GainNode">
          The GainNode Interface
        </h2>
        <p>
          Changing the gain of an audio signal is a fundamental operation in
          audio applications. The <code>GainNode</code> is one of the building
          blocks for creating <a href="#mixer-gain-structure">mixers</a>. This
          interface is an <a><code>AudioNode</code></a> with a single input and
          single output:
        </p>
        <pre>
  numberOfInputs  : 1
  numberOfOutputs : 1

  channelCountMode = "max";
  channelInterpretation = "speakers";
</pre>
        <p>
          Each sample of each channel of the input data of the
          <a><code>GainNode</code></a> MUST be multiplied by the
          <a>computedValue</a> of the <a href=
          "#widl-GainNode-gain"><code>gain</code></a>
          <a><code>AudioParam</code></a>.
        </p>
        <dl title="interface GainNode : AudioNode" class="idl">
          <dt>
            readonly attribute AudioParam gain
          </dt>
          <dd>
            Represents the amount of gain to apply. Its default
            <code>value</code> is 1 (no gain change). The nominal
            <code>minValue</code> is 0, but may be set negative for phase
            inversion. The nominal <code>maxValue</code> is 1, but higher
            values are allowed (no exception thrown).This parameter is
            <a>a-rate</a>
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="DelayNode">
          The DelayNode Interface
        </h2>
        <p>
          A delay-line is a fundamental building block in audio applications.
          This interface is an <a><code>AudioNode</code></a> with a single
          input and single output:
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <p>
          It delays the incoming audio signal by a certain amount.
          Specifically, at each time <em>t</em>, input signal
          <em>input(t)</em>, delay time <em>delayTime(t)</em> and output signal
          <em>output(t)</em>, the output will be <em>output(t) = input(t -
          delayTime(t))</em>. The default <code>delayTime</code> is 0 seconds
          (no delay).
        </p>
        <p>
          When the number of channels in a <a>DelayNode</a>'s input changes
          (thus changing the output channel count also), there may be delayed
          audio samples which have not yet been output by the node and are part
          of its internal state. If these samples were received earlier with a
          different channel count, they must be upmixed or downmixed before
          being combined with newly received input so that all internal
          delay-line mixing takes place using the single prevailing channel
          layout.
        </p>
        <dl title="interface DelayNode : AudioNode" class="idl">
          <dt>
            readonly attribute AudioParam delayTime
          </dt>
          <dd>
            <p>
              An <a><code>AudioParam</code></a> object representing the amount
              of delay (in seconds) to apply. Its default <code>value</code> is
              0 (no delay). The minimum value is 0 and the maximum value is
              determined by the <code>maxDelayTime</code> argument to the
              <code>AudioContext</code> method <code>createDelay</code>.
            </p>
            <p>
              If <a><code>DelayNode</code></a> is part of a <a>cycle</a>, then
              the value of the <a><code>delayTime</code></a> attribute is
              clamped to a minimum of 128 frames (one block).
            </p>
            <p>
              This parameter is <a>a-rate</a>.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="AudioBuffer">
          The AudioBuffer Interface
        </h2>
        <p>
          This interface represents a memory-resident audio asset (for one-shot
          sounds and other short audio clips). Its format is non-interleaved
          IEEE 32-bit linear PCM with a nominal range of -1 -&gt; +1. It can
          contain one or more channels. Typically, it would be expected that
          the length of the PCM data would be fairly short (usually somewhat
          less than a minute). For longer sounds, such as music soundtracks,
          streaming should be used with the <code>audio</code> element and
          <code>MediaElementAudioSourceNode</code>.
        </p>
        <p>
          An <a>AudioBuffer</a> may be used by one or more
          <a><code>AudioContext</code></a>s, and can be shared between an
          <a><code>OfflineAudioContext</code></a> and an
          <a><code>AudioContext</code></a>.
        </p>
        <dl title="interface AudioBuffer" class="idl">
          <dt>
            readonly attribute float sampleRate
          </dt>
          <dd>
            The sample-rate for the PCM audio data in samples per second.
          </dd>
          <dt>
            readonly attribute long length
          </dt>
          <dd>
            Length of the PCM audio data in sample-frames.
          </dd>
          <dt>
            readonly attribute double duration
          </dt>
          <dd>
            Duration of the PCM audio data in seconds.
          </dd>
          <dt>
            readonly attribute long numberOfChannels
          </dt>
          <dd>
            The number of discrete audio channels.
          </dd>
          <dt>
            Float32Array getChannelData()
          </dt>
          <dd>
            Returns the <code>Float32Array</code> representing the PCM audio
            data for the specific channel.
            <dl class="parameters">
              <dt>
                unsigned long channel
              </dt>
              <dd>
                This parameter is an index representing the particular channel
                to get data for. An index value of 0 represents the first
                channel. This index value MUST be less than
                <code>numberOfChannels</code> or an IndexSizeError exception
                MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void copyFromChannel()
          </dt>
          <dd>
            The <code>copyFromChannel</code> method copies the samples from the
            specified channel of the <a>AudioBuffer</a> to the
            <code>destination</code> array.
            <dl class="parameters">
              <dt>
                Float32Array destination
              </dt>
              <dd>
                The array the channel data will be copied to.
              </dd>
              <dt>
                unsigned long channelNumber
              </dt>
              <dd>
                The index of the channel to copy the data from. If
                <code>channelNumber</code> is greater or equal than the number
                of channel of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
              <dt>
                optional unsigned long startInChannel = 0
              </dt>
              <dd>
                An optional offset to copy the data from. If
                <code>startInChannel</code> is greater than the
                <code>length</code> of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void copyToChannel()
          </dt>
          <dd>
            The <code>copyToChannel</code> method copies the samples to the
            specified channel of the <a>AudioBuffer</a>, from the
            <code>source</code> array.
            <dl class="parameters">
              <dt>
                Float32Array source
              </dt>
              <dd>
                The array the channel data will be copied from.
              </dd>
              <dt>
                unsigned long channelNumber
              </dt>
              <dd>
                The index of the channel to copy the data to. If
                <code>channelNumber</code> is greater or equal than the number
                of channel of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
              <dt>
                optional unsigned long startInChannel = 0
              </dt>
              <dd>
                An optional offset to copy the data to. If
                <code>startInChannel</code> is greater than the
                <code>length</code> of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
            </dl>
          </dd>
        </dl>
        <p class="note">
          The methods <code>copyToChannel</code> and
          <code>copyFromChannel</code> can be used to fill part of an array by
          passing in a <code>Float32Array</code> that's a view onto the larger
          array. When reading data from an <a>AudioBuffer</a>'s channels, and
          the data can be processed in chunks, <code>copyFromChannel</code>
          should be preferred to calling <code>getChannelData</code> and
          accessing the resulting array, because it may avoid unnecessary
          memory allocation and copying.
        </p>
        <p>
          An internal operation <a href="#acquire-the-content">acquire the
          contents of an <code>AudioBuffer</code></a> is invoked when the
          contents of an <a>AudioBuffer</a> are needed by some API
          implementation. This operation returns immutable channel data to the
          invoker.
        </p>
        <p>
          When an <dfn id="acquire-the-content">acquire the content</dfn>
          operation occurs on an <a>AudioBuffer</a>, run the following steps:
        </p>
        <ol>
          <li>If any of the <a>AudioBuffer</a>'s <code>ArrayBuffer</code> have
          been neutered, abort these steps, and return a zero-length channel
          data buffers to the invoker.
          </li>
          <li>Neuter all <code>ArrayBuffer</code>s for arrays previously
          returned by <code>getChannelData</code> on this <a>AudioBuffer</a>.
          </li>
          <li>Retain the underlying data buffers from those
          <code>ArrayBuffer</code>s and return references to them to the
          invoker.
          </li>
          <li>Attach <code>ArrayBuffer</code>s containing copies of the data to
          the <a>AudioBuffer</a>, to be returned by the next call to
          <code>getChannelData</code>.
          </li>
        </ol>The <a href="#acquire-the-content">acquire the contents of an
        AudioBuffer</a> operation is invoked in the following cases:
        <ul>
          <li>When <code>AudioBufferSourceNode.start</code> is called, it
          <a href="#acquire-the-content">acquires the contents</a> of the
          node's <code>buffer</code>. If the operation fails, nothing is
          played.
          </li>
          <li>When a <a>ConvolverNode</a>'s <code>buffer</code> is set to an
          <a>AudioBuffer</a> while the node is connected to an output node, or
          a <a>ConvolverNode</a> is connected to an output node while the
          <a>ConvolverNode</a>'s <code>buffer</code> is set to an
          <a>AudioBuffer</a>, it <a href="#acquire-the-content">acquires the
          content</a> of the <a>AudioBuffer</a>.
          </li>
          <li>When the dispatch of an <a>AudioProcessingEvent</a> completes, it
          <a href="#acquire-the-content">acquires the contents</a> of its
          <code>outputBuffer</code>.
          </li>
        </ul>
        <p class="note">
          This means that <code>copyToChannel</code> cannot be used to change
          the content of an <a>AudioBuffer</a> currently in use by an
          <code>AudioNode</code> that has <a href=
          "#acquire-the-content">acquired the content of an AudioBuffer</a>,
          since the <a>AudioNode</a> will continue to use the data previously
          acquired.
        </p>
      </section>
      <section>
        <h2 id="AudioBufferSourceNode">
          The AudioBufferSourceNode Interface
        </h2>
        <p>
          This interface represents an audio source from an in-memory audio
          asset in an <code>AudioBuffer</code>. It is useful for playing audio
          assets which require a high degree of scheduling flexibility, for
          instance, playing back in rhythmically-perfect ways. If
          sample-accurate playback of network- or disk-backed assets is
          required, an implementer should use <a><code>AudioWorker</code></a>
          to implement playback.
        </p>
        <p>
          The start() method is used to schedule when sound playback will
          happen. The start() method may not be issued multiple times. The
          playback will stop automatically when the buffer's audio data has
          been completely played (if the <code>loop</code> attribute is false),
          or when the stop() method has been called and the specified time has
          been reached. Please see more details in the start() and stop()
          description.
        </p>
        <pre>
  numberOfInputs  : 0
  numberOfOutputs : 1
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the AudioBuffer assigned to the .buffer attribute, or is
          one channel of silence if .buffer is NULL.
        </p>
        <dl title="interface AudioBufferSourceNode : AudioNode" class="idl">
          <dt>
            attribute AudioBuffer? buffer
          </dt>
          <dd>
            Represents the audio asset to be played. This attribute can only be
            set once, or a <code>InvalidStateError</code> MUST be thrown.
          </dd>
          <dt>
            readonly attribute AudioParam playbackRate
          </dt>
          <dd>
            The speed at which to render the audio stream. Its default
            <code>value</code> is 1. This parameter is <a>k-rate</a>.
          </dd>
          <dt>
            readonly attribute AudioParam detune
          </dt>
          <dd>
            An aditional parameter to modulate the speed at which is rendered
            the audio stream. Its default value is 0. Its nominal range is
            [-1200; 1200]. This parameter is <a>k-rate</a>.
          </dd>
          <dt>
            attribute boolean loop
          </dt>
          <dd>
            Indicates if the audio data should play in a loop. The default
            value is false.
          </dd>
          <dt>
            attribute double loopStart
          </dt>
          <dd>
            An optional value in seconds where looping should begin if the
            <code>loop</code> attribute is true. Its default <code>value</code>
            is 0, and it may usefully be set to any value between 0 and the
            duration of the buffer.
          </dd>
          <dt>
            attribute double loopEnd
          </dt>
          <dd>
            An optional value in seconds where looping should end if the
            <code>loop</code> attribute is true. Its default <code>value</code>
            is 0, and it may usefully be set to any value between 0 and the
            duration of the buffer.
          </dd>
          <dt>
            void start()
          </dt>
          <dd>
            Schedules a sound to playback at an exact time. <code>start</code>
            may only be called one time and must be called before
            <code>stop</code> is called or an InvalidStateError exception MUST
            be thrown.
            <dl class="parameters">
              <dt>
                optional double when = 0
              </dt>
              <dd>
                The <a><code>when</code></a> parameter describes at what time
                (in seconds) the sound should start playing. It is in the same
                time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute. If
                0 is passed in for this value or if the value is less than
                <b>currentTime</b>, then the sound will start playing
                immediately. A TypeError exception MUST be thrown if
                <code>when</code> is negative.
              </dd>
              <dt>
                optional double offset = 0
              </dt>
              <dd>
                The <dfn id="dfn-offset">offset</dfn> parameter describes the
                offset time in the buffer (in seconds) where playback will
                begin. If 0 is passed in for this value, then playback will
                start from the beginning of the buffer. A TypeError exception
                MUST be thrown if <code>offset</code> is negative.
              </dd>
              <dt>
                optional double duration
              </dt>
              <dd>
                The <a><code>duration</code></a> parameter describes the
                duration of the portion (in seconds) to be played. If this
                parameter is not passed, the duration will be equal to the
                total duration of the AudioBuffer minus the <code>offset</code>
                parameter. Thus if neither <code>offset</code> nor
                <code>duration</code> are specified then the implied duration
                is the total duration of the AudioBuffer. An TypeError
                exception MUST be thrown if <code>duration</code> is negative.
              </dd>
            </dl>
          </dd>
          <dt>
            void stop()
          </dt>
          <dd>
            Schedules a sound to stop playback at an exact time.
            <dl class="parameters">
              <dt>
                optional double when = 0
              </dt>
              <dd>
                The <a><code>when</code></a> parameter describes at what time
                (in seconds) the sound should stop playing. It is in the same
                time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute. If
                0 is passed in for this value or if the value is less than
                <a><code>currentTime</code></a>, then the sound will stop
                playing immediately. A TypeError exception MUST be thrown if
                <code>when</code> is negative. If <code>stop</code> is called
                again after already have been called, the last invocation will
                be the only one applied; stop times set by previous calls will
                not be applied, unless the buffer has already stopped prior to
                any subsequent calls. If the buffer has already stopped,
                further calls to <code>stop</code> will have no effect. If a
                stop time is reached prior to the scheduled start time, the
                sound will not play.
              </dd>
            </dl>
          </dd>
          <dt>
            attribute EventHandler onended
          </dt>
          <dd>
            A property used to set the <code>EventHandler</code> (described in
            <cite><a href=
            "https://html.spec.whatwg.org/multipage/webappapis.html#eventhandler">
            HTML</a></cite>[[!HTML]]) for the ended event that is dispatched to
            <a><code>AudioBufferSourceNode</code></a> node types. When the
            playback of the buffer for an
            <a><code>AudioBufferSourceNode</code></a> is finished, an event of
            type <code>Event</code> (described in <cite><a href=
            "https://html.spec.whatwg.org/multipage/infrastructure.html#event">HTML</a></cite>
            [[!HTML]]) will be dispatched to the event handler.
          </dd>
        </dl>
        <p>
          Both <code>playbackRate</code> and <code>detune</code> are
          <a>k-rate</a> parameters and are used together to determine a
          <em>computedPlaybackRate</em> value:
        </p>
        <pre class="highlight">
  computedPlaybackRate(t) = playbackRate(t) * pow(2, detune(t) / 1200)
</pre>
        <p>
          The <code>computedPlaybackRate</code> is the effective speed at which
          the <a><code>AudioBuffer</code></a> of this
          <a><code>AudioBufferSourceNode</code></a> MUST be played.
        </p>
        <p>
          This MUST be implemented by <em>resampling</em> the input data using
          a resampling ratio of 1 / <code>computedPlaybackRate</code>, hence
          changing both the pitch and speed of the audio.
        </p>
        <section>
          <h3 id="looping-AudioBufferSourceNode">
            Looping
          </h3>
          <p>
            If the <code>loop</code> attribute is true when
            <code>start()</code> is called, then playback will continue
            indefinitely until <code>stop()</code> is called and the stop time
            is reached. We'll call this "loop" mode. Playback always starts at
            the point in the buffer indicated by the <code>offset</code>
            argument of <code>start()</code>, and in <em>loop</em> mode will
            continue playing until it reaches the <em>actualLoopEnd</em>
            position in the buffer (or the end of the buffer), at which point
            it will wrap back around to the <em>actualLoopStart</em> position
            in the buffer, and continue playing according to this pattern.
          </p>
          <p>
            In <em>loop</em> mode then the <em>actual</em> loop points are
            calculated as follows from the <code>loopStart</code> and
            <code>loopEnd</code> attributes:
          </p>
          <blockquote>
            <pre class="highlight">
if ((loopStart || loopEnd) &amp;& loopStart &gt;= 0 &amp;& loopEnd &gt; 0 &amp;& loopStart &lt; loopEnd) {
    actualLoopStart = loopStart;
    actualLoopEnd = min(loopEnd, buffer.duration);
} else {
    actualLoopStart = 0;
    actualLoopEnd = buffer.duration;
}
</pre>
          </blockquote>
          <p>
            Note that the default <code>value</code>s for
            <code>loopStart</code> and <code>loopEnd</code> are both 0, which
            indicates that looping should occur from the very start to the very
            end of the buffer.
          </p>
          <p>
            Please note that as a low-level implementation detail, the
            AudioBuffer is at a specific sample-rate (usually the same as the
            <a><code>AudioContext</code></a> sample-rate), and that the loop
            times (in seconds) must be converted to the appropriate
            sample-frame positions in the buffer according to this sample-rate.
          </p>
          <p>
            When scheduling the beginning and the end of playback using the
            <code>start()</code> and <code>stop()</code> methods, the resulting
            start or stop time MUST be rounded to the nearest sample-frame in
            the sample rate of the <a><code>AudioContext</code></a>. That is,
            no sub-sample scheduling is possible.
          </p>
        </section>
      </section>
      <section>
        <h2 id="MediaElementAudioSourceNode">
          The MediaElementAudioSourceNode Interface
        </h2>
        <p>
          This interface represents an audio source from an <code>audio</code>
          or <code>video</code> element.
        </p>
        <pre>
  numberOfInputs  : 0
  numberOfOutputs : 1
</pre>
        <p>
          The number of channels of the output corresponds to the number of
          channels of the media referenced by the
          <code>HTMLMediaElement</code>. Thus, changes to the media element's
          .src attribute can change the number of channels output by this node.
          If the .src attribute is not set, then the number of channels output
          will be one silent channel.
        </p>
        <dl title="interface MediaElementAudioSourceNode : AudioNode" class=
        "idl"></dl>
        <p>
          A <a>MediaElementAudioSourceNode</a> is created given an
          <code>HTMLMediaElement</code> using the <a>AudioContext</a>
          <code>createMediaElementSource()</code> method.
        </p>
        <p>
          The number of channels of the single output equals the number of
          channels of the audio referenced by the <code>HTMLMediaElement</code>
          passed in as the argument to <code>createMediaElementSource()</code>,
          or is 1 if the <code>HTMLMediaElement</code> has no audio.
        </p>
        <p>
          The <code>HTMLMediaElement</code> must behave in an identical fashion
          after the <a>MediaElementAudioSourceNode</a> has been created,
          <em>except</em> that the rendered audio will no longer be heard
          directly, but instead will be heard as a consequence of the
          <a>MediaElementAudioSourceNode</a> being connected through the
          routing graph. Thus pausing, seeking, volume, <code>src</code>
          attribute changes, and other aspects of the
          <code>HTMLMediaElement</code> must behave as they normally would if
          <em>not</em> used with a <a>MediaElementAudioSourceNode</a>.
        </p>
        <pre class="highlight example">
  var mediaElement = document.getElementById('mediaElementID');
  var sourceNode = context.createMediaElementSource(mediaElement);
  sourceNode.connect(filterNode);
</pre>
        <section>
          <h2>
            Security with MediaElementAudioSourceNode and cross-origin
            resources
          </h2>
          <p>
            <code>HTMLMediaElement</code> allows the playback of cross-origin
            resources. Because Web Audio can allows one to inspect the content
            of the resource (e.g. using a <a>MediaElementAudioSourceNode</a>,
            and a <a>ScriptProcessorNode</a> to read the samples), information
            leakage can occur if scripts from one <a href=
            "http://www.w3.org/html/wg/drafts/html/master/browsers.html#origin-0">
            origin</a> inspect the content of a resource from another <a href=
            "http://www.w3.org/html/wg/drafts/html/master/browsers.html#origin-0">
            origin</a>.
          </p>
          <p>
            To prevent this, a <a>MediaElementAudioSourceNode</a> MUST output
            <em>silence</em> instead of the normal output of the
            <code>HTMLMediaElement</code> if it has been created using an
            <code>HTMLMediaElement</code> for which the execution of the
            <a href=
            "http://www.w3.org/html/wg/drafts/html/master/infrastructure.html#cors-enabled-fetch">
            fetch algorithm</a> labeled the resource as <a href=
            "http://www.w3.org/html/wg/drafts/html/master/infrastructure.html#cors-cross-origin">
            CORS-cross-origin</a>.
          </p>
        </section>
      </section>
      <section>
        <h2 id="AudioWorker">
          The <dfn>AudioWorker</dfn> interface
        </h2>
        <p>
          An AudioWorker object is the main-thread representation of a worker
          "thread" that supports processing of audio in Javascript. This
          AudioWorker object is a factory that is used to create multiple audio
          nodes of the same type; this enables easy sharing of code, program
          data and global state across nodes. An AudioWorker can then be used
          to create instances of <a>AudioWorkerNode</a>, which is the
          main-thread representation of an individual node processed by that
          AudioWorker.
        </p>
        <p>
          These main thread objects cause the instantiation of a Web
          Worker-like thread context (Described in [[!Workers]]), which
          executes in the audio thread. All audio processing by
          AudioWorkerNodes run in the audio processing thread. This has a few
          side effects that bear mentioning: blocking the audio worker's thread
          can cause glitches in the audio, and if the audio thread is normally
          elevated in thread priority (to reduce glitching possibility), it
          must be demoted to normal thread priority (in order to avoid
          escalating thread priority of user-supplied script code).
        </p>
        <p>
          From inside an audio worker script, the Audio Worker factory is
          represented by an <a><code>AudioWorkerGlobalScope</code></a> object
          representing the node's contextual information, and individual audio
          nodes created by the factory are represented by
          <a><code>AudioWorkerNodeProcessor</code></a> objects.
        </p>
        <p>
          In addition, all <a>AudioWorkerNode</a>s that are created by the same
          <a>AudioWorker</a> share an <a>AudioWorkerGlobalScope</a>; this can
          allow them to share context and data across nodes (for example,
          loading a single instance of a shared database used by the individual
          nodes, or sharing context in order to implement oscillator
          synchronization).
        </p>
        <dl title="interface AudioWorker : Worker" class="idl">
          <dt>
            void terminate()
          </dt>
          <dd>
            The terminate() method, when invoked, must cause the <a href=
            "http://dev.w3.org/html5/workers/#terminate-a-worker">"terminate a
            worker"</a> algorithm to be run on the worker with which the object
            is associated. This will cease any <a>AudioProcessEvent</a>s being
            dispatched inside the AudioWorker's associated
            <a>AudioWorkerGlobalScope</a>, will cause all associated
            AudioWorkerNodes to cease processing, and will cause the
            destruction of the worker thread. In practical terms, this means
            all nodes created from this AudioWorker will disconnect themselves,
            and will cease performing any useful functions.
          </dd>
          <dt>
            void postMessage(any message, optional sequence&lt;Transferable&gt;
            transfer)
          </dt>
          <dd>
            postMessage may be called to send a message to the
            <a>AudioWorkerGlobalScope</a>, via the algorithm defined by
            [[!Workers]].
          </dd>
          <dt>
            readonly attribute AudioWorkerParamDescriptor[] parameters
          </dt>
          <dd>
            This array contains descriptors for each of the current parameters
            on nodes created by this AudioWorker. This enables users of the
            AudioWorker to easily iterate over the AudioParam names and default
            values.
          </dd>
          <dt>
            attribute EventHandler onmessage
          </dt>
          <dd>
            The onmessage handler is called whenever the
            <a>AudioWorkerGlobalScope</a> posts a message back to the main
            thread.
          </dd>
          <dt>
            attribute EventHandler onloaded
          </dt>
          <dd>
            The onloaded handler is called after the script is successfully
            loaded and its global scope code is run to initialize the
            <a>AudioWorkerGlobalScope</a>.
          </dd>
          <dt>
            AudioWorkerNode createNode(int numberOfInputs, int numberOfOutputs)
          </dt>
          <dd>
            Creates a node instance in the audio worker.
          </dd>
          <dt>
            AudioParam addParameter(DOMString name, float defaultValue)
          </dt>
          <dd>
            <p>
              Causes a correspondingly-named read-only <a>AudioParam</a> to be
              present on any <a>AudioWorkerNode</a>s created (previously or
              subsequently) by this <a>AudioWorker</a>, and a
              correspondingly-named read-only <a>Float32Array</a> to be present
              on the <a><code>parameters</code></a> object exposed on the
              <a>AudioProcessEvent</a> on subsequent audio processing events
              for such nodes. The AudioParam may immediately have its
              scheduling methods called, its .<a>value</a> set, or
              <a>AudioNode</a>s connected to it.
            </p>
            <p>
              The <code>name</code> parameter is the name used for the
              read-only AudioParam added to the AudioWorkerNode, and the name
              used for the read-only <code>Float32Array</code> that will be
              present on the <a><code>parameters</code></a> object exposed on
              subsequent <a>AudioProcessEvent</a>s.
            </p>
            <p>
              The <a><code>defaultValue</code></a> parameter is the default
              value for the <a>AudioParam</a>'s <a>value</a> attribute, as well
              as therefore the default value that will appear in the
              Float32Array in the worker script (if no other parameter changes
              or connections affect the value).
            </p>
          </dd>
          <dt>
            void removeParameter(DOMString name)
          </dt>
          <dd>
            <p>
              Removes a previously-added parameter named <code>name</code> from
              all <a>AudioWorkerNode</a>s associated with this
              <a>AudioWorker</a> and its <a>AudioWorkerGlobalScope</a>. This
              will also remove the correspondingly-named read-only
              <a>AudioParam</a> from the <a>AudioWorkerNode</a>, and will
              remove the correspondingly-named read-only <a>Float32Array</a>s
              from the <a>AudioProcessEvent</a>'s
              <a><code>parameters</code></a> member on subsequent audio
              processing events. A NotFoundError exception must be thrown if no
              parameter with that name exists on this <a>AudioWorker</a>.
            </p>
            <p>
              The <code>name</code> parameter identifies the parameter to be
              removed.
            </p>
          </dd>
        </dl>
        <p>
          Note that <a>AudioWorkerNode</a> objects will also have read-only
          AudioParam objects for each named parameter added via the
          <code>addParameter</code> method. As this is dynamic, it cannot be
          captured in IDL.
        </p>
        <p>
          As the <a>AudioWorker</a> interface inherits from
          <code>Worker</code>, <a>AudioWorker</a>s must implement the
          <code>Worker</code> interface for communication with the audio worker
          script.
        </p>
        <section>
          <h2 id="AudioWorkerNode">
            The <dfn>AudioWorkerNode</dfn> Interface
          </h2>
          <p>
            This interface represents an <a><code>AudioNode</code></a> which
            interacts with a <code>Worker</code> thread to generate, process,
            or analyse audio directly. The user creates a separate audio
            processing worker script, which is hosted inside the
            AudioWorkerGlobalScope and runs inside the audio processing thread,
            rather than the main UI thread. The AudioWorkerNode represents the
            processing node in the main processing thread's node graph; the
            AudioWorkerGlobalScope represents the context in which the user's
            audio processing script is run.
          </p>
          <p>
            Nota bene that if the Web Audio implementation normally runs audio
            process at higher than normal thread priority, utilizing
            AudioWorkerNodes may cause demotion of the priority of the audio
            thread (since user scripts cannot be run with higher than normal
            priority).
          </p>
          <pre>
    numberOfInputs  : variable
    numberOfOutputs : variable

    channelCount = numberOfInputChannels;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
          <p>
            The number of input and output channels specified in the
            createAudioWorkerNode() call determines the initial number of input
            and output channels (and the number of channels present for each
            input and output in the AudioBuffers passed to the AudioProcess
            event handler inside the <a>AudioWorkerGlobalScope</a>). It is
            invalid for both <a href=
            "#AudioWorker-numberOfInputChannels"><code>numberOfInputChannels</code></a>
            and <a href=
            "#AudioWorker-numberOfOutputChannels"><code>numberOfOutputChannels</code></a>
            to be zero.
          </p>
          <p>
            Example usage:
          </p>
          <pre class="highlight">
    var bitcrusherFactory = context.createAudioWorker( "bitcrusher.js" );
    var bitcrusherNode = bitcrusherFactory.createNode();
</pre>
          <dl title="interface AudioWorkerNode : AudioNode" class="idl">
            <dt>
              void postMessage(any message, optional
              sequence&lt;Transferable&gt; transfer)
            </dt>
            <dd>
              postMessage may be called to send a message to the
              AudioWorkerNodeProcessor, via the algorithm defined by <a href=
              "http://dev.w3.org/html5/workers/#dom-worker-postmessage">the
              Worker specification</a>. Note that this is different from
              calling postMessage() on the AudioWorker itself, as that would
              affect the AudioWorkerGlobalScope.
            </dd>
            <dt>
              attribute EventHandler onmessage
            </dt>
            <dd>
              The onmessage handler is called whenever the
              AudioWorkerNodeProcessor posts a node message back to the main
              thread.
            </dd>
          </dl>
          <p>
            Note that <a>AudioWorkerNode</a> objects will also have read-only
            AudioParam objects for each named parameter added via the
            <code>addParameter</code> method on the AudioWorker. As this is
            dynamic, it cannot be captured here in IDL.
          </p>
        </section>
        <section>
          <h2 id="AudioWorkerParamDescriptor">
            The <dfn>AudioWorkerParamDescriptor</dfn> Interface
          </h2>
          <p>
            This interface represents the description of an AudioWorkerNode
            AudioParam - in short, its name and default value. This enables
            easy iteration over the AudioParams from an AudioWorkerGlobalScope
            (which does not have an instance of those AudioParams).
          </p>
          <dl title="interface AudioWorkerParamDescriptor" class="idl">
            <dt>
              readonly attribute DOMString name
            </dt>
            <dd>
              The name of the AudioParam.
            </dd>
            <dt>
              readonly attribute float defaultValue
            </dt>
            <dd>
              The default value of the AudioParam.
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            The AudioWorkerGlobalScope Interface
          </h2>
          <p>
            This interface is a <code>DedicatedWorkerGlobalScope</code>-derived
            object representing the context in which an audio processing script
            is run; it is designed to enable the generation, processing, and
            analysis of audio data directly using JavaScript in a Worker
            thread, with shared context between multiple instances of audio
            nodes. This facilitates nodes that may have substantial shared
            data, e.g. a convolution node.
          </p>-
          <p>
            The <a><code>AudioWorkerGlobalScope</code></a> handles - <dfn id=
            "audioprocess-worker">audioprocess</dfn> events dispatched -
            synchronously to process audio frame blocks for nodes created by
            this worker. - <a href=
            "#audioprocess-worker"><code>audioprocess</code></a> events are
            only - dispatched for nodes that have at least one input - or one
            output connected. TODO: should this be true?
          </p>
          <dl title=
          "interface AudioWorkerGlobalScope : DedicatedWorkerGlobalScope"
          class="idl">
            <dt>
              readonly attribute float sampleRate
            </dt>
            <dd>
              The sample rate of the host <a>AudioContext</a> (since inside the
              <code>Worker</code> scope, the user will not have direct access
              to the <a>AudioContext</a>.
            </dd>
            <dt>
              AudioParam addParameter(DOMString name, float defaultValue)
            </dt>
            <dd>
              <p>
                Causes a correspondingly-named read-only <a>AudioParam</a> to
                be present on previously-created and subsequently-created
                <a>AudioWorkerNode</a>s created by this factory, and a
                correspondingly-named read-only <a>Float32Array</a> to be
                present on the <a><code>parameters</code></a> object exposed on
                the <a>AudioProcessEvent</a> on subsequent audio processing
                events for nodes created from this factory.
              </p>
              <p>
                It is purposeful that AudioParams can be added (or removed)
                from an Audio Worker from either the control thread or the
                worker script; this enables immediate creation of worker-based
                nodes and their prototypes, but also enables packaging an
                entire worker including its AudioParam configuration into a
                single script. It is recommended that nodes be used only after
                the AudioWorkerNode's oninitialized has been called, in order
                to allow the worker script to configure the node.
              </p>
              <p>
                The <code>name</code> parameter is the name used for the
                read-only AudioParam added to the AudioWorkerNode, and the name
                used for the read-only <code>Float32Array</code> that will be
                present on the <a><code>parameters</code></a> object exposed on
                subsequent <a>AudioProcessEvent</a>s.
              </p>
              <p>
                The <dfn id="dfn-defaultValue">defaultValue</dfn> parameter is
                the default value for the <a>AudioParam</a>'s <a>value</a>
                attribute, as well as therefore the default value that will
                appear in the Float32Array in the worker script (if no other
                parameter changes or connections affect the value).
              </p>
            </dd>
            <dt>
              void removeParameter(DOMString name)
            </dt>
            <dd>
              <p>
                Removes a previously-added parameter named <code>name</code>
                from nodes processed by this factory. This will also remove the
                correspondingly-named read-only <a>AudioParam</a> from the
                <a>AudioWorkerNode</a>, and will remove the
                correspondingly-named read-only <a>Float32Array</a> from the
                <a>AudioProcessEvent</a>'s <a><code>parameters</code></a>
                member on subsequent audio processing events. A NotFoundError
                exception MUST be thrown if no parameter with that name exists
                on this node.
              </p>
              <p>
                The <code>name</code> parameter identifies the parameter to be
                removed.
              </p>
            </dd>
            <dt>
              attribute EventHandler onaudioprocess
            </dt>
            <dd>
              A property used to set the <code>EventHandler</code> (described
              in [[!HTML]]) for the <a href=
              "#audioprocess-worker"><code>audioprocess</code></a> event that
              is dispatched to <a><code>AudioWorkerGlobalScope</code></a> to
              process audio while the associated nodes are connected (to at
              least one input or output). An event of type
              <a><code>AudioProcessEvent</code></a> will be dispatched to the
              event handler.
            </dd>
            <dt>
              attribute EventHandler onnodecreate
            </dt>
            <dd>
              A property used to set the <code>EventHandler</code> (described
              in [[!HTML]]) for the <a href=
              "#audioprocess-worker"><code>nodecreate</code></a> event that is
              dispatched to <a><code>AudioWorkerGlobalScope</code></a> when a
              new <a><code>AudioWorkerNode</code></a> has been created. This
              enables the scope to do node-level initialization of the
              <a>AudioNodeProcessor</a> object. An event of type
              <a><code>AudioWorkerNodeCreationEvent</code></a> will be
              dispatched to the event handler.
            </dd>
            <dt>
              readonly attribute AudioWorkerParamDescriptor[] parameters
            </dt>
            <dd>
              This array contains descriptors for each of the current
              parameters on nodes created in this AudioWorkerGlobalScope. This
              enables audio worker implementations to easily iterate over the
              AudioParam names and default values.
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            The AudioWorkerNodeProcessor Interface
          </h2>
          <p>
            An object supporting this interface represents each individual node
            instantiated in an <a><code>AudioWorkerGlobalScope</code></a>; it
            is designed to manage the data for an individual node. Shared
            context between multiple instances of audio nodes is accessible
            from the <a>AudioWorkerGlobalScope</a>; this object represents the
            individual node and can be used for data storage or main-thread
            communication.
          </p>
          <dl title="interface AudioWorkerNodeProcessor : EventTarget" class=
          "idl">
            <dt>
              void postMessage(any message, optional
              sequence&lt;Transferable&gt; transfer)
            </dt>
            <dd>
              postMessage may be called to send a message to the
              AudioWorkerNode, via the algorithm defined by <a href=
              "http://dev.w3.org/html5/workers/#dom-worker-postmessage">the
              Worker specification</a>. Note that this is different from
              calling postMessage() on the AudioWorker itself, as that would
              dispatch to the AudioWorkerGlobalScope.
            </dd>
            <dt>
              attribute EventHandler onmessage
            </dt>
            <dd>
              The onmessage handler is called whenever the AudioWorkerNode
              posts a node message back to the audio thread.
            </dd>
          </dl>
        </section>
        <section class="informative">
          <h3>
            Audio Worker Examples
          </h3>
          <section>
            <h4>
              A Bitcrusher Node
            </h4>
            <p>
              Bitcrushing is a mechanism by which the audio quality of an audio
              stream is reduced - both by quantizing the value (simulating
              lower bit-depth in integer-based audio), and by quantizing in
              time (simulating a lower digital sample rate). This example shows
              how to use AudioParams (in this case, treated as a-rate) inside
              an AudioWorker.
            </p>
            <h5>
              Main file javascript
            </h5>
            <pre class="highlight">var bitcrusherFactory = null;
audioContext.createAudioWorker("bitcrusher_worker.js").then( function(factory) 
    {  // cache 'factory' in case you want to create more nodes!
      bitcrusherFactory = factory;
      var bitcrusherNode = factory.createNode();
      bitcrusherNode.bits.setValueAtTime(8,0);
      bitcrusherNode.connect(output); 
      input.connect(bitcrusherNode);
    }
  );</pre>
            <h5>
              bitcrusher_worker.js
            </h5>
            <pre class="highlight">
            // Custom parameter - number of bits to crush down to - default 8
this.addParameter( "bits", 8 );

// Custom parameter - frequency reduction, 0-1, default 0.5
this.addParameter( "frequencyReduction", 0.5 );

onnodecreate=function(e) {
  e.node.phaser = 0;
  e.node.lastDataValue = 0;
}

onaudioprocess= function (e) {
  for (var channel=0; channel&lt;e.inputs[0].length; channel++) {
    var inputBuffer = e.inputs[0][channel];
    var outputBuffer = e.outputs[0][channel];
    var bufferLength = inputBuffer.length;
    var bitsArray = e.parameters.bits;
    var frequencyReductionArray = e.parameters.frequencyReduction;

    for (var i=0; i&lt;bufferLength; i++) {
      var bits = bitsArray ? bitsArray[i] : 8;
      var frequencyReduction = frequencyReductionArray ? frequencyReductionArray[i] : 0.5;

      var step = Math.pow(1/2, bits);
      e.node.phaser += frequencyReduction;
      if (e.node.phaser &gt;= 1.0) {
          e.node.phaser -= 1.0;
          e.node.lastDataValue = step * Math.floor(inputBuffer[i] / step + 0.5);
      }
      outputBuffer[i] = e.node.lastDataValue;
    }
  }
};</pre>
          </section>
          <section>
            <h4>
              TODO: fix up this example. A Volume Meter and Clip Detector
            </h4>
            <p>
              Another common need is a clip-detecting volume meter. This
              example shows how to communicate basic parameters (that do not
              need AudioParam scheduling) across to a Worker, as well as
              communicating data back to the main thread. This node does not
              use any output.
            </p>
            <h5>
              Main file javascript
            </h5>
            <pre class="highlight">
function setupNodeMessaging(node) {
  // This handles communication back from the volume meter
  node.onmessage = function (event) {
    if (event.data instanceof Object ) {
      if (event.data.hasOwnProperty("clip")
        this.clip = event.data.clip;
      if (event.data.hasOwnProperty("volume")
        this.volume = event.data.volume;
    }
  }

  // Set up some default configuration parameters
  node.postMessage(
    { "smoothing": 0.9,   // Smoothing parameter
      "clipLevel": 0.9,   // Level to consider "clipping"
      "clipLag": 750,     // How long to keep "clipping" lit up after clip (ms)
      "updating": 100      // How frequently to update volume and clip param (ms)
    });

  // Set up volume and clip attributes.  These will be updated by our onmessage.
  node.volume = 0;
  node.clip = false;
}

var vuNode = null;

audioContext.createAudioWorker("vu_meter_worker.js").then( function(factory) 
    {  // cache 'factory' in case you want to create more nodes!
      vuFactory = factory;
      vuNode = factory.createNode([1], []); // we don't need an output, and let's force to mono
      setupNodeMessaging(vuNode);
    }
  );

window.requestAnimationFrame( function(timestamp) {
  if (vuNode) {
  // Draw a bar based on vuNode.volume and vuNode.clip
  }
});

</pre>
            <h5>
              vu_meter_worker.js
            </h5>
            <pre class="highlight">
// Custom parameter - number of bits to crush down to - default 8
this.addParameter( "bits", 8 );

// Custom parameter - frequency reduction, 0-1, default 0.5
this.addParameter( "frequencyReduction", 0.5 );

onnodecreate=function(e) {
  e.node.timeToNextUpdate = 0.1 * sampleRate;
  e.node.smoothing = 0.5;
  e.node.clipLevel = 0.95;
  e.node.clipLag = 1;
  e.node.updatingInterval = 150;
  // This just handles setting attribute values
  e.node.onmessage = function ( event ) {
    if (event.data instanceof Object ) {
      if (event.data.hasOwnProperty("smoothing")
        this.smoothing = event.data.smoothing;
      if (event.data.hasOwnProperty("clipLevel")
        this.clipLevel = event.data.clipLevel;
      if (event.data.hasOwnProperty("clipLag")
        this.clipLag = event.data.clipLag / 1000;  // convert to seconds
      if (event.data.hasOwnProperty("updating")    // convert to samples
        this.updatingInterval = event.data.updating * sampleRate / 1000 ;
    }
  };
}

onaudioprocess = function ( event ) {
  var buf = event.inputs[0][0];  // Node forces mono
  var bufLength = buf.length;
  var sum = 0;
  var x;

  // Do a root-mean-square on the samples: sum up the squares...
  for (var i=0; i&lt;bufLength; i++) {
    x = buf[i];
    if (Math.abs(x)&gt;=event.node.clipLevel) {
      event.node.clipping = true;
      event.node.unsentClip = true;  // Make sure, for every clip, we send a message.
      event.node.lastClip = event.playbackTime + (i/sampleRate);
    }
    sum += x * x;
  }

  // ... then take the square root of the sum.
  var rms =  Math.sqrt(sum / bufLength);

  // Now smooth this out with the smoothing factor applied
  // to the previous sample - take the max here because we
  // want "fast attack, slow release."
  event.node.volume = Math.max(rms, event.node.volume*event.node.smoothing);
  if (event.node.clipping &amp;& (!event.node.unsentClip) &amp;& (event.playbackTime &gt; (this.lastClip + clipLag)))
    event.node.clipping = false;

  // How long has it been since our last update?
  event.node.timeToNextUpdate -= event.node.last;
  if (event.node.timeToNextUpdate&lt;0) {
    event.node.timeToNextUpdate = event.node.updatingInterval;
    event.node.postMessage(
      { "volume": event.node.volume,
        "clip": event.node.clipping });
    event.node.unsentClip = false;
  }
};</pre>
          </section>
          <section>
            <h4>
              Reimplementing ChannelMerger
            </h4>
            <p>
              This worker shows how to merge inputs into a single output
              channel.
            </p>
            <h5>
              Main file javascript
            </h5>
            <pre class="highlight">
            var mergerNode = audioContext.createAudioWorker("merger_worker.js", [1,1,1,1,1,1], [6] );</pre>
            <pre class="highlight">var mergerFactory = null;

audioContext.createAudioWorker("merger_worker.js").then( function(factory) 
    {  // cache 'factory' in case you want to create more nodes!
      mergerFactory = factory;
      var merger6channelNode = factory.createNode( [1,1,1,1,1,1], [6] );
      // connect inputs and outputs here
    }
  );</pre>
            <h5>
              merger_worker.js
            </h5>
            <pre class="highlight">
onaudioprocess= function (e) {
  for (var input=0; input&lt;e,node.inputs.length; input++)
    e.node.outputs[0][input].set(e.node.inputs[input][0]);
};</pre>
          </section>
        </section>
      </section>
      <section class="informative">
        <h2>
          The ScriptProcessorNode Interface - DEPRECATED
        </h2>
        <p>
          This interface is an <a><code>AudioNode</code></a> which can
          generate, process, or analyse audio directly using JavaScript. This
          node type is deprecated, to be replaced by the
          <a>AudioWorkerNode</a>; this text is only here for informative
          purposes until implementations remove this node type.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = numberOfInputChannels;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
        <p>
          The <code>channelCountMode</code> cannot be changed from "explicit"
          and the <code>channelCount</code> cannot be changed. An attempt to
          change either of these MUST throw an InvalidStateError exception.
        </p>
        <p>
          The <a><code>ScriptProcessorNode</code></a> is constructed with a
          <dfn>bufferSize</dfn> which must be one of the following values: 256,
          512, 1024, 2048, 4096, 8192, 16384. This value controls how
          frequently the <a href="#audioprocess-spnode">audioprocess</a> event
          is dispatched and how many sample-frames need to be processed each
          call. <a href="#audioprocess-spnode"><code>audioprocess</code></a>
          events are only dispatched if the
          <a><code>ScriptProcessorNode</code></a> has at least one input or one
          output connected. Lower numbers for <a href=
          "#widl-ScriptProcessorNode-bufferSize">bufferSize</a> will result in
          a lower (better) <a href="#latency">latency</a>. Higher numbers will
          be necessary to avoid audio breakup and <a href=
          "#audio-glitching">glitches</a>. This value will be picked by the
          implementation if the bufferSize argument to
          <code>createScriptProcessor</code> is not passed in, or is set to 0.
        </p>
        <p>
          <dfn>numberOfInputChannels</dfn> and
          <dfn>numberOfOutputChannels</dfn> determine the number of input and
          output channels. It is invalid for both
          <code>numberOfInputChannels</code> and
          <code>numberOfOutputChannels</code> to be zero.
        </p>
        <pre class="highlight">
    var node = context.createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels);
</pre>
        <dl title="interface ScriptProcessorNode : AudioNode" class="idl">
          <dt>
            attribute EventHandler onaudioprocess
          </dt>
          <dd>
            A property used to set the <code>EventHandler</code> (described in
            <cite><a href=
            "https://html.spec.whatwg.org/multipage/webappapis.html#eventhandler">
            HTML</a></cite>[[!HTML]]) for the <a href=
            "#audioprocess-spnode"><code>audioprocess</code></a> event that is
            dispatched to <a><code>ScriptProcessorNode</code></a> node types.
            An event of type <a><code>AudioProcessingEvent</code></a> will be
            dispatched to the event handler.
          </dd>
          <dt>
            readonly attribute long bufferSize
          </dt>
          <dd>
            The size of the buffer (in sample-frames) which needs to be
            processed each time <code>onaudioprocess</code> is called. Legal
            values are (256, 512, 1024, 2048, 4096, 8192, 16384).
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The AudioWorkerNodeCreationEvent Interface
        </h2>
        <p>
          This is an <code>Event</code> object which is dispatched to
          <a><code>AudioWorkerGlobalScope</code></a> objects when a new node
          instance is created. This allows AudioWorkers to initialize any
          node-local data (e.g. allocating a delay or initializing local
          variables).
        </p>
        <dl title="interface AudioWorkerNodeCreationEvent : Event" class="idl">
          <dt>
            readonly attribute AudioWorkerNodeProcessor node
          </dt>
          <dd>
            The new node being created. Any node-local data storage (e.g., the
            buffer for a delay node) should be created on this object.
          </dd>
          <dt>
            readonly attribute Array inputs
          </dt>
          <dd>
            An array of channelCounts for the inputs.
          </dd>
          <dt>
            readonly attribute Array outputs
          </dt>
          <dd>
            An array of channelCounts for the outputs.
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The AudioProcessEvent Interface
        </h2>
        <p>
          This is an <code>Event</code> object which is dispatched to
          <a><code>AudioWorkerGlobalScope</code></a> objects to perform
          processing.
        </p>
        <p>
          The event handler processes audio from the input (if any) by
          accessing the audio data from the <code>inputBuffers</code>
          attribute. The audio data which is the result of the processing (or
          the synthesized data if there are no inputs) is then placed into the
          <code>outputBuffers</code>.
        </p>
        <dl title="interface AudioProcessEvent : Event" class="idl">
          <dt>
            readonly attribute double playbackTime
          </dt>
          <dd>
            The time when the audio will be played in the same time coordinate
            system as the <a><code>AudioContext</code></a>'s <a href=
            "#widl-AudioContext-currentTime">currentTime</a>.
            <code>playbackTime</code> allows for very tight synchronization
            between processing directly in JavaScript with the other events in
            the context's rendering graph.
          </dd>
          <dt>
            readonly attribute AudioWorkerNodeProcessor node
          </dt>
          <dd>
            The node to which this processing event is being dispatched. Any
            node-local data storage (e.g., the buffer for a delay node) should
            be maintained on this object.
          </dd>
          <dt>
            readonly attribute Float32Array[][] inputs
          </dt>
          <dd>
            <p>
              A readonly Array of Arrays of Float32Arrays. The top-level Array
              is organized by input; each input may contain multiple channels;
              each channel contains a Float32Array of sample data. The initial
              size of the channel array will be determined by the number of
              channels specified for that input in the createAudioWorkerNode()
              method. However, an onprocess handler may alter this number of
              channels in the input dynamically, either by adding a
              Float32Array of blocksize length (128) or by reducing the Array
              (by reducing the Array.length or by using Array.pop() or
              Array.slice(). The event object, the Array and the Float32Arrays
              will be reused by the processing system, in order to minimize
              memory churn.
            </p>
            <p>
              Any reordering performed on the Array for an input will not
              reorganize the connections to the channels for subsequent events.
            </p>
          </dd>
          <dt>
            readonly attribute Float32Array[][] outputs
          </dt>
          <dd>
            <p>
              A readonly Array of Arrays of Float32Arrays. The top-level Array
              is organized by output; each output may contain multiple
              channels; each channel contains a Float32Array of sample data.
              The initial size of the channel array will be determined by the
              number of channels specified for that output in the
              createAudioWorkerNode() method. However, an onprocess handler may
              alter this number of channels in the output dynamically, either
              by adding a Float32Array of blocksize length (128) or by reducing
              the Array (by reducing the Array.length or by using Array.pop()
              or Array.slice(). The event object, the Array and the
              Float32Arrays will be reused by the processing system, in order
              to minimize memory churn.
            </p>
            <p>
              Any reordering performed on the Array for an output will not
              reorganize the connections to the channels for subsequent events.
            </p>
          </dd>
          <dt>
            readonly attribute object parameters
          </dt>
          <dd>
            This object attribute exposes a correspondingly-named read-only
            <a>Float32Array</a> for each parameter that has been added via
            <a>addParameter</a>. As this is dynamic, this cannot be captured in
            IDL. The length of this Float32Array will correspond to the length
            of the inputBuffer. The contents of this Float32Array will be the
            values to be used for the AudioParam at the corresponding points in
            time. It is expected that this Float32Array will be reused by the
            audio engine.
          </dd>
        </dl>
      </section>
      <section class="informative">
        <h2>
          The AudioProcessingEvent Interface - DEPRECATED
        </h2>
        <p>
          This is an <code>Event</code> object which is dispatched to
          <a><code>ScriptProcessorNode</code></a> nodes. It will be removed
          when the ScriptProcessorNode is removed, as the replacement
          <a>AudioWorker</a> uses the <a>AudioProcessEvent</a>.
        </p>
        <p>
          The event handler processes audio from the input (if any) by
          accessing the audio data from the <code>inputBuffer</code> attribute.
          The audio data which is the result of the processing (or the
          synthesized data if there are no inputs) is then placed into the
          <code>outputBuffer</code>.
        </p>
        <dl title="interface AudioProcessingEvent : Event" class="idl">
          <dt>
            readonly attribute double playbackTime
          </dt>
          <dd>
            The time when the audio will be played in the same time coordinate
            system as the <a><code>AudioContext</code></a>'s <a href=
            "#widl-AudioContext-currentTime">currentTime</a>.
            <code>playbackTime</code> allows for very tight synchronization
            between processing directly in JavaScript with the other events in
            the context's rendering graph.
          </dd>
          <dt>
            readonly attribute AudioBuffer inputBuffer
          </dt>
          <dd>
            An AudioBuffer containing the input audio data. It will have a
            number of channels equal to the <code>numberOfInputChannels</code>
            parameter of the createScriptProcessor() method. This AudioBuffer
            is only valid while in the scope of the <code>onaudioprocess</code>
            function. Its values will be meaningless outside of this scope.
          </dd>
          <dt>
            readonly attribute AudioBuffer outputBuffer
          </dt>
          <dd>
            An AudioBuffer where the output audio data should be written. It
            will have a number of channels equal to the
            <code>numberOfOutputChannels</code> parameter of the
            createScriptProcessor() method. Script code within the scope of the
            <code>onaudioprocess</code> function is expected to modify the
            <code>Float32Array</code> arrays representing channel data in this
            AudioBuffer. Any script modifications to this AudioBuffer outside
            of this scope will not produce any audible effects.
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The PannerNode Interface
        </h2>
        <p>
          This interface represents a processing node which <a href=
          "#Spatialization">positions / spatializes</a> an incoming audio
          stream in three-dimensional space. The spatialization is in relation
          to the <a>AudioContext</a>'s <a><code>AudioListener</code></a>
          (<code>listener</code> attribute).
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The input of this node is either mono (1 channel) or stereo (2
          channels) and cannot be increased. Connections from nodes with fewer
          or more channels will be <a href=
          "#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a>, but a NotSupportedError MUST be thrown if an
          attempt is made to set channelCount to a value greater than 2 or if
          channelCountMode is set to "max".
        </p>
        <p>
          The output of this node is hard-coded to stereo (2 channels) and
          <em>currently</em> cannot be configured.
        </p>
        <p>
          The <a><code>PanningModelType</code></a> enum determines which
          spatialization algorithm will be used to position the audio in 3D
          space. The default is <code>"equalpower"</code>.
        </p>
        <dl title="enum PanningModelType" class="idl">
          <dt>
            equalpower
          </dt>
          <dd>
            A simple and efficient spatialization algorithm using equal-power
            panning.
          </dd>
          <dt>
            HRTF
          </dt>
          <dd>
            A higher quality spatialization algorithm using a convolution with
            measured impulse responses from human subjects. This panning method
            renders stereo output.
          </dd>
        </dl>
        <p>
          The <a><code>DistanceModelType</code></a> enum determines which
          algorithm will be used to reduce the volume of an audio source as it
          moves away from the listener. The default is "inverse".
        </p>
        <p>
          In the description of each distance model below, let \(d\) be the
          distance between the listener and the panner; \(d_{ref}\) be the
          value of the <code>refDistance</code> attribute; \(d_{max}\) be the
          value of the <code>maxDistance</code> attribute; and \(f\) be the
          value of the <code>rolloffFactor</code> attribute.
        </p>
        <dl title="enum DistanceModelType" class="idl">
          <dt>
            linear
          </dt>
          <dd>
            <p>
              A linear distance model which calculates <em>distanceGain</em>
              according to:
            </p>
            <pre>
            $$
              1 - f\frac{\max(\min(d, d_{max}), d_{ref}) - d_{ref}}{d_{max} - d_{ref}}
            $$
            </pre>
            <p>
              That is, \(d\) is clamped to the interval \([d_{ref},\,
              d_{max}]\).
            </p>
          </dd>
          <dt>
            inverse
          </dt>
          <dd>
            <p>
              An inverse distance model which calculates <em>distanceGain</em>
              according to:
            </p>
            <pre>
              $$
                \frac{d_{ref}}{d_{ref} + f (\max(d, d_{ref}) - d_{ref})}
              $$
            </pre>
            <p>
              That is, \(d\) is clamped to the interval \([d_{ref},\,
              \infty)\).
            </p>
          </dd>
          <dt>
            exponential
          </dt>
          <dd>
            <p>
              An exponential distance model which calculates
              <em>distanceGain</em> according to:
            </p>
            <pre>
              $$
                \left(\frac{\max(d, d_{ref})}{d_{ref}}\right)^{-f}
              $$
            </pre>
            <p>
              That is, \(d\) is clamped to the interval \([d_{ref},\,
              \infty)\).
            </p>
          </dd>
        </dl>
        <dl title="interface PannerNode : AudioNode" class="idl">
          <!--<dt>// Default for stereo is HRTF</dt>-->
          <dt>
            attribute PanningModelType panningModel
          </dt>
          <dd>
            Specifies the panning model used by this
            <a><code>PannerNode</code></a>. Defaults to
            <a><code>"equalpower"</code></a>.
          </dd><!--<dt> // Uses a 3D cartesian coordinate system </dt>-->
          <dt>
            void setPosition(float x, float y, float z)
          </dt>
          <dd>
            <p>
              Sets the position of the audio source relative to the
              <a><code>listener</code></a> attribute. A 3D cartesian coordinate
              system is used.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent the coordinates in
              3D space.
            </p>
            <p>
              The default value is (0,0,0)
            </p>
          </dd>
          <dt>
            void setOrientation(float x, float y, float z)
          </dt>
          <dd>
            <p>
              Describes which direction the audio source is pointing in the 3D
              cartesian coordinate space. Depending on how directional the
              sound is (controlled by the <b>cone</b> attributes), a sound
              pointing away from the listener can be very quiet or completely
              silent.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent a direction vector
              in 3D space.
            </p>
            <p>
              The default value is (1,0,0)
            </p>
          </dd>
          <dt>
            void setVelocity(float x, float y, float z)
          </dt>
          <dd>
            <p>
              Sets the velocity vector of the audio source. This vector
              controls both the direction of travel and the speed in 3D space.
              This velocity relative to the listener's velocity is used to
              determine how much doppler shift (pitch change) to apply. The
              units used for this vector is <em>meters / second</em> and is
              independent of the units used for position and orientation
              vectors.
            </p>
            <p>
              The <code>x, y, z</code> parameters describe a direction vector
              indicating direction of travel and intensity.
            </p>
            <p>
              The default value is (0,0,0)
            </p>
          </dd><!--<dt> // Distance model and attributes </dt>-->
          <dt>
            attribute DistanceModelType distanceModel
          </dt>
          <dd>
            Specifies the distance model used by this
            <a><code>PannerNode</code></a>. Defaults to
            <a><code>"inverse"</code></a>.
          </dd>
          <dt>
            attribute float refDistance
          </dt>
          <dd>
            A reference distance for reducing volume as source move further
            from the listener. The default value is 1.
          </dd>
          <dt>
            attribute float maxDistance
          </dt>
          <dd>
            The maximum distance between source and listener, after which the
            volume will not be reduced any further. The default value is 10000.
          </dd>
          <dt>
            attribute float rolloffFactor
          </dt>
          <dd>
            Describes how quickly the volume is reduced as source moves away
            from listener. The default value is 1.
          </dd><!--<dt> // Directional sound cone </dt>-->
          <dt>
            attribute float coneInnerAngle
          </dt>
          <dd>
            A parameter for directional audio sources, this is an angle, in
            degrees, inside of which there will be no volume reduction. The
            default value is 360, and the value is used modulo 360.
          </dd>
          <dt>
            attribute float coneOuterAngle
          </dt>
          <dd>
            A parameter for directional audio sources, this is an angle, in
            degrees, outside of which the volume will be reduced to a constant
            value of <a><code>coneOuterGain</code></a>. The default value is
            360 and the value is used modulo 360.
          </dd>
          <dt>
            attribute float coneOuterGain
          </dt>
          <dd>
            A parameter for directional audio sources, this is the gain outside
            of the <a><code>coneOuterAngle</code></a>. The default value is 0.
            It is a linear value (not dB) in the range [0, 1]. An
            InvalidStateError MUST be thrown if the parameter is outside this
            range.
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="AudioListener">
          The AudioListener Interface
        </h2>
        <p>
          This interface interface is DEPRECATED, as it will be replaced by the
          <a><code>SpatialListener</code></a>. This interface represents the
          position and orientation of the person listening to the audio scene.
          All <a><code>PannerNode</code></a> objects spatialize in relation to
          the <a><code>AudioContext</code></a>'s <a href=
          "widl-AudioContext-listener"><code>listener</code></a>. See <a href=
          "#Spatialization">the Spatialization/Panning section</a> for more
          details about spatialization.
        </p>
        <dl title="interface AudioListener" class="idl">
          <dt>
            void setPosition(float x, float y, float z)
          </dt>
          <dd>
            <p>
              Sets the position of the listener in a 3D cartesian coordinate
              space. <a><code>PannerNode</code></a> objects use this position
              relative to individual audio sources for spatialization.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent the coordinates in
              3D space.
            </p>
            <p>
              The default value is (0,0,0)
            </p>
          </dd>
          <dt>
            void setOrientation(float x, float y, float z, float xUp, float
            yUp, float zUp)
          </dt>
          <dd>
            <p>
              Describes which direction the listener is pointing in the 3D
              cartesian coordinate space. Both a <b>front</b> vector and an
              <b>up</b> vector are provided. In simple human terms, the
              <b>front</b> vector represents which direction the person's nose
              is pointing. The <b>up</b> vector represents the direction the
              top of a person's head is pointing. These values are expected to
              be linearly independent (at right angles to each other). For
              normative requirements of how these values are to be interpreted,
              see the <a href="#Spatialization">spatialization section</a>.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent a <b>front</b>
              direction vector in 3D space, with the default value being
              (0,0,-1).
            </p>
            <p>
              The <code>xUp, yUp, zUp</code> parameters represent an <b>up</b>
              direction vector in 3D space, with the default value being
              (0,1,0).
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The SpatialPannerNode Interface
        </h2>
        <p>
          This interface represents a processing node which <a href=
          "#Spatialization">positions</a> an incoming audio stream in
          three-dimensional space. The spatialization is in relation to the
          <a>AudioContext</a>'s <a><code>SpatialListener</code></a>
          (<code>listener</code> attribute).
        </p>
        <p>
          It should be explicitly noticed that the auditory effects of this
          spatialization may not work well unless the SpatialPanner is directly
          connected to the destination node; subsequent processing (after the
          SpatialPanner, before the destination) may disrupt the effects.
        </p>
        <pre>    numberOfInputs  : 1
    numberOfOutputs : 1
    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";</pre>
        <p>
          The input of this node is either mono (1 channel) or stereo (2
          channels) and cannot be increased. Connections from nodes with more
          channels will be <a href=
          "#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a>, but a NotSupportedError MUST be thrown if an
          attempt is made to set channelCount to a value greater than 2 or if
          channelCountMode is set to "max". The output of this node will be
          stereo (2 channels) and currently cannot be configured.
        </p>
        <p>
          The <a><code>PanningModelType</code></a> enum determines which
          spatialization algorithm will be used to position the audio in 3D
          space. The default is <code>"equal-power"</code>.
        </p>
        <dl title="enum PanningModelType" class="idl">
          <dt>
            equalpower
          </dt>
          <dd>
            A simple and efficient spatialization algorithm using equal-power
            panning.
          </dd>
          <dt>
            HRTF
          </dt>
          <dd>
            A higher quality spatialization algorithm using a convolution with
            measured impulse responses from human subjects. This panning method
            renders stereo output.
          </dd>
        </dl>
        <p>
          The <a><code>DistanceModelType</code></a> enum determines which
          algorithm will be used to reduce the volume of an audio source as it
          moves away from the listener. The default is "inverse".
        </p>
        <dl title="enum DistanceModelType" class="idl">
          <dt>
            linear
          </dt>
          <dd>
            A linear distance model which calculates <em>distanceGain</em>
            according to:
            <pre>
            1 - rolloffFactor * (distance - refDistance) / (maxDistance - refDistance)</pre>
          </dd>
          <dt>
            inverse
          </dt>
          <dd>
            An inverse distance model which calculates <em>distanceGain</em>
            according to:
            <pre>
            refDistance / (refDistance + rolloffFactor * (distance - refDistance))</pre>
          </dd>
          <dt>
            exponential
          </dt>
          <dd>
            An exponential distance model which calculates
            <em>distanceGain</em> according to:
            <pre>pow(distance / refDistance, -rolloffFactor)</pre>
          </dd>
        </dl>
        <dl title="interface SpatialPannerNode : AudioNode" class="idl">
          <dt>
            attribute PanningModelType panningModel
          </dt>
          <dd>
            Specifies the panning model used by this
            <a><code>PannerNode</code></a>. Defaults to
            <a><code>"equal-power"</code></a>.
          </dd>
          <dt>
            readonly attribute AudioParam positionX
          </dt>
          <dd>
            Sets the x coordinate position of the audio source in a 3D
            Cartesian system. The default value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam positionY
          </dt>
          <dd>
            Sets the y coordinate position of the audio source in a 3D
            Cartesian system. The default value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam positionZ
          </dt>
          <dd>
            Sets the z coordinate position of the audio source in a 3D
            Cartesian system. The default value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam orientationX
          </dt>
          <dd>
            The <code>orientationX, orientationY, orientationZ</code>
            parameters represent a direction vector in 3D space.
          </dd>
          <dd>
            Describes the x component of the vector of the direction the audio
            source is pointing in 3D Cartesian coordinate space. Depending on
            how directional the sound is (controlled by the <b>cone</b>
            attributes), a sound pointing away from the listener can be very
            quiet or completely silent. The default value is 1. This parameter
            is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam orientationY
          </dt>
          <dd>
            Describes the y component of the vector of the direction the audio
            source is pointing in 3D cartesian coordinate space. The default
            value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam orientationZ
          </dt>
          <dd>
            Describes the Z component of the vector of the direction the audio
            source is pointing in 3D cartesian coordinate space. The default
            value is 0. This parameter is a-rate.
          </dd>
          <dt>
            attribute DistanceModelType distanceModel
          </dt>
          <dd>
            Specifies the distance model used by this
            <a><code>PannerNode</code></a>. Defaults to
            <a><code>"inverse"</code></a>.
          </dd>
          <dt>
            attribute float refDistance
          </dt>
          <dd>
            A reference distance for reducing volume as source move further
            from the listener. The default value is 1.
          </dd>
          <dt>
            attribute float maxDistance
          </dt>
          <dd>
            The maximum distance between source and listener, after which the
            volume will not be reduced any further. The default value is 10000.
          </dd>
          <dt>
            attribute float rolloffFactor
          </dt>
          <dd>
            Describes how quickly the volume is reduced as source moves away
            from listener. The default value is 1.
          </dd>
          <dt>
            attribute float coneInnerAngle
          </dt>
          <dd>
            A parameter for directional audio sources, this is an angle, in
            degrees, inside of which there will be no volume reduction. The
            default value is 360.
          </dd>
          <dt>
            attribute float coneOuterAngle
          </dt>
          <dd>
            A parameter for directional audio sources, this is an angle, in
            degrees, outside of which the volume will be reduced to a constant
            value of <a><code>coneOuterGain</code></a>. The default value is
            360.
          </dd>
          <dt>
            attribute float coneOuterGain
          </dt>
          <dd>
            A parameter for directional audio sources, this is the amount of
            volume reduction outside of the <a><code>coneOuterAngle</code></a>.
            The default value is 0.
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="SpatialListener">
          The SpatialListener Interface
        </h2>
        <p>
          This interface represents the position and orientation of the person
          listening to the audio scene. All
          <a><code>SpatialPannerNode</code></a> objects spatialize in relation
          to the <a><code>AudioContext</code></a>'s <a href=
          "widl-AudioContext-spatialListener"><code>spatialListener</code></a>.
          See <a href="#Spatialization">the Spatialization/Panning section</a>
          for more details about spatialization.
        </p>
        <dl title="interface SpatialListener" class="idl">
          <dt>
            readonly attribute AudioParam positionX
          </dt>
          <dd>
            Sets the x coordinate position of the audio listener in a 3D
            Cartesian coordinate space. <a><code>SpatialPannerNode</code></a>
            objects use this position relative to individual audio sources for
            spatialization. The default value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam positionY
          </dt>
          <dd>
            Sets the y coordinate position of the audio listener in a 3D
            Cartesian coordinate space. The default value is 0. This parameter
            is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam positionZ
          </dt>
          <dd>
            Sets the z coordinate position of the audio listener in a 3D
            Cartesian coordinate space. The default value is 0. This parameter
            is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam forwardX
          </dt>
          <dd>
            The <code>forwardX, forwardY, forwardZ</code> parameters represent
            a direction vector in 3D space. Both a <code>forward</code> vector
            and an <code>up</code> vector are used to determine the orientation
            of the listener. In simple human terms, the <code>forward</code>
            vector represents which direction the person's nose is pointing.
            The <code>up</code> vector represents the direction the top of a
            person's head is pointing. These values are expected to be linearly
            independent (at right angles to each other), and unpredictable
            behavior may result if they are not. For normative requirements of
            how these values are to be interpreted, see the <a href=
            "#Spatialization">spatialization section</a>.
          </dd>
          <dd>
            Sets the x coordinate component of the forward direction the
            listener is pointing in 3D Cartesian coordinate space. The default
            value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam forwardY
          </dt>
          <dd>
            Sets the y coordinate component of the forward direction the
            listener is pointing in 3D Cartesian coordinate space. The default
            value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam forwardZ
          </dt>
          <dd>
            Sets the z coordinate component of the forward direction the
            listener is pointing in 3D Cartesian coordinate space. The default
            value is 0. This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam upX
          </dt>
          <dd>
            The <code>upX, upY, upZ</code> parameters represent a direction
            vector in 3D space, indicating the direction of "up" to the
            listener. For normative requirements of how these values are to be
            interpreted, see the <a href="#Spatialization">spatialization
            section</a>.
          </dd>
          <dd>
            Sets the x coordinate component of the up direction the listener is
            pointing in 3D Cartesian coordinate space. The default value is 0.
            This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam upY
          </dt>
          <dd>
            Sets the y coordinate component of the up direction the listener is
            pointing in 3D Cartesian coordinate space. The default value is 0.
            This parameter is a-rate.
          </dd>
          <dt>
            readonly attribute AudioParam upZ
          </dt>
          <dd>
            Sets the z coordinate component of the up direction the listener is
            pointing in 3D Cartesian coordinate space. The default value is 0.
            This parameter is a-rate.
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The StereoPannerNode Interface
        </h2>
        <p>
          This interface represents a processing node which positions an
          incoming audio stream in a stereo image using a low-cost <a href=
          "#equal-power-panning">equal-power panning algorithm</a>. This
          panning effect is common in positioning audio components in a stereo
          stream.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The input of this node is stereo (2 channels) and cannot be
          increased. Connections from nodes with fewer or more channels will be
          <a href="#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a> , but a NotSupportedError will be thrown if an
          attempt is made to set <code>channelCount</code> to a value great
          than 2 or if <code>channelCountMode</code> is set to
          <code>"max"</code>.
        </p>
        <p>
          The output of this node is hard-coded to stereo (2 channels) and
          cannot be configured.
        </p>
        <dl title="interface StereoPannerNode : AudioNode" class="idl">
          <dt>
            readonly attribute AudioParam pan
          </dt>
          <dd>
            The position of the input in the output's stereo image. -1
            represents full left, +1 represents full right. Its default value
            is 0, and its nominal range is from -1 to 1. This parameter is a
            <a>a-rate</a>.
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="ConvolverNode">
          The ConvolverNode Interface
        </h2>
        <p>
          This interface represents a processing node which applies a <a href=
          "#Convolution">linear convolution effect</a> given an impulse
          response. Normative requirements for multi-channel convolution
          matrixing are described <a href=
          "#Convolution-reverb-effect">here</a>.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The input of this node is either mono (1 channel) or stereo (2
          channels) and cannot be increased. Connections from nodes with fewer
          or more channels will be <a href=
          "#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a>, but a NotSupportedError MUST be thrown if an
          attempt is made to set channelCount to a value great than 2 or if
          channelCountMode is set to "max".
        </p>
        <dl title="interface ConvolverNode : AudioNode" class="idl">
          <dt>
            attribute AudioBuffer? buffer
          </dt>
          <dd>
            A mono, stereo, or 4-channel <a><code>AudioBuffer</code></a>
            containing the (possibly multi-channel) impulse response used by
            the <a><code>ConvolverNode</code></a>. The <code>AudioBuffer</code>
            must have 1, 2, or 4 channels or a NotSupportedError exception MUST
            be thrown. This <a><code>AudioBuffer</code></a> must be of the same
            sample-rate as the <a><code>AudioContext</code></a> or a
            NotSupportedError exception MUST be thrown. At the time when this
            attribute is set, the <em>buffer</em> and the state of the
            <em>normalize</em> attribute will be used to configure the
            <a><code>ConvolverNode</code></a> with this impulse response having
            the given normalization. The initial value of this attribute is
            null.
          </dd>
          <dt>
            attribute boolean normalize
          </dt>
          <dd>
            <p>
              Controls whether the impulse response from the buffer will be
              scaled by an equal-power normalization when the
              <code>buffer</code> atttribute is set. Its default value is
              <code>true</code> in order to achieve a more uniform output level
              from the convolver when loaded with diverse impulse responses. If
              <code>normalize</code> is set to <code>false</code>, then the
              convolution will be rendered with no pre-processing/scaling of
              the impulse response. Changes to this value do not take effect
              until the next time the <em>buffer</em> attribute is set.
            </p>
            <p>
              If the <em>normalize</em> attribute is false when the
              <em>buffer</em> attribute is set then the
              <a><code>ConvolverNode</code></a> will perform a linear
              convolution given the exact impulse response contained within the
              <em>buffer</em>.
            </p>
            <p>
              Otherwise, if the <em>normalize</em> attribute is true when the
              <em>buffer</em> attribute is set then the
              <a><code>ConvolverNode</code></a> will first perform a scaled
              RMS-power analysis of the audio data contained within
              <em>buffer</em> to calculate a <em>normalizationScale</em> given
              this algorithm:
            </p>
            <pre class="highlight">

float calculateNormalizationScale(buffer)
{
    const float GainCalibration = 0.00125;
    const float GainCalibrationSampleRate = 44100;
    const float MinPower = 0.000125;

    // Normalize by RMS power.
    size_t numberOfChannels = buffer-&gt;numberOfChannels();
    size_t length = buffer-&gt;length();

    float power = 0;

    for (size_t i = 0; i &lt; numberOfChannels; ++i) {
        float* sourceP = buffer-&gt;channel(i)-&gt;data();
        float channelPower = 0;

        int n = length;
        while (n--) {
            float sample = *sourceP++;
            channelPower += sample * sample;
        }

        power += channelPower;
    }

    power = sqrt(power / (numberOfChannels * length));

    // Protect against accidental overload.
    if (isinf(power) || isnan(power) || power &lt; MinPower)
        power = MinPower;

    float scale = 1 / power;

    // Calibrate to make perceived volume same as unprocessed.
    scale *= GainCalibration;

    // Scale depends on sample-rate.
    if (buffer-&gt;sampleRate())
        scale *= GainCalibrationSampleRate / buffer-&gt;sampleRate();

    // True-stereo compensation.
    if (buffer-&gt;numberOfChannels() == 4)
        scale *= 0.5;

    return scale;
}
      </pre>
            <p>
              During processing, the ConvolverNode will then take this
              calculated <em>normalizationScale</em> value and multiply it by
              the result of the linear convolution resulting from processing
              the input with the impulse response (represented by the
              <em>buffer</em>) to produce the final output. Or any
              mathematically equivalent operation may be used, such as
              pre-multiplying the input by <em>normalizationScale</em>, or
              pre-multiplying a version of the impulse-response by
              <em>normalizationScale</em>.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The AnalyserNode Interface
        </h2>
        <p>
          This interface represents a node which is able to provide real-time
          frequency and time-domain analysis information. The audio stream will
          be passed un-processed from input to output.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1    <em>Note that this output may be left unconnected.</em>

    channelCount = 1;
    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <dl title="interface AnalyserNode : AudioNode" class="idl">
          <!--<dt> // Real-time frequency-domain data </dt>-->
          <dt>
            void getFloatFrequencyData()
          </dt>
          <dd>
            <p>
              Copies the <a>current frequency data</a> into the passed
              floating-point array. If the array has fewer elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be dropped. If the array has more elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be ignored.
            </p>
            <p>
              The frequency data are in dB units.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array array
              </dt>
              <dd>
                This parameter is where the frequency-domain analysis data will
                be copied.
              </dd>
            </dl>
          </dd>
          <dt>
            void getByteFrequencyData()
          </dt>
          <dd>
            <p>
              Copies the <a>current frequency data</a> into the passed unsigned
              byte array. If the array has fewer elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be dropped. If the array has more elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be ignored.
            </p>
            <p>
              The values stored in the unsigned byte array are computed in the
              following way. Let \(Y[k]\) be the <a>current frequency data</a>
              as described in <a href=
              "#fft-windowing-and-smoothing-over-time">FFT windowing and
              smoothing</a>. Then the byte value, \(b[k]\), is
            </p>
            <pre>
              $$
                b[k] = \frac{255}{\mbox{dB}_{max} - \mbox{dB}_{min}}
                 \left(Y[k] - \mbox{dB}_{min}\right)
              $$
            </pre>
            <p>
              where \(\mbox{dB}_{min}\) is <code><a>minDecibels</a></code> and
              \(\mbox{dB}_{max}\) is <code><a>maxDecibels</a></code>. If
              \(b[k]\) lies outside the range of 0 to 255, \(b[k]\) is clipped
              to lie in that range.
            </p>
            <dl class="parameters">
              <dt>
                Uint8Array array
              </dt>
              <dd>
                This parameter is where the frequency-domain analysis data will
                be copied.
              </dd>
            </dl>
          </dd><!--<dt>// Real-time waveform data </dt>-->
          <dt>
            void getFloatTimeDomainData()
          </dt>
          <dd>
            <p>
              Copies the current time-domain (waveform) data into the passed
              floating-point array. If the array has fewer elements than the
              value of <a><code>fftSize</code></a>, the excess elements will be
              dropped. If the array has more elements than
              <a><code>fftSize</code></a>, the excess elements will be ignored.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array array
              </dt>
              <dd>
                This parameter is where the time-domain sample data will be
                copied.
              </dd>
            </dl>
          </dd>
          <dt>
            void getByteTimeDomainData()
          </dt>
          <dd>
            <p>
              Copies the current time-domain (waveform) data into the passed
              unsigned byte array. If the array has fewer elements than the
              value of <a><code>fftSize</code></a>, the excess elements will be
              dropped. If the array has more elements than
              <a><code>fftSize</code></a>, the excess elements will be ignored.
            </p>
            <p>
              The values stored in the unsigned byte array are computed in the
              following way. Let \(x[k]\) be the time-domain data. Then the
              byte value, \(b[k]\), is
            </p>
            <pre>
              $$
                b[k] = 128(1 + x[k]).
              $$
           </pre>
            <p>
              If \(b[k]\) lies outside the range 0 to 255, \(b[k]\) is clipped
              to lie in that range.
            </p>
            <dl class="parameters">
              <dt>
                Uint8Array array
              </dt>
              <dd>
                This parameter is where the time-domain sample data will be
                copied.
              </dd>
            </dl>
          </dd>
          <dt>
            attribute unsigned long fftSize
          </dt>
          <dd>
            The size of the FFT used for frequency-domain analysis. This must
            be a power of two in the range 32 to 32768, otherwise an
            IndexSizeError exception MUST be thrown. The default value is 2048.
            Note that large FFT sizes can be costly to compute.
          </dd>
          <dt>
            readonly attribute unsigned long frequencyBinCount
          </dt>
          <dd>
            Half the FFT size.
          </dd>
          <dt>
            attribute float minDecibels
          </dt>
          <dd>
            <dfn id="minDecibels">minDecibels</dfn> is the minimum power value
            in the scaling range for the FFT analysis data for conversion to
            unsigned byte values. The default value is -100. If the value of
            this attribute is set to a value more than or equal to
            <code><a>maxDecibels</a></code>, an IndexSizeError exception MUST
            be thrown.
          </dd>
          <dt>
            attribute float maxDecibels
          </dt>
          <dd>
            <dfn id="maxDecibels">maxDecibels</dfn> is the maximum power value
            in the scaling range for the FFT analysis data for conversion to
            unsigned byte values. The default value is -30. If the value of
            this attribute is set to a value less than or equal to
            <code><a>minDecibels</a></code>, an IndexSizeError exception MUST
            be thrown.
          </dd>
          <dt>
            attribute float smoothingTimeConstant
          </dt>
          <dd>
            A value from 0 -&gt; 1 where 0 represents no time averaging with
            the last analysis frame. The default value is 0.8. If the value of
            this attribute is set to a value less than 0 or more than 1, an
            IndexSizeError exception MUST be thrown.
          </dd>
        </dl>
        <section>
          <h3>
            FFT Windowing and smoothing over time
          </h3>When the <dfn id="current-frequency-data">current frequency
          data</dfn> are computed, the following operations are to be
          performed:
          <ol>
            <li>
              <a href="#blackman-window">Apply a Blackman window</a> to the
              time domain input data
            </li>
            <li>
              <a href="#fourier-transform">Apply a Fourier tranform</a> to the
              windowed time domain input data to get imaginary and real
              frequency data
            </li>
            <li>
              <a href="#smoothing-over-time">Smooth over time</a> the frequency
              domain data
            </li>
            <li>
              <a href="#conversion-to-db">Conversion to dB</a>.
            </li>
          </ol>
          <p>
            In the following, let \(N\) be the value of the
            <code>.fftSize</code> attribute of this <code>AnalyserNode</code>.
          </p>
          <p>
            <dfn id="blackman-window">Applying a Blackman window</dfn> consists
            in the following operation on the input time domain data. Let
            \(x[n]\) for \(n = 0, \ldots, N - 1\) be the time domain data. The
            Blackman window is defined by
          </p>
          <pre>
          $$
          \begin{align*}
            \alpha &amp;= \mbox{0.16} \\ a_0 &amp;= \frac{1-\alpha}{2} \\
             a_1   &amp;= \frac{1}{2} \\
             a_2   &amp;= \frac{\alpha}{2} \\
             w[n] &amp;= a_0 - a_1 \cos\frac{2\pi n}{N} + a_2 \cos\frac{4\pi n}{N}, \mbox{ for } n = 0, \ldots, N - 1
           \end{align*}
           $$
          </pre>
          <p>
            The windowed signal \(\hat{x}[n]\) is
          </p>
          <pre>
            $$
              \hat{x}[n] = x[n] w[n], \mbox{ for } n = 0, \ldots, N - 1
            $$
          </pre>
          <p>
            <dfn id="fourier-transform">Applying a Fourier tranform</dfn>
            consists of computing the Fourier transform in the following way.
            Let \(X[k]\) be the complex frequency domain data and
            \(\hat{x}[n]\) be the windowed time domain data computed above.
            Then
          </p>
          <pre>
            $$
              X[k] = \sum_{n = 0}^{N - 1} \hat{x}[n] e^{\frac{-2\pi i k n}{N}}
            $$
          </pre>
          <p>
            for \(k = 0, \dots, N/2-1\).
          </p>
          <p>
            <dfn id="smoothing-over-time">Smoothing over time</dfn> frequency
            data consists in the following operation:
          </p>
          <ul>
            <li>Let \(\hat{X}_{-1}[k]\) be the result of this operation on the
            <a>previous block</a>. The <dfn>previous block</dfn> is defined as
            being the buffer computed by the previous <a href=
            "#smoothing-over-time">smoothing over time</a> operation, or an
            array of \(N\) zeros if this is the first time we are <a href=
            "#smoothing-over-time">smoothing over time</a>.
            </li>
            <li>Let \(\tau\) be the value of the <a href=
            "#widl-AnalyserNode-smoothingTimeConstant"><code>smoothingTimeConstant</code></a>
            attribute for this <a><code>AnalyserNode</code></a>.
            </li>
            <li>Let \(X[k]\) be the result of <a href=
            "#fourier-transform">applying a Fourier transform</a> of the
            current block.
            </li>
          </ul>
          <p>
            Then the smoothed value, \(\hat{X}[k]\), is computed by
          </p>
          <pre>
            $$
              \hat{X}[k] = \tau\, \hat{X}_{-1}[k] + (1 - \tau)\, |X[k]|
            $$
          </pre>
          <p>
            for \(k = 0, \ldots, N - 1\).
          </p>
          <p>
            <dfn id="conversion-to-db">Conversion to dB</dfn> consists of the
            following operation, where \(\hat{X}[k]\) is computed in <a href=
            "#smoothing-over-time">smoothing over time</a>:
          </p>
          <pre>
          $$
            Y[k] = 20\log_{10}\hat{X}[k]
          $$
          </pre>
          <p>
            for \(k = 0, \ldots, N-1\).
          </p>
          <p>
            This array, \(Y[k]\), is copied to the output array for
            <code>getFloatFrequencyData</code>. For
            <code>getByteFrequencyData</code>, the \(Y[k]\) is clipped to lie
            between <code><a>minDecibels</a></code> and
            <code><a>maxDecibels</a></code> and then scaled to fit in an
            unsigned byte such that <code><a>minDecibels</a></code> is
            represented by the value 0 and <code><a>maxDecibels</a></code> is
            represented by the value 255.
          </p>
        </section>
      </section>
      <section>
        <h2>
          The ChannelSplitterNode Interface
        </h2>
        <p>
          The <code>ChannelSplitterNode</code> is for use in more advanced
          applications and would often be used in conjunction with
          <a><code>ChannelMergerNode</code></a>.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : Variable N (defaults to 6) // number of "active" (non-silent) outputs is determined by number of channels in the input

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          This interface represents an <a><code>AudioNode</code></a> for
          accessing the individual channels of an audio stream in the routing
          graph. It has a single input, and a number of "active" outputs which
          equals the number of channels in the input audio stream. For example,
          if a stereo input is connected to an
          <a><code>ChannelSplitterNode</code></a> then the number of active
          outputs will be two (one from the left channel and one from the
          right). There are always a total number of N outputs (determined by
          the <code>numberOfOutputs</code> parameter to the
          <a><code>AudioContext</code></a> method <a href=
          "#widl-AudioContext-createChannelSplitter-ChannelSplitterNode-unsigned-long-numberOfOutputs">
          <code>createChannelSplitter()</code></a>), The default number is 6 if
          this value is not provided. Any outputs which are not "active" will
          output silence and would typically not be connected to anything.
        </p>
        <h3>
          Example:
        </h3>
        <figure>
          <img alt="channel splitter" src="images/channel-splitter.png">
          <figcaption>
            A diagram of a ChannelSplitter
          </figcaption>
        </figure>
        <p>
          Please note that in this example, the splitter does <b>not</b>
          interpret the channel identities (such as left, right, etc.), but
          simply splits out channels in the order that they are input.
        </p>
        <p>
          One application for <code>ChannelSplitterNode</code> is for doing
          "matrix mixing" where individual gain control of each channel is
          desired.
        </p>
        <dl title="interface ChannelSplitterNode : AudioNode" class="idl"></dl>
      </section>
      <section>
        <h2>
          The ChannelMergerNode Interface
        </h2>
        <p>
          The <a><code>ChannelMergerNode</code></a> is for use in more advanced
          applications and would often be used in conjunction with
          <a><code>ChannelSplitterNode</code></a>.
        </p>
        <pre>
  numberOfInputs  : Variable N (default to 6)
  numberOfOutputs : 1

  channelCount = 1;
  channelCountMode = "explicit";
  channelInterpretation = "speakers";
</pre>
        <p>
          This interface represents an <a><code>AudioNode</code></a> for
          combining channels from multiple audio streams into a single audio
          stream. It has a variable number of inputs (defaulting to 6), but not
          all of them need be connected. There is a single output whose audio
          stream has a number of channels equal to the number of inputs.
        </p>
        <p>
          To merge multiple inputs into one stream, each input gets downmixed
          into one channel (mono) based on the specified mixing rule. An
          unconnected input still counts as <b>one silent channel</b> in the
          output. Changing input streams does <b>not</b> affect the order of
          output channels.
        </p>
        <p>
          For <a><code>ChannelMergerNode</code></a>, <code>channelCount</code>
          and <code>channelCountMode</code> properties cannot be changed.
          <code>InvalidState</code> error MUST be thrown when they changed.
        </p>
        <h3 id="example-2">
          Example:
        </h3>
        <p>
          For example, if a default <a><code>ChannelMergerNode</code></a> has
          two connected stereo inputs, the first and second input will be
          downmixed to mono respectively before merging. The output will be a
          6-channel stream whose first two channels are be filled with the
          first two (downmixed) inputs and the rest of channels will be silent.
        </p>
        <p>
          Also the <a><code>ChannelMergerNode</code></a> can be used to arrange
          multiple audio streams in a certain order for the multi-channel
          speaker array such as 5.1 surround set up. The merger does not
          interpret the channel identities (such as left, right, etc.), but
          simply combines channels in the order that they are input.
        </p>
        <figure>
          <img alt="channel merger" src="images/channel-merger.png">
          <figcaption>
            A diagram of ChannelMerger
          </figcaption>
        </figure>
        <dl title="interface ChannelMergerNode : AudioNode" class="idl"></dl>
      </section>
      <section>
        <h2>
          The DynamicsCompressorNode Interface
        </h2>
        <p>
          <a><code>DynamicsCompressorNode</code></a> is an
          <a><code>AudioNode</code></a> processor implementing a dynamics
          compression effect.
        </p>
        <p>
          Dynamics compression is very commonly used in musical production and
          game audio. It lowers the volume of the loudest parts of the signal
          and raises the volume of the softest parts. Overall, a louder,
          richer, and fuller sound can be achieved. It is especially important
          in games and musical applications where large numbers of individual
          sounds are played simultaneous to control the overall signal level
          and help avoid clipping (distorting) the audio output to the
          speakers.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
        <dl title="interface DynamicsCompressorNode : AudioNode" class="idl">
          <dt>
            readonly attribute AudioParam threshold
          </dt>
          <dd>
            The decibel value above which the compression will start taking
            effect. Its default <code>value</code> is -24, with a nominal range
            of -100 to 0.
          </dd>
          <dt>
            readonly attribute AudioParam knee
          </dt>
          <dd>
            A decibel value representing the range above the threshold where
            the curve smoothly transitions to the "ratio" portion. Its default
            <code>value</code> is 30, with a nominal range of 0 to 40.
          </dd>
          <dt>
            readonly attribute AudioParam ratio
          </dt>
          <dd>
            The amount of dB change in input for a 1 dB change in output. Its
            default <code>value</code> is 12, with a nominal range of 1 to 20.
          </dd>
          <dt>
            readonly attribute float reduction
          </dt>
          <dd>
            A read-only decibel value for metering purposes, representing the
            current amount of gain reduction that the compressor is applying to
            the signal. If fed no signal the value will be 0 (no gain
            reduction).
          </dd>
          <dt>
            readonly attribute AudioParam attack
          </dt>
          <dd>
            The amount of time (in seconds) to reduce the gain by 10dB. Its
            default <code>value</code> is 0.003, with a nominal range of 0 to
            1.
          </dd>
          <dt>
            readonly attribute AudioParam release
          </dt>
          <dd>
            The amount of time (in seconds) to increase the gain by 10dB. Its
            default <code>value</code> is 0.250, with a nominal range of 0 to
            1.
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The BiquadFilterNode Interface
        </h2>
        <p>
          <a><code>BiquadFilterNode</code></a> is an
          <a><code>AudioNode</code></a> processor implementing very common
          low-order filters.
        </p>
        <p>
          Low-order filters are the building blocks of basic tone controls
          (bass, mid, treble), graphic equalizers, and more advanced filters.
          Multiple <a><code>BiquadFilterNode</code></a> filters can be combined
          to form more complex filters. The filter parameters such as <a href=
          "#widl-BiquadFilterNode-freuqnecy"><code>frequency</code></a> can be
          changed over time for filter sweeps, etc. Each
          <a><code>BiquadFilterNode</code></a> can be configured as one of a
          number of common filter types as shown in the IDL below. The default
          filter type is <code>"lowpass"</code>.
        </p>
        <p>
          Both <a href=
          "#widl-BiquadFilterNode-frequency"><code>frequency</code></a> and
          <a href="#widl-BiquadFilterNode-detune"><code>detune</code></a> are
          <a>a-rate</a> parameters and are used together to determine a
          <dfn id="computedFreq-biquad">computedFrequency</dfn> value:
        </p>
        <pre class="highlight">
  computedFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)
</pre>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <dl title="enum BiquadFilterType" class="idl">
          <dt>
            lowpass
          </dt>
          <dd>
            <p>
              A <a href="https://en.wikipedia.org/wiki/Low-pass_filter">lowpass
              filter</a> allows frequencies below the cutoff frequency to pass
              through and attenuates frequencies above the cutoff. It
              implements a standard second-order resonant lowpass filter with
              12dB/octave rolloff.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The cutoff frequency
                </dd>
                <dt>
                  Q
                </dt>
                <dd>
                  Controls how peaked the response will be at the cutoff
                  frequency. A large value makes the response more peaked.
                  Please note that for this filter type, this value is not a
                  traditional Q, but is a resonance value in decibels.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            highpass
          </dt>
          <dd>
            <p>
              A <a href=
              "https://en.wikipedia.org/wiki/High-pass_filter">highpass
              filter</a> is the opposite of a lowpass filter. Frequencies above
              the cutoff frequency are passed through, but frequencies below
              the cutoff are attenuated. It implements a standard second-order
              resonant highpass filter with 12dB/octave rolloff.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The cutoff frequency below which the frequencies are
                  attenuated
                </dd>
                <dt>
                  Q
                </dt>
                <dd>
                  Controls how peaked the response will be at the cutoff
                  frequency. A large value makes the response more peaked.
                  Please note that for this filter type, this value is not a
                  traditional Q, but is a resonance value in decibels.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            bandpass
          </dt>
          <dd>
            <p>
              A <a href=
              "https://en.wikipedia.org/wiki/Band-pass_filter">bandpass
              filter</a> allows a range of frequencies to pass through and
              attenuates the frequencies below and above this frequency range.
              It implements a second-order bandpass filter.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The center of the frequency band
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls the width of the band. The width becomes narrower as
                  the Q value increases.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            lowshelf
          </dt>
          <dd>
            <p>
              The lowshelf filter allows all frequencies through, but adds a
              boost (or attenuation) to the lower frequencies. It implements a
              second-order lowshelf filter.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The upper limit of the frequences where the boost (or
                  attenuation) is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  The boost, in dB, to be applied. If the value is negative,
                  the frequencies are attenuated.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            highshelf
          </dt>
          <dd>
            <p>
              The highshelf filter is the opposite of the lowshelf filter and
              allows all frequencies through, but adds a boost to the higher
              frequencies. It implements a second-order highshelf filter
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The lower limit of the frequences where the boost (or
                  attenuation) is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  The boost, in dB, to be applied. If the value is negative,
                  the frequencies are attenuated.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            peaking
          </dt>
          <dd>
            <p>
              The peaking filter allows all frequencies through, but adds a
              boost (or attenuation) to a range of frequencies.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The center frequency of where the boost is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls the width of the band of frequencies that are
                  boosted. A large value implies a narrow width.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  The boost, in dB, to be applied. If the value is negative,
                  the frequencies are attenuated.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            notch
          </dt>
          <dd>
            <p>
              The notch filter (also known as a <a href=
              "https://en.wikipedia.org/wiki/Band-stop_filter">band-stop or
              band-rejection filter</a>) is the opposite of a bandpass filter.
              It allows all frequencies through, except for a set of
              frequencies.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The center frequency of where the notch is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls the width of the band of frequencies that are
                  attenuated. A large value implies a narrow width.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            allpass
          </dt>
          <dd>
            <p>
              An <a href=
              "https://en.wikipedia.org/wiki/All-pass_filter#Digital_Implementation">
              allpass filter</a> allows all frequencies through, but changes
              the phase relationship between the various frequencies. It
              implements a second-order allpass filter
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The frequency where the center of the phase transition
                  occurs. Viewed another way, this is the frequency with
                  maximal <a href=
                  "https://en.wikipedia.org/wiki/Group_delay">group delay</a>.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls how sharp the phase transition is at the center
                  frequency. A larger value implies a sharper transition and a
                  larger group delay.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
              </dl>
            </blockquote>
          </dd>
        </dl>
        <p>
          All attributes of the <a><code>BiquadFilterNode</code></a> are
          <a>a-rate</a> <a><code>AudioParam</code></a>.
        </p>
        <dl title="interface BiquadFilterNode : AudioNode" class="idl">
          <dt>
            attribute BiquadFilterType type
          </dt>
          <dd>
            The type of this <a><code>BiquadFilterNode</code></a>. The exact
            meaning of the other parameters depend on the value of the
            <a><code>type</code></a> attribute.
          </dd>
          <dt>
            readonly attribute AudioParam frequency
          </dt>
          <dd>
            The frequency at which the <a><code>BiquadFilterNode</code></a>
            will operate, in Hz. Its default value is 350Hz, and its nominal
            range is from 10Hz to half the Nyquist frequency.
          </dd>
          <dt>
            readonly attribute AudioParam detune
          </dt>
          <dd>
            A detune value, in cents, for the frequency. Its default value is
            0.
          </dd>
          <dt>
            readonly attribute AudioParam Q
          </dt>
          <dd>
            The <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a> factor
            has a default value of 1, with a nominal range of 0.0001 to 1000.
          </dd>
          <dt>
            readonly attribute AudioParam gain
          </dt>
          <dd>
            The gain has a default value of 0, with a nominal range of -40 to
            40.
          </dd>
          <dt>
            void getFrequencyResponse()
          </dt>
          <dd>
            <p>
              Given the current filter parameter settings, calculates the
              frequency response for the specified frequencies. The three
              parameters MUST be <code>Float32Array</code>s of the same length,
              or an <code>InvalidAccessError</code> MUST be thrown.
            </p>
            <p>
              The frequency response returned MUST be computed with the
              <a><code>AudioParam</code></a> sampled for the current processing
              block.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array frequencyHz
              </dt>
              <dd>
                <p>
                  This parameter specifies an array of frequencies at which the
                  response values will be calculated.
                </p>
              </dd>
              <dt>
                Float32Array magResponse
              </dt>
              <dd>
                <p>
                  This parameter specifies an output array receiving the linear
                  magnitude response values.
                </p>
                <p>
                  If a value in the <code>frequencyHz</code> parameter is not
                  within [0; sampleRate/2], where <code>sampleRate</code> is
                  the value of the <a href=
                  "#widl-AudioContext-sampleRate"><code>sampleRate</code></a>
                  property of the <a>AudioContext</a>, the corresponding value
                  at the same index of the <code>magResponse</code> array MUST
                  be <code>NaN</code>.
                </p>
              </dd>
              <dt>
                Float32Array phaseResponse
              </dt>
              <dd>
                <p>
                  This parameter specifies an output array receiving the phase
                  response values in radians.
                </p>
                <p>
                  If a value in the <code>frequencyHz</code> parameter is not
                  within [0; sampleRate/2], where <code>sampleRate</code> is
                  the value of the <a href=
                  "#widl-AudioContext-sampleRate"><code>sampleRate</code></a>
                  property of the <a>AudioContext</a>, the corresponding value
                  at the same index of the <code>phaseResponse</code> array
                  MUST be <code>NaN</code>.
                </p>
              </dd>
            </dl>
          </dd>
        </dl>
        <section>
          <h3>
            Filters characteristics
          </h3>
          <p>
            There are multiple ways of implementing the type of filters
            available through the <a><code>BiquadFilterNode</code></a> each
            having very different characteristics. The formulas in this section
            describe the filters that a <a>conforming implementation</a> MUST
            implement, as they determine the characteristics of the different
            filter types. They are inspired by formulas found in the <a href=
            "http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt">Audio EQ
            Cookbook</a>.
          </p>
          <p>
            The transfer function for the filters implemented by the
            <a><code>BiquadFilterNode</code></a> is:
          </p>
          <pre>
  $$
  H(z) = \frac{\frac{b_0}{a_0} + \frac{b_1}{a_0}z^{-1} + \frac{b_2}{a_0}z^{-2}}
              {1+\frac{a_1}{a_0}z^{-1}+\frac{a_2}{a_0}z^{-2}}
  $$
            </pre>
          <p>
            The initial filter state is 0.
          </p>The coefficients in the transfer function above are different for
          each node type. The following intermediate variable are necessary for
          their computation, based on the <a>computedValue</a> of the
          <a><code>AudioParam</code></a>s of the
          <a><code>BiquadFilterNode</code></a>.
          <ul>
            <li>Let \(F_s\) be the value of the <a href=
            "#widl-AudioContext-sampleRate"><code>sampleRate</code></a>
            attribute for this <a>AudioContext</a>.
            </li>
            <li>Let \(f_0\) be the value of the
            <a><code>computedFrequency</code></a>.
            </li>
            <li>Let \(G\) be the value of the <a href=
            "#widl-BiquadFilterNode-gain"><code>gain</code></a>
            <a><code>AudioParam</code></a>.
            </li>
            <li>Let \(Q\) be the value of the <a href=
            "#widl-BiquadFilterNode-Q"><code>Q</code></a>
            <a><code>AudioParam</code></a>.
            </li>
            <li>Finally let 
            <!-- Should \alpha_S be simplified since S is always 1?-->
              <pre>
$$
\begin{align*}
  A        &amp;= 10^{\frac{G}{40}} \\
  \omega_0 &amp;= 2\pi\frac{f_0}{F_s} \\
  \alpha_Q &amp;= \frac{\sin\omega_0}{2Q} \\
  \alpha_B &amp;= \frac{\sin\omega_0}{2} \sqrt{\frac{4-\sqrt{16-\frac{16}{G^2}}}{2}} \\
  S        &amp;= 1 \\
  \alpha_S &amp;= \frac{\sin\omega_0}{2}\sqrt{\left(A+\frac{1}{A}\right)\left(\frac{1}{S}-1\right)+2}
\end{align*}
$$
            </pre>
            </li>
          </ul>The six coefficients (\(b_0, b_1, b_2, a_0, a_1, a_2\)) for each
          filter type, are:
          <dl>
            <dt>
              <code>lowpass</code>
            </dt>
            <dd>
              <pre>
                $$
                  \begin{align*}
                    b_0 &amp;= \frac{1 - \cos\omega_0}{2} \\
                    b_1 &amp;= 1 - \cos\omega_0 \\
                    b_2 &amp;= \frac{1 - \cos\omega_0}{2} \\
                    a_0 &amp;= 1 + \alpha_B \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \alpha_B
                  \end{align*}
                $$
              </pre>
            </dd>
            <dt>
              <code>highpass</code>
            </dt>
            <dd>
              <pre>
                  $$
                    \begin{align*}
                      b_0 &amp;= \frac{1 + \cos\omega_0}{2} \\
                      b_1 &amp;= -(1 + \cos\omega_0) \\
                      b_2 &amp;= \frac{1 + \cos\omega_0}{2} \\
                      a_0 &amp;= 1 + \alpha_B \\
                      a_1 &amp;= -2 \cos\omega_0 \\
                      a_2 &amp;= 1 - \alpha_B
                    \end{align*}
                  $$
              </pre>
            </dd>
            <dt>
              <code>bandpass</code>
            </dt>
            <dd>
              <pre>
              $$
                \begin{align*}
                  b_0 &amp;= \alpha_Q \\
                  b_1 &amp;= 0 \\
                  b_2 &amp;= -\alpha_Q \\
                  a_0 &amp;= 1 + \alpha_Q \\
                  a_1 &amp;= -2 \cos\omega_0 \\
                  a_2 &amp;= 1 - \alpha_Q
                \end{align*}
              $$
            </pre>
            </dd>
            <dt>
              <code>notch</code>
            </dt>
            <dd>
              <pre>
                $$
                  \begin{align*}
                    b_0 &amp;= 1 \\
                    b_1 &amp;= -2\cos\omega_0 \\
                    b_2 &amp;= 1 \\
                    a_0 &amp;= 1 + \alpha_Q \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \alpha_Q
                  \end{align*}
                $$
              </pre>
            </dd>
            <dt>
              <code>allpass</code>
            </dt>
            <dd>
              <pre>
                $$
                  \begin{align*}
                    b_0 &amp;= 1 - \alpha_Q \\
                    b_1 &amp;= -2\cos\omega_0 \\
                    b_2 &amp;= 1 + \alpha_Q \\
                    a_0 &amp;= 1 + \alpha_Q \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \alpha_Q
                  \end{align*}
                $$
              </pre>
            </dd>
            <dt>
              <code>peaking</code>
            </dt>
            <dd>
              <pre>
                $$
                  \begin{align*}
                    b_0 &amp;= 1 + \alpha_Q\, A \\
                    b_1 &amp;= -2\cos\omega_0 \\
                    b_2 &amp;= 1 - \alpha_Q\,A \\
                    a_0 &amp;= 1 + \frac{\alpha_Q}{A} \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \frac{\alpha_Q}{A}
                  \end{align*}
                $$
              </pre>
            </dd>
            <dt>
              <code>lowshelf</code>
            </dt>
            <dd>
              <pre>
                $$
                  \begin{align*}
                    b_0 &amp;= A \left[ (A+1) - (A-1) \cos\omega_0 + 2 \alpha_S \sqrt{A})\right] \\
                    b_1 &amp;= 2 A \left[ (A-1) - (A+1) \cos\omega_0 )\right] \\
                    b_2 &amp;= A \left[ (A+1) - (A-1) \cos\omega_0 - 2 \alpha_S \sqrt{A}) \right] \\
                    a_0 &amp;= (A+1) + (A-1) \cos\omega_0 + 2 \alpha_S \sqrt{A} \\
                    a_1 &amp;= -2 \left[ (A-1) + (A+1) \cos\omega_0\right] \\
                    a_2 &amp;= (A+1) + (A-1) \cos\omega_0 - 2 \alpha_S \sqrt{A})
                  \end{align*}
                $$
              </pre>
            </dd>
            <dt>
              <code>highshelf</code>
            </dt>
            <dd>
              <pre>
                $$
                  \begin{align*}
                    b_0 &amp;= A\left[ (A+1) + (A-1)\cos\omega_0 + 2\alpha_S\sqrt{A} )\right] \\
                    b_1 &amp;= -2A\left[ (A-1) + (A+1)\cos\omega_0 )\right] \\
                    b_2 &amp;= A\left[ (A+1) + (A-1)\cos\omega_0 - 2\alpha_S\sqrt{A} )\right] \\
                    a_0 &amp;= (A+1) - (A-1)\cos\omega_0 + 2\alpha_S\sqrt{A} \\
                    a_1 &amp;= 2\left[ (A-1) - (A+1)\cos\omega_0\right] \\
                    a_2 &amp;= (A+1) - (A-1)\cos\omega_0 - 2\alpha_S\sqrt{A}
                  \end{align*}
                $$
              </pre>
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The IIRFilterNode Interface
        </h2>
        <p>
          <a><code>IIRFilterNode</code></a> is an <a><code>AudioNode</code></a>
          processor implementing a general IIR Filter. In general, it is best
          to use <a><code>BiquadFilterNode</code></a>'s to implement
          higher-order filters for the following reasons:
        </p>
        <ul>
          <li>Generally less sensitive to numeric issues
          </li>
          <li>Filter parameters can be automated
          </li>
          <li>Can be used to create all even-ordered IIR filters
          </li>
        </ul>
        <p>
          However, odd-ordered filters cannot be created, so if such filters
          are needed or automation is not needed, then IIR filters may be
          appropriate.
        </p>
        <p>
          Once created, the coefficients of the IIR filter cannot be changed.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <dl title="interface IIRFilterNode : AudioNode" class="idl">
          <dt>
            void getFrequencyResponse()
          </dt>
          <dd>
            <p>
              Given the current filter parameter settings, calculates the
              frequency response for the specified frequencies.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array frequencyHz
              </dt>
              <dd>
                This parameter specifies an array of frequencies at which the
                response values will be calculated.
              </dd>
              <dt>
                Float32Array magResponse
              </dt>
              <dd>
                This parameter specifies an output array receiving the linear
                magnitude response values. If this array is shorter than
                <code>frequencyHz</code> a NotSupportedError MUST be signaled.
              </dd>
              <dt>
                Float32Array phaseResponse
              </dt>
              <dd>
                This parameter specifies an output array receiving the phase
                response values in radians. If this array is shorter than
                <code>frequencyHz</code> a NotSupportedError MUST be signaled.
              </dd>
            </dl>
          </dd>
        </dl>
        <section>
          <h3>
            Filter Definition
          </h3>
          <p>
            Let \(b_m\) be the <code>feedforward</code> coefficients and
            \(a_n\) be the <code>feedback</code> coefficients specified by
            <a href=
            "#widl-AudioContext-createIIRFilter-IIRFilterNode-Float32Array-feedforward-Float32Array-feedback">
            <code>createIIRFilter</code></a>. Then the transfer function of the
            general IIR filter is given by
          </p>
          <pre>
            $$
              H(z) = \frac{\sum_{m=0}^{M} b_m z^{-m}}{\sum_{n=0}^{N} a_n z^{-n}}
            $$
          </pre>
          <p>
            where \(M + 1\) is the length of the \(b\) array and \(N + 1\) is
            the length of the \(a\) array. The coefficient \(a_0\) cannot be 0.
            At least one of \(b_m\) must be non-zero.
          </p>
          <p>
            Equivalently, the time-domain equation is:
          </p>
          <pre>
            $$
              \sum_{k=0}^{N} a_k y(n-k) = \sum_{k=0}^{M} b_k x(n-k)
            $$
          </pre>
          <p>
            The initial filter state is the all-zeroes state.
          </p>
        </section>
      </section>
      <section>
        <h2 id="WaveShaperNode">
          The WaveShaperNode Interface
        </h2>
        <p>
          <a><code>WaveShaperNode</code></a> is an
          <a><code>AudioNode</code></a> processor implementing non-linear
          distortion effects.
        </p>
        <p>
          Non-linear waveshaping distortion is commonly used for both subtle
          non-linear warming, or more obvious distortion effects. Arbitrary
          non-linear shaping curves may be specified.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <dl title="enum OverSampleType" class="idl">
          <dt>
            none
          </dt>
          <dd>
            Don't oversample
          </dd>
          <dt>
            2x
          </dt>
          <dd>
            Oversample two times
          </dd>
          <dt>
            4x
          </dt>
          <dd>
            Oversample four times
          </dd>
        </dl>
        <dl title="interface WaveShaperNode : AudioNode" class="idl"
        data-merge="OverSampleType">
          <dt>
            attribute Float32Array? curve
          </dt>
          <dd>
            <p>
              The shaping curve used for the waveshaping effect. The input
              signal is nominally within the range [-1; 1]. Each input sample
              within this range will index into the shaping curve, with a
              signal level of zero corresponding to the center value of the
              curve array if there are an odd number of entries, or
              interpolated between the two centermost values if there are an
              even number of entries in the array. Any sample value less than
              -1 will correspond to the first value in the curve array. Any
              sample value greater than +1 will correspond to the last value in
              the curve array.
            </p>
            <p>
              The implementation must perform linear interpolation between
              adjacent points in the curve. Initially the curve attribute is
              null, which means that the WaveShaperNode will pass its input to
              its output without modification.
            </p>
            <p>
              Values of the curve are spread with equal spacing in the [-1; 1]
              range. This means that a <a><code>curve</code></a> with a even
              number of value will not have a value for a signal at zero, and a
              <a><code>curve</code></a> with an odd number of value will have a
              value for a signal at zero.
            </p>
            <p>
              A <code>InvalidStateError</code> MUST be thrown if this attribute
              is set with a <code>Float32Array</code> that has a
              <code>length</code> less than 2.
            </p>
            <p>
              When this attribute is set, an internal copy of the curve is
              created by the <a><code>WaveShaperNode</code></a>. Subsequent
              modifications of the contents of the array used to set the
              attribute therefore have no effect: the attribute must be set
              again in order to change the curve.
            </p>
          </dd>
          <dt>
            attribute OverSampleType oversample
          </dt>
          <dd>
            <p>
              Specifies what type of oversampling (if any) should be used when
              applying the shaping curve. The default value is "none", meaning
              the curve will be applied directly to the input samples. A value
              of "2x" or "4x" can improve the quality of the processing by
              avoiding some aliasing, with the "4x" value yielding the highest
              quality. For some applications, it's better to use no
              oversampling in order to get a very precise shaping curve.
            </p>
            <p>
              A value of "2x" or "4x" means that the following steps must be
              performed:
            </p>
            <ol>
              <li>Up-sample the input samples to 2x or 4x the sample-rate of
              the <a><code>AudioContext</code></a>. Thus for each processing
              block of 128 samples, generate 256 (for 2x) or 512 (for 4x)
              samples.
              </li>
              <li>Apply the shaping curve.
              </li>
              <li>Down-sample the result back to the sample-rate of the
              <a><code>AudioContext</code></a>. Thus taking the 256 (or 512)
              processed samples, generating 128 as the final result.
              </li>
            </ol>
            <p>
              The exact up-sampling and down-sampling filters are not
              specified, and can be tuned for sound quality (low aliasing,
              etc.), low latency, and performance.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The OscillatorNode Interface
        </h2>
        <p>
          <a><code>OscillatorNode</code></a> represents an audio source
          generating a periodic waveform. It can be set to a few commonly used
          waveforms. Additionally, it can be set to an arbitrary periodic
          waveform through the use of a <a><code>PeriodicWave</code></a>
          object.
        </p>
        <p>
          Oscillators are common foundational building blocks in audio
          synthesis. An OscillatorNode will start emitting sound at the time
          specified by the <code>start()</code> method.
        </p>
        <p>
          Mathematically speaking, a <em>continuous-time</em> periodic waveform
          can have very high (or infinitely high) frequency information when
          considered in the frequency domain. When this waveform is sampled as
          a discrete-time digital audio signal at a particular sample-rate,
          then care must be taken to discard (filter out) the high-frequency
          information higher than the <em>Nyquist</em> frequency (half the
          sample-rate) before converting the waveform to a digital form. If
          this is not done, then <em>aliasing</em> of higher frequencies (than
          the Nyquist frequency) will fold back as mirror images into
          frequencies lower than the Nyquist frequency. In many cases this will
          cause audibly objectionable artifacts. This is a basic and well
          understood principle of audio DSP.
        </p>
        <p>
          There are several practical approaches that an implementation may
          take to avoid this aliasing. Regardless of approach, the
          <em>idealized</em> discrete-time digital audio signal is well defined
          mathematically. The trade-off for the implementation is a matter of
          implementation cost (in terms of CPU usage) versus fidelity to
          achieving this ideal.
        </p>
        <p>
          It is expected that an implementation will take some care in
          achieving this ideal, but it is reasonable to consider lower-quality,
          less-costly approaches on lower-end hardware.
        </p>
        <p>
          Both .frequency and .detune are <a>a-rate</a> parameters and are used
          together to determine a <em>computedFrequency</em> value:
        </p>
        <pre class="highlight">
  computedFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)
</pre>
        <p>
          The OscillatorNode's instantaneous phase at each time is the time
          integral of <em>computedFrequency</em>.
        </p>
        <pre>
  numberOfInputs  : 0
  numberOfOutputs : 1 (mono output)
</pre>
        <dl title="enum OscillatorType" class="idl">
          <dt>
            sine
          </dt>
          <dd>
            A sine wave
          </dd>
          <dt>
            square
          </dt>
          <dd>
            A square wave of duty period 0.5
          </dd>
          <dt>
            sawtooth
          </dt>
          <dd>
            A sawtooth wave
          </dd>
          <dt>
            triangle
          </dt>
          <dd>
            A triangle wave
          </dd>
          <dt>
            custom
          </dt>
          <dd>
            A custom periodic wave
          </dd>
        </dl>
        <dl title="interface OscillatorNode : AudioNode" class="idl">
          <dt>
            attribute OscillatorType type
          </dt>
          <dd>
            The shape of the periodic waveform. It may directly be set to any
            of the type constant values except for "custom". Doing so MUST
            throw an InvalidStateError exception. The <a href=
            "#widl-OscillatorNode-setPeriodicWave-void-PeriodicWave-periodicWave">
            <code>setPeriodicWave()</code></a> method can be used to set a
            custom waveform, which results in this attribute being set to
            "custom". The default value is "sine". When this attribute is set,
            the phase of the oscillator MUST be conserved.
          </dd>
          <dt>
            readonly attribute AudioParam frequency
          </dt>
          <dd>
            The frequency (in Hertz) of the periodic waveform. Its default
            <code>value</code> is 440. This parameter is <a>a-rate</a>.
          </dd>
          <dt>
            readonly attribute AudioParam detune
          </dt>
          <dd>
            A detuning value (in Cents) which will offset the
            <a><code>frequency</code></a> by the given amount. Its default
            <code>value</code> is 0. This parameter is <a>a-rate</a>.
          </dd>
          <dt>
            void start(optional double when = 0)
          </dt>
          <dd>
            Defined the same as the <code>when</code> parameter of the
            <a><code>AudioBufferSourceNode</code></a>
          </dd>
          <dt>
            void stop(optional double when = 0)
          </dt>
          <dd>
            Defined as in <a><code>AudioBufferSourceNode</code></a>.
          </dd>
          <dt>
            void setPeriodicWave(PeriodicWave periodicWave)
          </dt>
          <dd>
            Sets an arbitrary custom periodic waveform given a
            <a><code>PeriodicWave</code></a>.
          </dd>
          <dt>
            attribute EventHandler onended
          </dt>
          <dd>
            A property used to set the <code>EventHandler</code> (described in
            <cite><a href=
            "https://html.spec.whatwg.org/multipage/webappapis.html#eventhandler">
            HTML</a></cite>[[HTML]]) for the ended event that is dispatched to
            <a>OscillatorNode</a> node types. When the
            <a><code>OscillatorNode</code></a> has finished playing (i.e. its
            stop time has been reached), an event of type <code>Event</code>
            (described in <cite><a href=
            "https://html.spec.whatwg.org/multipage/infrastructure.html#event">HTML</a></cite>[[HTML]])
            will be dispatched to the event handler.
          </dd>
        </dl>
        <section>
          <h2>
            Basic Waveform Phase
          </h2>
          <p>
            The idealized mathematical waveforms for the various oscillator
            types are defined here. In summary, all waveforms are defined
            mathematically to be an odd function with a positive slope at time
            0. The actual waveforms produced by the oscillator may differ to
            prevent aliasing affects.
          </p>
          <dl>
            <dt>
              "sine"
            </dt>
            <dd>
              The waveform for sine oscillator is:
              <pre>
                $$
                  x(t) = \sin t
                $$.
              </pre>
            </dd>
            <dt>
              "square"
            </dt>
            <dd>
              The waveform for the square wave oscillator is:
              <pre>
                $$
                  x(t) = \begin{cases}
                         1 & \mbox{for } 0≤ t &lt; \pi \\
                         -1 & \mbox{for } -\pi &lt; t &lt; 0.
                         \end{cases}
                $$
              </pre>
            </dd>
            <dt>
              "sawtooth"
            </dt>
            <dd>
              The waveform for the sawtooth oscillator is the ramp:
              <pre>
                $$
                  x(t) = \frac{t}{\pi} \mbox{ for } -\pi &lt; t ≤ \pi;
                $$
              </pre>
            </dd>
            <dt>
              "triangle"
            </dt>
            <dd>
              The waveform for the triangle oscillator is:
              <pre>
                $$
                  x(t) = \begin{cases}
                           \frac{2}{\pi} t & \mbox{for } 0 ≤ t ≤ \frac{\pi}{2} \\
                           1-\frac{2}{\pi} (t-\frac{\pi}{2}) & \mbox{for }
                           \frac{\pi}{2} &lt; t ≤ \pi.
                         \end{cases}
                $$
              </pre>This is extended to all \(t\) by using the fact that the
waveform is an odd function with period \(2\pi\).
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The PeriodicWave Interface
        </h2>
        <p>
          PeriodicWave represents an arbitrary periodic waveform to be used
          with an <a><code>OscillatorNode</code></a>. Please see <a href=
          "#widl-AudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag">
          createPeriodicWave()</a> and <a href=
          "#widl-OscillatorNode-setPeriodicWave-void-PeriodicWave-periodicWave">
          setPeriodicWave()</a> and for more details.
        </p>
        <dl title="interface PeriodicWave" class="idl"></dl>
        <section>
          <h2>
            PeriodicWaveConstraints
          </h2>The <code>PeriodicWaveConstraints</code> dictionary is used to
          specify how the waveform is normalized.
          <dl title="dictionary PeriodicWaveConstraints" class="idl">
            <dt>
              boolean disableNormalization = false
            </dt>
            <dd>
              Controls whether the periodic wave is normalized or not. If
              <code>true</code>, the waveform is not normalized; otherwise, the
              waveform is normalized.
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            Waveform Generation
          </h2>
          <p>
            The <a href=
            "#widl-AudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag">
            createPeriodicWave()</a> method takes two arrays to specify the
            Fourier coefficients of the PeriodicWave. Let \(a\) and \(b\)
            represent the real and imaginary arrays of length \(L\). Then the
            basic time-domain waveform, \(x(t)\), can be computed using:
          </p>
          <pre>
            $$
              x(t) = \sum_{k=1}^{L-1} \left(a[k]\cos2\pi k t + b[k]\sin2\pi k t\right)
            $$
          </pre>
          <p>
            This is the basic (unnormalized) waveform.
          </p>
        </section>
        <section>
          <h2>
            Waveform Normalization
          </h2>
          <p>
            By default, the waveform defined in the previous section is
            normalized so that the maximum value is 1. The normalization is
            done as follows.
          </p>
          <p>
            Let
          </p>
          <pre>
          $$
            \tilde{x}(n) = \sum_{k=1}^{L-1} \left(a[k]\cos\frac{2\pi k n}{N} + b[k]\sin\frac{2\pi k n}{N}\right)
          $$
          </pre>
          <p>
            where \(N\) is a power of two. (Note: \(\tilde{x}(n)\) can
            conveniently be computed using an inverse FFT.) The fixed
            normalization factor \(f\) is computed as follows:
          </p>
          <pre>
            $$
              f = \max_{n = 0, \ldots, N - 1} |\tilde{x}(n)|
            $$
          </pre>
          <p>
            Thus, the actual normalized waveform \(\hat{x}(n)\) is
          </p>
          <pre>
            $$
              \hat{x}(n) = \frac{\tilde{x}(n)}{f}
            $$
          </pre>
          <p>
            This fixed normalization factor must be applied to all generated
            waveforms.
          </p>
        </section>
        <section>
          <h2>
            Oscillator Coefficients
          </h2>
          <p>
            The builtin oscillator types are created using <a>PeriodicWave</a>
            objects. For completeness the coefficients for the PeriodicWave for
            each of the builtin oscillator types is given here. This is useful
            if a builtin type is desired but without the default normalization.
          </p>
          <p>
            In the following descriptions, let \(a\) be the array of real
            coefficients and \(b\) be the array of imaginary coefficients for
            <a href=
            "#widl-AudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag">
            <code>createPeriodicWave()</code></a>. In all cases \(a[n] = 0\)
            for all \(n\) because the waveforms are odd functions. Also, \(b[0]
            = 0\) in all cases. Hence, only \(b[n]\) for \(n \ge 1\) is
            specified below.
          </p>
          <dl>
            <dt>
              "sine"
            </dt>
            <dd>
              <pre>
                  $$
                    b[n] = \begin{cases}
                             1 & \mbox{for } n = 1 \\
                             0 & \mbox{otherwise}
                           \end{cases}
                  $$
              </pre>
            </dd>
            <dt>
              "square"
            </dt>
            <dd>
              <pre>
                  $$
                    b[n] = \frac{2}{n\pi}\left[1 - (-1)^n\right]
                  $$
              </pre>
            </dd>
            <dt>
              "sawtooth"
            </dt>
            <dd>
              <pre>
                $$
                  b[n] = (-1)^{n+1} \dfrac{2}{n\pi}
                $$
            </pre>
            </dd>
            <dt>
              "triangle"
            </dt>
            <dd>
              <pre>
                  $$
                    b[n] = \frac{8\sin\dfrac{n\pi}{2}}{(\pi n)^2}
                  $$
              </pre>
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2 id="MediaStreamAudioSourceNode">
          The MediaStreamAudioSourceNode Interface
        </h2>
        <p>
          This interface represents an audio source from a
          <code>MediaStream</code>. The first
          <code>AudioMediaStreamTrack</code> from the <code>MediaStream</code>
          will be used as a source of audio. Those interfaces are described in
          [[!mediacapture-streams]].
        </p>
        <pre>
    numberOfInputs  : 0
    numberOfOutputs : 1
</pre>
        <p>
          The number of channels of the output corresponds to the number of
          channels of the <code>AudioMediaStreamTrack</code>. If there is no
          valid audio track, then the number of channels output will be one
          silent channel.
        </p>
        <dl title="interface MediaStreamAudioSourceNode : AudioNode" class=
        "idl"></dl>
      </section>
      <section>
        <h2>
          The MediaStreamAudioDestinationNode Interface
        </h2>
        <p>
          This interface is an audio destination representing a
          <code>MediaStream</code> with a single
          <code>AudioMediaStreamTrack</code>. This MediaStream is created when
          the node is created and is accessible via the <dfn>stream</dfn>
          attribute. This stream can be used in a similar way as a
          <code>MediaStream</code> obtained via <code>getUserMedia()</code>,
          and can, for example, be sent to a remote peer using the
          <code>RTCPeerConnection</code> (described in [[!webrtc]])
          <code>addStream()</code> method.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 0

    channelCount = 2;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the input is by default 2 (stereo). Any
          connections to the input are up-mixed/down-mixed to the number of
          channels of the input.
        </p>
        <dl title="interface MediaStreamAudioDestinationNode : AudioNode"
        class="idl">
          <dt>
            readonly attribute MediaStream stream
          </dt>
          <dd>
            A MediaStream containing a single AudioMediaStreamTrack with the
            same number of channels as the node itself.
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h2>
        Processing model
      </h2>
      <section class="informative">
        <h3>
          Background
        </h3>
        <p>
          Real-time audio systems that require low latency are, often time,
          implemented using <em>callback functions</em>, where the operating
          system calls the program back when more audio has to be computed in
          order for the playback to stay uninterrupted. Such callback is called
          on a high priority thread (often the highest priority on the system).
          This means that a program that deals with audio only executes code
          from this callback, as any buffering between a rendering thread and
          the callback would naturally add latency or make the system less
          resilient to glitches.
        </p>
        <p>
          For this reason, the traditional way of executing asynchronous
          operations on the Web Platform, the event loop, does not work here,
          as the thread is no <em>continuously executing</em>. Additionaly, a
          lot of unnecessary and potentially blocking operations are available
          from traditional execution contexts (Windows and Workers), which is
          not something that is desirable to reach an acceptable level of
          performance.
        </p>
        <p>
          Additionaly, the Worker model makes creating a dedicated thread
          necessary for a script execution context, while all <a>AudioNode</a>s
          usually share the same execution context.
        </p>
      </section>
      <section>
        <h3>
          Control thread and rendering thread
        </h3>
        <p>
          The Web Audio API MUST be implemented using a <a>control thread</a>,
          and one <a>rendering thread</a>.
        </p>
        <p>
          The <dfn>control thread</dfn> is the thread from
          which the <a>AudioContext</a> is instanciated, and from which authors
          manipulate the audio graph. The <dfn>rendering
          thread</dfn> is the thread on which the actual audio output is
          computed, in reaction to the calls from the <a>control thread</a>. It
          can be a real-time, callback-based audio thread, if computing audio
          for an <a>AudioContext</a>, or a normal thread if rendering and audio
          graph offline using an <a>OfflineAudioContext</a>.
        </p>
        <p>
          The <a>control thread</a> uses a traditional
          event loop, as described in [[HTML]].
        </p>
        <p>
          The <a>rendering thread</a> uses a
          specialized rendering loop, described in the section <a href=
          "#rendering-audio">Rendering an audio graph</a>
        </p>
        <p>
          Communication from the <a>control thread</a>
          to the <a>rendering thread</a> is done using
          <a>Control message</a> passing. Communication
          in the other direction is done using regular event loop tasks.
        </p>
        <p>
          Each <a>AudioContext</a> has a single <dfn>control message
          queue</dfn>, that is a list of <dfn data-lt="control message">control
          messages</dfn>, that are a operations running on the <a>control
          thread</a>.
        </p>
        <p>
          <dfn id="queuing">Queuing an control message</dfn> means adding the
          message to the end of the <a>control message queue</a> of an
          <a>AudioContext</a>.
        </p>
        <p>
          <a>Control messages</a> in a <a>control message queue</a> are ordered by
          date of insertion. The <dfn>oldest message</dfn> is therefore the one
          at the front of the <a>control message queue</a>.
        </p>
        <p>
          <dfn data-lt="swap">Swapping</dfn> a <a>control message queue</a>
          <var>Q<sub>A</sub></var> with another <a>control message queue</a>
          <var>Q<sub>B</sub></var> means executing the following steps:
        </p>
        <ol>
          <li> Let <var>Q<sub>C</sub></var> be a new, empty <a>control message
                queue</a>. </li>
          <li> Move all the <a>control messages</a> <var>Q<sub>A</sub></var> to
              <var>Q<sub>C</sub></var>.</li>
          <li> Move all the <a>control messages</a> <var>Q<sub>B</sub></var> to
              <var>Q<sub>A</sub></var>.</li>
          <li> Move all the <a>control messages</a> <var>Q<sub>C</sub></var> to
              <var>Q<sub>B</sub></var>.</li>
        </ol>
        <p class="note">
          For example, successfuly calling <code>start()</code> on an
          <a>AudioBufferSourceNode</a> <code>source</code> adds a <a>control
          message</a> to the <a href="#control-message-queue">control message
          queue</a> of the <a>AudioContext</a> <code>source.context</code>.
        </p>
      </section>
      <section>
        <h3>
          Asynchronous operations
        </h3>
        <p>
          Calling methods on <a>AudioNode</a>s MUST to be done in two phases, a
          synchronous operation and an asynchronous operation. For each method,
          some part of the execution happens on the <a>control thread</a> (for
          example, throwing an exception in case of invalid parameters), and
          some part happens on the <a>rendering thread</a> (for example,
          changing the value of an <a>AudioParam</a>).
        </p>
        <p>
          In the description of each operation on <a>AudioNode</a>s and
          <a>AudioContext</a>s, the synchronous section is marked with a ⌛. All
          the other operations are executed <a href=
          "https://html.spec.whatwg.org/multipage/infrastructure.html#in-parallel">
          in parallel</a>, as described in [[HTML]].
        </p>
        <p>
          The synchronous section is executed on the <a>control thread</a>, and
          happens immediately. If it fails, the method execution is aborted,
          possibly throwing an exception. If it succeeds, a <a>control
          message</a>, encoding the operation to be executed on the
          <a>rendering thread</a> is enqueued on the <a>control message
          queue</a> of this <a>rendering thread</a>.
        </p>
        <p>
          The synchronous and asynchronous sections order with respect to other
          events MUST be the same: given two operation <em>A</em> and
          <em>B</em> with respectively synchronous and asynchronous section
          <em>A<sub>Sync</sub></em> and <em>A<sub>Async</sub></em>, and
          <em>B<sub>Sync</sub></em> and <em>B<sub>Async</sub></em>, if
          <em>A</em> happens before <em>B</em>, then <em>A<sub>Sync</sub></em>
          happens before <em>B<sub>Sync</sub></em>, and
          <em>A<sub>Async</sub></em> happens before <em>B<sub>Async</sub></em>.
        </p>
      </section>
      <section id="rendering-loop">
        <h3>
          Rendering an audio graph
        </h3>
        <p>
        Rendering an audio graph is done by blocks of 128 samples-frames.
        </p>

        <p>
        Operations that happens <dfn data-lt="atomic">atomically</dfn> on a given thread, can only
        be executed when no other <a>atomic</a> operation is
        running on another thread.
        </p>

        <p>
        The algorithm for rendering a block of audio from an <a>AudioContext</a>
        <em>G</em> with an <a href=#control-message-queue>control message queue</a>
        <em>Q</em> is as follow:
        </p>
        <ol>
          <li> Let <em>Q<sub>rendering</sub></em> be an empty <a
            href=#control-message-queue>control message queue</a>.
          <a>Atomically</a> swap <em>Q<sub>rendering</sub></em> and
          <em>Q</em></li>
        <li> Until there is no more event in <em>Q<sub>rendering</sub></em>,
        execute the following steps:
        <ol>
          <li>
          Execute the asynchronous section of the oldest element of
          <em>Q<sub>rendering</sub></em>.
          </li>
          <li>
          Remove the oldest element of <em>Q<sub>rendering</sub></em>.
          <li>Let <em>Q<sub>rendering</sub></em> be an empty <a>control message
          queue</a>. <a>Atomically</a> <a>swap</a> <em>Q<sub>rendering</sub></em> and
          <em>Q</em>
          </li>
          <li>Until there is no more messages in <em>Q<sub>rendering</sub></em>,
          execute the following steps:
            <ol>
              <li>Execute the asynchronous section of the <a>oldest message</a>
              of <em>Q<sub>rendering</sub></em>.
              </li>
              <li>Remove the <a>oldest message</a> of
              <em>Q<sub>rendering</sub></em>.  </li>
            </ol>
          </li>
          <li>Order the <a>AudioNode</a>s of the <a>AudioContext</a> to be
          processed.
            <ol>
              <li>Let <em>ordered</em> be an empty list of <a>AudioNode</a>s.
              It will contain an ordered list of <a>AudioNode</a>s when this
              ordering algorithm terminates.
              </li>
              <li>Let <em>nodes</em> be the set of all nodes belonging to an
              <a>AudioContext</a>.
              </li>
              <li>Let <em>cycle breakers</em> be an empty set of
              <a>DelayNode</a>s. It will contain all the <a>DelayNode</a>s that
              are part of a cycle.
              </li>
              <li>For each <a>AudioNode</a> <em>node</em> of <em>nodes</em>
                <ol>
                  <li>
                    <em>node</em> is a <a>DelayNode</a> part of a cycle, add it
                    to <em>cycle breakers</em> and remove it from
                    <em>nodes</em>.
                  </li>
                </ol>
              </li>
              <li>If <em>nodes</em> contains cycles, mute all the
              <a>AudioNode</a>s that are part of this cycle, and remove them
              from <em>nodes</em>.
              </li>
              <li>While there are unmarked <a>AudioNode</a>s in <em>nodes</em>:
                <ol>
                  <li>Choose an <a>AudioNode</a> <em>node</em> in
                  <em>nodes</em>
                  </li>
                  <li>
                    <a>Visit</a> <em>node</em>.
                  </li>
                </ol><dfn data-lt="Visit">Visiting a node</dfn> <em>node</em>
                mean performing the following steps:
                <ol>
                  <li>If <em>node</em> is marked, abort these steps.
                  </li>
                  <li>For each <a>AudioNode</a> <em>input</em> connect to <em>
                    node</em>:
                    <ol>
                      <li>
                        <a>Visit</a> <em>input</em>.
                      </li>
                    </ol>
                  </li>
                  <li>Mark <em>node</em>.
                  </li>
                  <li>Add <em>node</em> to the beginning of <em>ordered</em>.
                  </li>
                </ol>
              </li>
            </ol>
          </li>
          <li>For each <a>DelayNode</a> in a cycle, <a href="availabel">make
          available for reading</a> a block of audio from the <a>DelayNode</a>
          buffer.
          </li>
          <li>For each <a>AudioNode</a>, in the order determined previously:
            <ol>
              <li>If this <a>AudioNode</a> has any <a>AudioNode</a>s connected
              to it, <a href="#SummingJunction">sum</a> the buffers <a href=
              "available">made available for reading</a> by all
              <a>AudioNode</a>s connected to this <a>AudioNode</a>. The
              resulting buffer is called the <dfn>input buffer</dfn>.
                <a href="#channel-up-mixing-and-down-mixing">Up or down-mix</a>
                it to match if number of input channels of this
                <a>AudioNode</a>.
              </li>
              <li>If this <a>AudioNode</a> is a <a>source node</a>, <a href=
              "computing">compute a block of audio</a>, and <a href=
              "available">make it available for reading</a>.
              </li>
              <li>Else, if this <a>AudioNode</a> is a <a>destination node</a>,
              <a href="record">record the input</a> of this <a>AudioNode</a>.
              </li>
              <li>Else, <a>process</a> the <a>input buffer</a>, and <a href=
              "available">make available for reading</a> the resulting buffer.
              </li>
            </ol>
          </li>
        </ol>
        </li>
      </ol>
    <p>
      <dfn id=available>Making a buffer available for reading</dfn> from an
      <a>AudioNode</a> means putting it in a state where other <a>AudioNode</a>s
      connected to this <a>AudioNode</a> can safely read from it.
      <p class=note>For example, implementations can choose to allocate a new
      buffer, or have a more elaborate mechanism, reusing an existing buffer
      that is now unused.
    </p>

    <p>
      <dfn id=record>Recording the input</dfn> of an <a>AudioNode</a> means
      copying the input data of this <a>AudioNode</a> for future usage.
    </p>

    <p>
      <dfn id=computing>Computing a block of audio</dfn> means running the
      algorithm for this <a>AudioNode</a> to produce 128 sample-frames.
    </p>

    <p>
      <dfn data-lt=process>Processing an input buffer</dfn> means running the
      algorithm for an <a>AudioNode</a>, using an <a>input buffer</a> as the
      input for this algorithm.
    </p>
      </section>
    </section>
    <section class="informative">
      <h2>
        Mixer Gain Structure
      </h2>
      <h3 id="background">
        Background
      </h3>
      <p>
        One of the most important considerations when dealing with audio
        processing graphs is how to adjust the gain (volume) at various points.
        For example, in a standard mixing board model, each input bus has
        pre-gain, post-gain, and send-gains. Submix and master out busses also
        have gain control. The gain control described here can be used to
        implement standard mixing boards as well as other architectures.
      </p>
      <section>
        <h3 id="SummingJunction">
          Summing Inputs
        </h3>
        <p>
          The inputs to <a><code>AudioNode</code></a>s have the ability to
          accept connections from multiple outputs. The input then acts as a
          unity gain summing junction with each output signal being added with
          the others:
        </p>
        <figure>
          <img alt="unity gain summing junction" src=
          "images/unity-gain-summing-junction.png">
          <figcaption>
            A graph showing Source 1 and Source 2 output summed at the input of
            Destination
          </figcaption>
        </figure>
        <p>
          In cases where the channel layouts of the outputs do not match, a mix
          (usually up-mix) will occur according to the <a href=
          "#channel-up-mixing-and-down-mixing">mixing rules</a>.
        </p>
        <p>
          No clipping is applied at the inputs or outputs of the
          <a><code>AudioNode</code></a> to allow a maximum of dynamic range
          within the audio graph.
        </p>
      </section>
      <section>
        <h3>
          Gain Control
        </h3>
        <p>
          In many scenarios, it's important to be able to control the gain for
          each of the output signals. The <a><code>GainNode</code></a> gives
          this control:
        </p>
        <figure>
          <img alt="mixer architecture new" src=
          "images/mixer-architecture-new.png">
          <figcaption>
            A graph featuring volume control for each voice
          </figcaption>
        </figure>
        <p>
          Using these two concepts of unity gain summing junctions and
          GainNodes, it's possible to construct simple or complex mixing
          scenarios.
        </p>
      </section>
      <section>
        <h3 id="Example-mixer-with-send-busses">
          Example: Mixer with Send Busses
        </h3>
        <p>
          In a routing scenario involving multiple sends and submixes, explicit
          control is needed over the volume or "gain" of each connection to a
          mixer. Such routing topologies are very common and exist in even the
          simplest of electronic gear sitting around in a basic recording
          studio.
        </p>
        <p>
          Here's an example with two send mixers and a main mixer. Although
          possible, for simplicity's sake, pre-gain control and insert effects
          are not illustrated:
        </p>
        <figure>
          <img alt="mixer gain structure" src=
          "images/mixer-gain-structure.png">
          <figcaption>
            A graph showing a full mixer with send busses.
          </figcaption>
        </figure>
        <p>
          This diagram is using a shorthand notation where "send 1", "send 2",
          and "main bus" are actually inputs to <a><code>AudioNode</code></a>s,
          but here are represented as summing busses, where the intersections
          g2_1, g3_1, etc. represent the "gain" or volume for the given source
          on the given mixer. In order to expose this gain, an
          <a><code>GainNode</code></a> is used:
        </p>
        <p>
          Here's how the above diagram could be constructed in JavaScript:
        </p>
        <pre class="highlight example">

  var context = 0;
  var compressor = 0;
  var reverb = 0;
  var delay = 0;
  var s1 = 0;
  var s2 = 0;

  var source1 = 0;
  var source2 = 0;
  var g1_1 = 0;
  var g2_1 = 0;
  var g3_1 = 0;
  var g1_2 = 0;
  var g2_2 = 0;
  var g3_2 = 0;

  // Setup routing graph
  function setupRoutingGraph() {
      context = new AudioContext();

      compressor = context.createDynamicsCompressor();

      // Send1 effect
      reverb = context.createConvolver();
      // Convolver impulse response may be set here or later

      // Send2 effect
      delay = context.createDelay();

      // Connect final compressor to final destination
      compressor.connect(context.destination);

      // Connect sends 1 & 2 through effects to main mixer
      s1 = context.createGain();
      reverb.connect(s1);
      s1.connect(compressor);

      s2 = context.createGain();
      delay.connect(s2);
      s2.connect(compressor);

      // Create a couple of sources
      source1 = context.createBufferSource();
      source2 = context.createBufferSource();
      source1.buffer = manTalkingBuffer;
      source2.buffer = footstepsBuffer;

      // Connect source1
      g1_1 = context.createGain();
      g2_1 = context.createGain();
      g3_1 = context.createGain();
      source1.connect(g1_1);
      source1.connect(g2_1);
      source1.connect(g3_1);
      g1_1.connect(compressor);
      g2_1.connect(reverb);
      g3_1.connect(delay);

      // Connect source2
      g1_2 = context.createGain();
      g2_2 = context.createGain();
      g3_2 = context.createGain();
      source2.connect(g1_2);
      source2.connect(g2_2);
      source2.connect(g3_2);
      g1_2.connect(compressor);
      g2_2.connect(reverb);
      g3_2.connect(delay);

      // We now have explicit control over all the volumes g1_1, g2_1, ..., s1, s2
      g2_1.gain.value = 0.2;  // For example, set source1 reverb gain

      // Because g2_1.gain is an "AudioParam",
      // an automation curve could also be attached to it.
      // A "mixing board" UI could be created in canvas or WebGL controlling these gains.
  }

   </pre>
      </section>
    </section>
    <section>
      <h2 id="DynamicLifetime">
        Dynamic Lifetime
      </h2>
      <section>
        <h3>
          Background
        </h3>
        <p class="norm">
          <em>This section is non-normative. Please see <a href=
          "#lifetime-AudioContext">AudioContext lifetime</a> and <a href=
          "#lifetime-AudioNode">AudioNode lifetime</a> for normative
          requirements.</em>
        </p>
        <p>
          In addition to allowing the creation of static routing
          configurations, it should also be possible to do custom effect
          routing on dynamically allocated voices which have a limited
          lifetime. For the purposes of this discussion, let's call these
          short-lived voices "notes". Many audio applications incorporate the
          ideas of notes, examples being drum machines, sequencers, and 3D
          games with many one-shot sounds being triggered according to game
          play.
        </p>
        <p>
          In a traditional software synthesizer, notes are dynamically
          allocated and released from a pool of available resources. The note
          is allocated when a MIDI note-on message is received. It is released
          when the note has finished playing either due to it having reached
          the end of its sample-data (if non-looping), it having reached a
          sustain phase of its envelope which is zero, or due to a MIDI
          note-off message putting it into the release phase of its envelope.
          In the MIDI note-off case, the note is not released immediately, but
          only when the release envelope phase has finished. At any given time,
          there can be a large number of notes playing but the set of notes is
          constantly changing as new notes are added into the routing graph,
          and old ones are released.
        </p>
        <p>
          The audio system automatically deals with tearing-down the part of
          the routing graph for individual "note" events. A "note" is
          represented by an <a><code>AudioBufferSourceNode</code></a>, which
          can be directly connected to other processing nodes. When the note
          has finished playing, the context will automatically release the
          reference to the <a><code>AudioBufferSourceNode</code></a>, which in
          turn will release references to any nodes it is connected to, and so
          on. The nodes will automatically get disconnected from the graph and
          will be deleted when they have no more references. Nodes in the graph
          which are long-lived and shared between dynamic voices can be managed
          explicitly. Although it sounds complicated, this all happens
          automatically with no extra JavaScript handling required.
        </p>
      </section>
      <section>
        <h3>
          Example
        </h3>
        <figure>
          <img alt="dynamic allocation" src="images/dynamic-allocation.png">
          <figcaption>
            A graph featuring a subgraph that will be releases early.
          </figcaption>
        </figure>
        <p>
          The low-pass filter, panner, and second gain nodes are directly
          connected from the one-shot sound. So when it has finished playing
          the context will automatically release them (everything within the
          dotted line). If there are no longer any JavaScript references to the
          one-shot sound and connected nodes, then they will be immediately
          removed from the graph and deleted. The streaming source, has a
          global reference and will remain connected until it is explicitly
          disconnected. Here's how it might look in JavaScript:
        </p>
        <pre class="example highlight">

var context = 0;
var compressor = 0;
var gainNode1 = 0;
var streamingAudioSource = 0;

// Initial setup of the "long-lived" part of the routing graph
function setupAudioContext() {
    context = new AudioContext();

    compressor = context.createDynamicsCompressor();
    gainNode1 = context.createGain();

    // Create a streaming audio source.
    var audioElement = document.getElementById('audioTagID');
    streamingAudioSource = context.createMediaElementSource(audioElement);
    streamingAudioSource.connect(gainNode1);

    gainNode1.connect(compressor);
    compressor.connect(context.destination);
}

// Later in response to some user action (typically mouse or key event)
// a one-shot sound can be played.
function playSound() {
    var oneShotSound = context.createBufferSource();
    oneShotSound.buffer = dogBarkingBuffer;

    // Create a filter, panner, and gain node.
    var lowpass = context.createBiquadFilter();
    var panner = context.createPanner();
    var gainNode2 = context.createGain();

    // Make connections
    oneShotSound.connect(lowpass);
    lowpass.connect(panner);
    panner.connect(gainNode2);
    gainNode2.connect(compressor);

    // Play 0.75 seconds from now (to play immediately pass in 0)
    oneShotSound.start(context.currentTime + 0.75);
}
</pre>
      </section>
    </section>
    <section>
      <h2>
        Channel up-mixing and down-mixing
      </h2>
      <p class="norm">
        This section is normative.
      </p>
      <p>
        <a href="#mixer-gain-structure"></a> describes how an input to an
        <a><code>AudioNode</code></a> can be connected from one or more outputs
        of an <a><code>AudioNode</code></a>. Each of these connections from an
        output represents a stream with a specific non-zero number of channels.
        An input has <em>mixing rules</em> for combining the channels from all
        of the connections to it. As a simple example, if an input is connected
        from a mono output and a stereo output, then the mono connection will
        usually be up-mixed to stereo and summed with the stereo connection.
        But, of course, it's important to define the exact <em>mixing
        rules</em> for every input to every <a><code>AudioNode</code></a>. The
        default mixing rules for all of the inputs have been chosen so that
        things "just work" without worrying too much about the details,
        especially in the very common case of mono and stereo streams. Of
        course, the rules can be changed for advanced use cases, especially
        multi-channel.
      </p>
      <p>
        To define some terms, <em>up-mixing</em> refers to the process of
        taking a stream with a smaller number of channels and converting it to
        a stream with a larger number of channels. <em>down-mixing</em> refers
        to the process of taking a stream with a larger number of channels and
        converting it to a stream with a smaller number of channels.
      </p>
      <p>
        An <a><code>AudioNode</code></a> input use three basic pieces of
        information to determine how to mix all the outputs connected to it. As
        part of this process it computes an internal value
        <dfn><code>computedNumberOfChannels</code></dfn> representing the
        actual number of channels of the input at any given time:
      </p>
      <p>
        The <a><code>AudioNode</code></a> attributes involved in channel
        up-mixing and down-mixing rules are defined <a href=
        "#the-audionode-interface">above</a>. The following is a more precise
        specification on what each of them mean.
      </p>
      <ul>
        <li>
          <a href="#widl-AudioNode-channelCount"><code>channelCount</code></a>
          is used to help compute <a><code>computedNumberOfChannels</code></a>.
        </li>
        <li>
          <a href=
          "#widl-AudioNode-channelCountMode"><code>channelCountMode</code></a>
          determines how <a><code>computedNumberOfChannels</code></a> will be
          computed. Once this number is computed, all of the connections will
          be up or down-mixed to that many channels. For most nodes, the
          default value is <a href=
          "#idl-def-ChannelCountMode.max"><code>"max"</code></a>.
          <ul>
            <li>
              <a href="#idl-def-ChannelCountMode.max"><code>"max"</code></a>:
              <a><code>computedNumberOfChannels</code></a> is computed as the
              maximum of the number of channels of all connections. In this
              mode <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a> is
              ignored.
            </li>
            <li>
              <a href=
              "#idl-def-ChannelCountMode.clamped-max"><code>"clamped-max"</code></a>:
              same as “max” up to a limit of the <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a>
            </li>
            <li>
              <a href=
              "#idl-def-ChannelCountMode.explicit"><code>"explicit"</code></a>:
              <a><code>computedNumberOfChannels</code></a> is the exact value
              as specified in <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a>
            </li>
          </ul>
        </li>
        <li>
          <a href=
          "#widl-AudioNode-channelInterpretation"><code>channelInterpretation</code></a>
          determines how the individual channels will be treated. For example,
          will they be treated as speakers having a specific layout, or will
          they be treated as simple discrete channels? This value influences
          exactly how the up and down mixing is performed. The default value is
          "speakers".
          <ul>
            <li>
              <a href=
              "#idl-def-ChannelInterpretation.speakers"><code>“speakers”</code></a>:
              use <a href="#ChannelLayouts">up-down-mix equations for
              mono/stereo/quad/5.1</a>. In cases where the number of channels
              do not match any of these basic speaker layouts, revert to
              "discrete".
            </li>
            <li>
              <a href=
              "#idl-def-ChannelInterpretation.discrete"><code>“discrete”</code></a>:
              up-mix by filling channels until they run out then zero out
              remaining channels. down-mix by filling as many channels as
              possible, then dropping remaining channels
            </li>
          </ul>
        </li>
      </ul>
      <p>
        For each input of an <a><code>AudioNode</code></a>, an implementation
        must:
      </p>
      <ol>
        <li>Compute <a><code>computedNumberOfChannels</code></a>.
        </li>
        <li>For each connection to the input:
          <ul>
            <li>up-mix or down-mix the connection to
            <a><code>computedNumberOfChannels</code></a> according to
              <a href="#widl-AudioNode-channelInterpretation"><code>channelInterpretation</code></a>.
            </li>
            <li>Mix it together with all of the other mixed streams (from other
            connections). This is a straight-forward mixing together of each of
            the corresponding channels from each connection.
            </li>
          </ul>
        </li>
      </ol>
      <section>
        <h3 id="ChannelLayouts">
          Speaker Channel Layouts
        </h3>
        <p>
          When <a href=
          "#widl-AudioNode-channelInterpretation"><code>channelInterpretation</code></a>
          is <a href="#idl-def-ChannelInterpretation.speakers">"speakers"</a>
          then the up-mixing and down-mixing is defined for specific channel
          layouts.
        </p>
        <p>
          Mono (one channel), stereo (two channels), quad (four channels), and
          5.1 (six channels) MUST be supported. Other channel layout may be
          supported in future version of this specification.
        </p>
      </section>
      <section>
        <h4 id="ChannelOrdering">
          Channel ordering
        </h4>
        <pre>
    Mono
      0: M: mono

    Stereo
      0: L: left
      1: R: right
    </pre>
        <pre>  Quad
      0: L:  left
      1: R:  right
      2: SL: surround left
      3: SR: surround right

    5.1
      0: L:   left
      1: R:   right
      2: C:   center
      3: LFE: subwoofer
      4: SL:  surround left
      5: SR:  surround right
  </pre>
      </section>
      <section>
        <h4 id="UpMix-sub">
          Up Mixing speaker layouts
        </h4>
        <pre>Mono up-mix:

    1 -&gt; 2 : up-mix from mono to stereo
        output.L = input;
        output.R = input;

    1 -&gt; 4 : up-mix from mono to quad
        output.L = input;
        output.R = input;
        output.SL = 0;
        output.SR = 0;

    1 -&gt; 5.1 : up-mix from mono to 5.1
        output.L = 0;
        output.R = 0;
        output.C = input; // put in center channel
        output.LFE = 0;
        output.SL = 0;
        output.SR = 0;

Stereo up-mix:

    2 -&gt; 4 : up-mix from stereo to quad
        output.L = input.L;
        output.R = input.R;
        output.SL = 0;
        output.SR = 0;

    2 -&gt; 5.1 : up-mix from stereo to 5.1
        output.L = input.L;
        output.R = input.R;
        output.C = 0;
        output.LFE = 0;
        output.SL = 0;
        output.SR = 0;

Quad up-mix:

    4 -&gt; 5.1 : up-mix from quad to 5.1
        output.L = input.L;
        output.R = input.R;
        output.C = 0;
        output.LFE = 0;
        output.SL = input.SL;
        output.SR = input.SR;</pre>
      </section>
      <section>
        <h4 id="down-mix">
          Down Mixing speaker layouts
        </h4>
        <p>
          A down-mix will be necessary, for example, if processing 5.1 source
          material, but playing back stereo.
        </p>
        <pre>
  Mono down-mix:

      2 -&gt; 1 : stereo to mono
          output = 0.5 * (input.L + input.R);

      4 -&gt; 1 : quad to mono
          output = 0.25 * (input.L + input.R + input.SL + input.SR);

      5.1 -&gt; 1 : 5.1 to mono
          output = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL + input.SR)


  Stereo down-mix:

      4 -&gt; 2 : quad to stereo
          output.L = 0.5 * (input.L + input.SL);
          output.R = 0.5 * (input.R + input.SR);

      5.1 -&gt; 2 : 5.1 to stereo
          output.L = L + 0.7071 * (input.C + input.SL)
          output.R = R + 0.7071 * (input.C + input.SR)

  Quad down-mix:

      5.1 -&gt; 4 : 5.1 to quad
          output.L = L + 0.7071 * input.C
          output.R = R + 0.7071 * input.C
          output.SL = input.SL
          output.SR = input.SR

  </pre>
      </section>
      <section class="informative">
        <h3 id="ChannelRules-section">
          Channel Rules Examples
        </h3>
        <pre class="highlight example">
  // Set gain node to explicit 2-channels (stereo).
  gain.channelCount = 2;
  gain.channelCountMode = "explicit";
  gain.channelInterpretation = "speakers";

  // Set "hardware output" to 4-channels for DJ-app with two stereo output busses.
  context.destination.channelCount = 4;
  context.destination.channelCountMode = "explicit";
  context.destination.channelInterpretation = "discrete";

  // Set "hardware output" to 8-channels for custom multi-channel speaker array
  // with custom matrix mixing.
  context.destination.channelCount = 8;
  context.destination.channelCountMode = "explicit";
  context.destination.channelInterpretation = "discrete";

  // Set "hardware output" to 5.1 to play an HTMLAudioElement.
  context.destination.channelCount = 6;
  context.destination.channelCountMode = "explicit";
  context.destination.channelInterpretation = "speakers";

  // Explicitly down-mix to mono.
  gain.channelCount = 1;
  gain.channelCountMode = "explicit";
  gain.channelInterpretation = "speakers";
  </pre>
      </section>
    </section>
    <section>
      <h2 id="Spatialization">
        Spatialization / Panning
      </h2>
      <section>
        <h3 id="Spatialization-background">
          Background
        </h3>
        <p>
          A common feature requirement for modern 3D games is the ability to
          dynamically spatialize and move multiple audio sources in 3D space.
          Game audio engines such as OpenAL, FMOD, Creative's EAX, Microsoft's
          XACT Audio, etc. have this ability.
        </p>
        <p>
          Using an <a><code>PannerNode</code></a>, an audio stream can be
          spatialized or positioned in space relative to an
          <a><code>AudioListener</code></a>. An
          <a><code>AudioContext</code></a> will contain a single
          <a><code>AudioListener</code></a>. Both panners and listeners have a
          position in 3D space using a right-handed cartesian coordinate
          system. The units used in the coordinate system are not defined, and
          do not need to be because the effects calculated with these
          coordinates are independent/invariant of any particular units such as
          meters or feet. <a><code>PannerNode</code></a> objects (representing
          the source stream) have an <em>orientation</em> vector representing
          in which direction the sound is projecting. Additionally, they have a
          <em>sound cone</em> representing how directional the sound is. For
          example, the sound could be omnidirectional, in which case it would
          be heard anywhere regardless of its orientation, or it can be more
          directional and heard only if it is facing the listener.
          <a><code>AudioListener</code></a> objects (representing a person's
          ears) have an <em>orientation</em> and <em>up</em> vector
          representing in which direction the person is facing. Because both
          the source stream and the listener can be moving, they both have a
          <em>velocity</em> vector representing both the speed and direction of
          movement. Taken together, these two velocities can be used to
          generate a doppler shift effect which changes the pitch.
        </p>
        <p>
          During rendering, the <a><code>PannerNode</code></a> calculates an
          <em>azimuth</em> and <em>elevation</em>. These values are used
          internally by the implementation in order to render the
          spatialization effect. See the <a href=
          "#Spatialization-panning-algorithm">Panning Algorithm</a> section for
          details of how these values are used.
        </p>
      </section>
      <section id="azimuth-elevation">
        <h3>
          Azimuth and Elevation
        </h3>
        <p>
          The following algorithm must be used to calculate the
          <em>azimuth</em> and <em>elevation</em>: for the
          <a><code>PannerNode</code></a>
        </p>
        <pre class="code highlight">
  // Calculate the source-listener vector.
  vec3 sourceListener = source.position - listener.position;

  if (sourceListener.isZero()) {
      // Handle degenerate case if source and listener are at the same point.
      azimuth = 0;
      elevation = 0;
      return;
  }

  sourceListener.normalize();

  // Align axes.
  vec3 listenerFront = listener.orientation;
  vec3 listenerUp = listener.up;
  vec3 listenerRight = listenerFront.cross(listenerUp);
  listenerRight.normalize();

  vec3 listenerFrontNorm = listenerFront;
  listenerFrontNorm.normalize();

  vec3 up = listenerRight.cross(listenerFrontNorm);

  float upProjection = sourceListener.dot(up);

  vec3 projectedSource = sourceListener - upProjection * up;
  projectedSource.normalize();

  azimuth = 180 * acos(projectedSource.dot(listenerRight)) / PI;

  // Source in front or behind the listener.
  float frontBack = projectedSource.dot(listenerFrontNorm);
  if (frontBack &lt; 0)
      azimuth = 360 - azimuth;

  // Make azimuth relative to "front" and not "right" listener vector.
  if ((azimuth &gt;= 0) &amp;& (azimuth &lt;= 270))
      azimuth = 90 - azimuth;
  else
      azimuth = 450 - azimuth;

  elevation = 90 - 180 * acos(sourceListener.dot(up)) / PI;

  if (elevation &gt; 90)
      elevation = 180 - elevation;
  else if (elevation &lt; -90)
      elevation = -180 - elevation;
  </pre>
      </section>
      <section>
        <h3 id="Spatialization-panning-algorithm">
          Panning Algorithm
        </h3>
        <p>
          <em>Mono-to-stereo</em> and <em>stereo-to-stereo</em> panning must be
          supported. <em>Mono-to-stereo</em> processing is used when all
          connections to the input are mono. Otherwise
          <em>stereo-to-stereo</em> processing is used.
        </p>
        <section>
          <h4 id="Spatialzation-equal-power-panning">
            Equal-power panning
          </h4>
          <p>
            This is a simple and relatively inexpensive algorithm which
            provides basic, but reasonable results. It is used for the
            <a><code>StereoPannerNode</code></a>, and for the
            <a><code>PannerNode</code></a> when the <a href=
            "#widl-PannerNode-panningModel"><code>panningModel</code></a>
            attribute is set to <code>"equalpower"</code>, in which case the
            the <em>elevation</em> value is ignored.
          </p>
          <p>
            For a <a><code>PannerNode</code></a>, the following algorithm MUST
            be implemented.
          </p>
          <ol>
            <li>
              <p>
                Let <em>azimuth</em> be the value computed in the <a href=
                "#azimuth-elevation">azimuth and elevation</a> section.
              </p>
            </li>
            <li>
              <p>
                The <em>azimuth</em> value is first contained to be within the
                range [-90, 90] according to:
              </p>
              <pre class="highlight">
  // First, clamp azimuth to allowed range of [-180, 180].
  azimuth = max(-180, azimuth);
  azimuth = min(180, azimuth);

  // Then wrap to range [-90, 90].
  if (azimuth &lt; -90)
    azimuth = -180 - azimuth;
  else if (azimuth &gt; 90)
    azimuth = 180 - azimuth;
        </pre>
            </li>
            <li>
              <p>
                A normalized value <em>x</em> is calculated from
                <em>azimuth</em> for a mono input as:
              </p>
              <pre class="highlight">
  x = (azimuth + 90) / 180;
        </pre>
              <p>
                Or for a stereo input as:
              </p>
              <pre class="highlight">
  if (azimuth &lt;= 0) { // -90 ~ 0
    // Transform the azimuth value from [-90, 0] degrees into the range [-90, 90].
    x = (azimuth + 90) / 90;
  } else { // 0 ~ 90
    // Transform the azimuth value from [0, 90] degrees into the range [-90, 90].
    x = azimuth / 90;
  }
        </pre>
            </li>
          </ol>
          <p>
            For a <a><code>StereoPannerNode</code></a>, the following algorithm
            MUST be implemented.
          </p>
          <ol>
            <li>
              <p>
                Let <em>pan</em> be the <a>computedValue</a> of the
                <code>pan</code> <a>AudioParam</a> of this
                <a><code>StereoPannerNode</code></a>.
              </p>
            </li>
            <li>
              <p>
                Clamp <em>pan</em> to [-1, 1].
              </p>
              <pre class="highlight">
pan = max(-1, pan);
pan = min(1, pan);
        </pre>
            </li>
            <li>
              <p>
                Calculate <em>x</em> by normalizing <em>pan</em> value to [0,
                1]. For mono input:
              </p>
              <pre class="highlight">
x = (pan + 1) / 2;
        </pre>
              <p>
                For stereo input:
              </p>
              <pre class="highlight">
if (pan &lt;= 0)
  x = pan + 1;
else
  x = pan;
        </pre>
            </li>
          </ol>
          <p>
            Then following steps are used to achieve equal-power panning:
          </p>
          <ol>
            <li>
              <p>
                Left and right gain values are calculated as:
              </p>
              <pre class="highlight">
gainL = cos(x * Math.PI / 2);
gainR = sin(x * Math.PI / 2);
        </pre>
            </li>
            <li>
              <p>
                For mono input, the stereo output is calculated as:
              </p>
              <pre class="highlight">
outputL = input * gainL;
outputR = input * gainR;
        </pre>
              <p>
                Else for stereo input, the output is calculated as:
              </p>
              <pre class="highlight">
if (pan &lt;= 0) {
  // Pass through inputL to outputL and equal-power pan inputR as in mono case.
  outputL = inputL + inputR * gainL;
  outputR = inputR * gainR;
} else {
  // Pass through inputR to outputR and equal-power pan inputR as in mono case.
  outputL = inputL * gainL;
  outputR = inputR + inputL * gainR;
}
        </pre>
            </li>
          </ol>
        </section>
        <section>
          <h4>
            HRTF panning (stereo only)
          </h4>
          <p>
            This requires a set of <a href=
            "https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a>
            (Head-related Transfer Function) impulse responses recorded at a
            variety of azimuths and elevations. The implementation requires a
            highly optimized convolution function. It is somewhat more costly
            than "equalpower", but provides more perceptually spatialized
            sound.
          </p>
          <figure>
            <img alt="HRTF panner" src="images/HRTF_panner.png">
            <figcaption>
              A diagram showing the process of panning a source using HRTF.
            </figcaption>
          </figure>
        </section>
      </section>
      <section>
        <h3 id="Spatialization-distance-effects">
          Distance Effects
        </h3>
        <p>
          Sounds which are closer are louder, while sounds further away are
          quieter. Exactly <em>how</em> a sound's volume changes according to
          distance from the listener depends on the <em>distanceModel</em>
          attribute.
        </p>
        <p>
          During audio rendering, a <em>distance</em> value will be calculated
          based on the panner and listener positions according to:
        </p>
        <pre class="highlight">
  v = panner.position - listener.position
  </pre>
        <pre class="highlight">
  distance = sqrt(dot(v, v))
  </pre>
        <p>
          <em>distance</em> will then be used to calculate
          <em>distanceGain</em> which depends on the <em>distanceModel</em>
          attribute. See the <a href=
          "#idl-def-DistanceModelType">distanceModel</a> section for details of
          how this is calculated for each distance model. The value computed by
          the <a href="#idl-def-DistanceModelType">distanceModel</a> equations
          are to be clamped to [0, 1].
        </p>
        <p>
          As part of its processing, the <a><code>PannerNode</code></a>
          scales/multiplies the input audio signal by <em>distanceGain</em> to
          make distant sounds quieter and nearer ones louder.
        </p>
      </section>
      <section>
        <h3 id="Spatialization-sound-cones">
          Sound Cones
        </h3>
        <p>
          The listener and each sound source have an orientation vector
          describing which way they are facing. Each sound source's sound
          projection characteristics are described by an inner and outer "cone"
          describing the sound intensity as a function of the source/listener
          angle from the source's orientation vector. Thus, a sound source
          pointing directly at the listener will be louder than if it is
          pointed off-axis. Sound sources can also be omni-directional.
        </p>
        <p>
          The following algorithm must be used to calculate the gain
          contribution due to the cone effect, given the source (the
          <a><code>PannerNode</code></a>) and the listener:
        </p>
        <pre class="highlight">
  if (source.orientation.isZero() || ((source.coneInnerAngle == 0) &amp;& (source.coneOuterAngle == 0)))
      return 1; // no cone specified - unity gain

  // Normalized source-listener vector
  vec3 sourceToListener = listener.position - source.position;
  sourceToListener.normalize();

  vec3 normalizedSourceOrientation = source.orientation;
  normalizedSourceOrientation.normalize();

  // Angle between the source orientation vector and the source-listener vector
  float dotProduct = sourceToListener.dot(normalizedSourceOrientation);
  float angle = 180 * acos(dotProduct) / PI;
  float absAngle = fabs(angle);

  // Divide by 2 here since API is entire angle (not half-angle)
  float absInnerAngle = fabs(source.coneInnerAngle) / 2;
  float absOuterAngle = fabs(source.coneOuterAngle) / 2;
  float gain = 1;

  if (absAngle &lt;= absInnerAngle)
      // No attenuation
      gain = 1;
  else if (absAngle &gt;= absOuterAngle)
      // Max attenuation
      gain = source.coneOuterGain;
  else {
      // Between inner and outer cones
      // inner -&gt; outer, x goes from 0 -&gt; 1
      float x = (absAngle - absInnerAngle) / (absOuterAngle - absInnerAngle);
      gain = (1 - x) + source.coneOuterGain * x;
  }

  return gain;
  </pre>
      </section>
      <section>
        <h3 id="Spatialization-doppler-shift">
          Doppler Shift
        </h3>
        <ul>
          <li>Introduces a pitch shift which can realistically simulate moving
          sources.
          </li>
          <li>Depends on: source / listener velocity vectors, speed of sound,
          doppler factor.
          </li>
        </ul>
        <p>
          The following algorithm must be used to calculate the doppler shift
          value which is used as an additional playback rate scalar for all
          <a><code>AudioBufferSourceNode</code></a>s connecting directly or
          indirectly to the <a><code>PannerNode</code></a>:
        </p>
        <pre class="highlight">
  float dopplerShift = 1; // Initialize to default value
  float dopplerFactor = listener.dopplerFactor;

  if (dopplerFactor &gt; 0) {
      float speedOfSound = listener.speedOfSound;

      // Don't bother if both source and listener have no velocity.
      if (!source.velocity.isZero() || !listener.velocity.isZero()) {
          // Calculate the source to listener vector.
          vec3 sourceToListener = source.position - listener.position;

          float sourceListenerMagnitude = sourceToListener.length();

          float listenerProjection = sourceToListener.dot(listener.velocity) / sourceListenerMagnitude;
          float sourceProjection = sourceToListener.dot(source.velocity) / sourceListenerMagnitude;

          listenerProjection = -listenerProjection;
          sourceProjection = -sourceProjection;

          float scaledSpeedOfSound = speedOfSound / dopplerFactor;
          listenerProjection = min(listenerProjection, scaledSpeedOfSound);
          sourceProjection = min(sourceProjection, scaledSpeedOfSound);

          dopplerShift = ((speedOfSound - dopplerFactor * listenerProjection) / (speedOfSound - dopplerFactor * sourceProjection));
          fixNANs(dopplerShift); // Avoid illegal values

          // Limit the pitch shifting to 4 octaves up and 3 octaves down.
          dopplerShift = min(dopplerShift, 16);
          dopplerShift = max(dopplerShift, 0.125);
      }
  }
  </pre>
      </section>
    </section>
    <section>
      <h2 id="Convolution">
        Linear Effects using Convolution
      </h2>
      <section>
        <h3 id="Convolution-background">
          Background
        </h3>
        <p>
          <a href="https://en.wikipedia.org/wiki/Convolution">Convolution</a>
          is a mathematical process which can be applied to an audio signal to
          achieve many interesting high-quality linear effects. Very often, the
          effect is used to simulate an acoustic space such as a concert hall,
          cathedral, or outdoor amphitheater. It can also be used for complex
          filter effects, like a muffled sound coming from inside a closet,
          sound underwater, sound coming through a telephone, or playing
          through a vintage speaker cabinet. This technique is very commonly
          used in major motion picture and music production and is considered
          to be extremely versatile and of high quality.
        </p>
        <p>
          Each unique effect is defined by an <em>impulse response</em>. An
          impulse response can be represented as an audio file and <a href=
          "#recording-impulse-responses">can be recorded</a> from a real
          acoustic space such as a cave, or can be synthetically generated
          through a great variety of techniques.
        </p>
      </section>
    </section>
    <section>
      <h3 id="Convolution-motivation">
        Motivation for use as a Standard
      </h3>
      <p>
        A key feature of many game audio engines (OpenAL, FMOD, Creative's EAX,
        Microsoft's XACT Audio, etc.) is a reverberation effect for simulating
        the sound of being in an acoustic space. The code used to generate this
        effect has generally been custom and algorithmic (generally using a
        hand-tweaked set of delay lines and allpass filters which feedback into
        each other). In nearly all cases, not only is the implementation
        custom, but the code is proprietary and closed-source, each company
        adding its own "black magic" to achieve its unique quality. Each
        implementation being custom with a different set of parameters makes it
        impossible to achieve a uniform desired effect. And the code being
        proprietary makes it impossible to adopt a single one of the
        implementations as a standard. Additionally, algorithmic reverberation
        effects are limited to a relatively narrow range of different effects,
        regardless of how the parameters are tweaked.
      </p>
      <p>
        A convolution effect solves these problems by using a very precisely
        defined mathematical algorithm as the basis of its processing. An
        impulse response represents an exact sound effect to be applied to an
        audio stream and is easily represented by an audio file which can be
        referenced by URL. The range of possible effects is enormous.
      </p>
    </section>
    <section>
      <h3 id="Convolution-implementation-guide">
        Implementation Guide
      </h3>
      <p>
        Linear convolution can be implemented efficiently. Here are some
        <a href="convolution.html">notes</a> describing how it can be
        practically implemented.
      </p>
    </section>
    <section>
      <h3 id="Convolution-reverb-effect">
        Reverb Effect (with matrixing)
      </h3>
      <p class="norm">
        This section is normative.
      </p>
      <p>
        In the general case the source has N input channels, the impulse
        response has K channels, and the playback system has M output channels.
        Thus it's a matter of how to matrix these channels to achieve the final
        result.
      </p>
      <p>
        The subset of N, M, K below must be implemented (note that the first
        image in the diagram is just illustrating the general case and is not
        normative, while the following images are normative). Without loss of
        generality, developers desiring more complex and arbitrary matrixing
        can use multiple <a><code>ConvolverNode</code></a> objects in
        conjunction with an <a><code>ChannelMergerNode</code></a>.
      </p>
      <p>
        Single channel convolution operates on a mono audio input, using a mono
        impulse response, and generating a mono output. To achieve a more
        spacious sound, 2 channel audio inputs and 1, 2, or 4 channel impulse
        responses will be considered. The following diagram, illustrates the
        common cases for stereo playback where N and M are 1 or 2 and K is 1,
        2, or 4.
      </p>
      <figure>
        <img alt="reverb matrixing" src="images/reverb-matrixing.png">
        <figcaption>
          A graphical representation of the different input and output channel
          count possibilities when using a <a><code>ConvolverNode</code></a>.
        </figcaption>
      </figure>
    </section>
    <section>
      <h2 id="Performance">
        Performance Considerations
      </h2>
      <section>
        <h3>
          Latency
        </h3>
        <figure>
          <img alt="latency" src="images/latency.png">
          <figcaption>
            Use cases in which the latency can be important
          </figcaption>
        </figure>
        <p>
          For web applications, the time delay between mouse and keyboard
          events (keydown, mousedown, etc.) and a sound being heard is
          important.
        </p>
        <p>
          This time delay is called latency and is caused by several factors
          (input device latency, internal buffering latency, DSP processing
          latency, output device latency, distance of user's ears from
          speakers, etc.), and is cummulative. The larger this latency is, the
          less satisfying the user's experience is going to be. In the extreme,
          it can make musical production or game-play impossible. At moderate
          levels it can affect timing and give the impression of sounds lagging
          behind or the game being non-responsive. For musical applications the
          timing problems affect rhythm. For gaming, the timing problems affect
          precision of gameplay. For interactive applications, it generally
          cheapens the users experience much in the same way that very low
          animation frame-rates do. Depending on the application, a reasonable
          latency can be from as low as 3-6 milliseconds to 25-50 milliseconds.
        </p>
      </section>
      <section>
        <h3>
          Audio Buffer Copying
        </h3>
        <p>
          When an <a href="#acquire-the-content">acquire the content</a>
          operation is performed on an <a>AudioBuffer</a>, the entire operation
          can usually be implemented without copying channel data. In
          particular, the last step should be performed lazily at the next
          <a href=
          "#widl-AudioBuffer-getChannelData-Float32Array-unsigned-long-channel">
          <code>getChannelData</code></a> call. That means a sequence of
          consecutive <a href="#acquire-the-content">acquire the contents</a>
          operations with no intervening <a href=
          "#widl-AudioBuffer-getChannelData-Float32Array-unsigned-long-channel">
          <code>getChannelData</code></a> (e.g. multiple
          <a><code>AudioBufferSourceNode</code></a>s playing the same
          <a><code>AudioBuffer</code></a>) can be implemented with no
          allocations or copying.
        </p>
        <p>
          Implementations can perform an additional optimization: if <a href=
          "#widl-AudioBuffer-getChannelData-Float32Array-unsigned-long-channel">
          getChannelData</a> is called on an <a>AudioBuffer</a>, fresh
          <code>ArrayBuffer</code>s have not yet been allocated, but all
          invokers of previous <a href="#acquire-the-content">acquire the
          content</a> operations on an <a>AudioBuffer</a> have stopped using
          the <a>AudioBuffer</a>'s data, the raw data buffers can be recycled
          for use with new <a>AudioBuffer</a>s, avoiding any reallocation or
          copying of the channel data.
        </p>
      </section>
      <section class="informative">
        <h3>
          AudioParam Transitions
        </h3>
        <p>
          While no automatic smoothing is done when directly setting the
          <a href="#widl-AudioParam-value"><code>value</code></a> attribute of
          an <a><code>AudioParam</code></a>, for certain parameters, smooth
          transition are preferable to directly setting the value.
        </p>
        <p>
          Using the <a href=
          "#widl-AudioParam-setTargetAtTime-void-float-target-double-startTime-float-timeConstant">
          <code>setTargetAtTime</code></a> method with a low
          <code>timeConstant</code> allows authors to perform a smooth
          transition.
        </p>
      </section>
      <section>
        <h3>
          Audio Glitching
        </h3>
        <p>
          Audio glitches are caused by an interruption of the normal continuous
          audio stream, resulting in loud clicks and pops. It is considered to
          be a catastrophic failure of a multi-media system and must be
          avoided. It can be caused by problems with the threads responsible
          for delivering the audio stream to the hardware, such as scheduling
          latencies caused by threads not having the proper priority and
          time-constraints. It can also be caused by the audio DSP trying to do
          more work than is possible in real-time given the CPU's speed.
        </p>
      </section>
      <section>
        <h3 id="JavaScriptPerformance">
          JavaScript Issues with Real-Time Processing and Synthesis:
        </h3>While processing audio in JavaScript, it is extremely challenging
        to get reliable, glitch-free audio while achieving a reasonably
        low-latency, especially under heavy processor load.
        <ul>
          <li>JavaScript is very much slower than heavily optimized C++ code
          and is not able to take advantage of SSE optimizations and
          multi-threading which is critical for getting good performance on
          today's processors. Optimized native code can be on the order of
          twenty times faster for processing FFTs as compared with JavaScript.
          It is not efficient enough for heavy-duty processing of audio such as
          convolution and 3D spatialization of large numbers of audio sources.
          </li>
          <li>setInterval() and XHR handling will steal time from the audio
          processing. In a reasonably complex game, some JavaScript resources
          will be needed for game physics and graphics. This creates challenges
          because audio rendering is deadline driven (to avoid glitches and get
          low enough latency).
          </li>
          <li>JavaScript does not run in a real-time processing thread and thus
          can be pre-empted by many other threads running on the system.
          </li>
          <li>Garbage Collection (and autorelease pools on Mac OS X) can cause
          unpredictable delay on a JavaScript thread.
          </li>
          <li>Multiple JavaScript contexts can be running on the main thread,
          stealing time from the context doing the processing.
          </li>
          <li>Other code (other than JavaScript) such as page rendering runs on
          the main thread.
          </li>
          <li>Locks can be taken and memory is allocated on the JavaScript
          thread. This can cause additional thread preemption.
          </li>
        </ul>The problems are even more difficult with today's generation of
        mobile devices which have processors with relatively poor performance
        and power consumption / battery-life issues.
      </section>
    </section>
    <section class="informative">
      <h2 id="SecurityConsiderations">
        Security Considerations
      </h2>
    </section>
    <section class="informative">
      <h2 id="PrivacyConsiderations">
        Privacy Considerations
      </h2>
      <p>
        When giving various information on available
        <a><code>AudioNode</code></a>s, the Web Audio API potentially exposes
        information on characteristic features of the client (such as audio
        hardware sample-rate) to any page that makes use of the
        <a><code>AudioNode</code></a> interface. Additionally, timing
        information can be collected through the
        <a><code>AnalyserNode</code></a> or
        <a><code>ScriptProcessorNode</code></a> interface. The information
        could subsequently be used to create a fingerprint of the client.
      </p>
      <p>
        Currently audio input is not specified in this document, but it will
        involve gaining access to the client machine's audio input or
        microphone. This will require asking the user for permission in an
        appropriate way, probably via the <a href=
        "https://w3c.github.io/mediacapture-main/#dom-mediadevices-getusermedia">
        getUserMedia() API</a>.
      </p>
    </section>
    <section>
      <h2 id="requirements">
        Requirements and Use Cases
      </h2>
      <p>
        Please see [[webaudio-usecases]].
      </p>
    </section>
    <section>
      <h2>
        Acknowledgements
      </h2>
      <p>
        This specification is the collective work of the W3C <a href=
        "http://www.w3.org/2011/audio/">Audio Working Group</a>.
      </p>
      <p>
        Members of the Working Group are (at the time of writing, and by
        alphabetical order):<br>
        Adenot, Paul (Mozilla Foundation) - Specification Co-editor; Akhgari,
        Ehsan (Mozilla Foundation); Berkovitz, Joe (Hal Leonard/Noteflight) –
        WG Chair; Bossart, Pierre (Intel Corporation); Carlson, Eric (Apple,
        Inc.); Choi, Hongchan (Google, Inc.); Geelnard, Marcus (Opera
        Software); Goode, Adam (Google, Inc.); Gregan, Matthew (Mozilla
        Foundation); Hofmann, Bill (Dolby Laboratories); Jägenstedt, Philip
        (Opera Software); Kalliokoski, Jussi (Invited Expert); Lilley, Chris
        (W3C Staff); Lowis, Chris (Invited Expert. WG co-chair from December
        2012 to September 2013, affiliated with British Broadcasting
        Corporation); Mandyam, Giridhar (Qualcomm Innovation Center, Inc);
        Noble, Jer (Apple, Inc.); O'Callahan, Robert(Mozilla Foundation);
        Onumonu, Anthony (British Broadcasting Corporation); Paradis, Matthew
        (British Broadcasting Corporation); Raman, T.V. (Google, Inc.);
        Schepers, Doug (W3C/MIT); Shires, Glen (Google, Inc.); Smith, Michael
        (W3C/Keio); Thereaux, Olivier (British Broadcasting Corporation); Toy,
        Raymond (Google, Inc.); Verdie, Jean-Charles (MStar Semiconductor,
        Inc.); Wilson, Chris (Google,Inc.) - Specification Co-editor; ZERGAOUI,
        Mohamed (INNOVIMAX)
      </p>
      <p>
        Former members of the Working Group and contributors to the
        specification include:<br>
        Caceres, Marcos (Invited Expert); Cardoso, Gabriel (INRIA); Chen, Bin
        (Baidu, Inc.); MacDonald, Alistair (W3C Invited Experts) — WG co-chair
        from March 2011 to July 2012; Michel, Thierry (W3C/ERCIM); Rogers,
        Chris (Google, Inc.) – Specification Editor until August 2013; Wei,
        James (Intel Corporation);
      </p>
    </section>
    <section>
      <h2 id="ChangeLog">
        Web Audio API Change Log
      </h2>
      <p>
        See <a href="changelog.html">changelog.html</a>.
      </p>
    </section>
  </body>
</html>

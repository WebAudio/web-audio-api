<!DOCTYPE html>
<html>
  <head>
    <title>
      Web Audio API
    </title>
    <meta charset="utf-8">
    <script src='respec-w3c-common-3.3.2.js' class='remove'>
    </script>
    <script class='remove'>
    var respecConfig = {
        specStatus: "ED",
        shortName:  "webaudio",
        edDraftURI: "https://webaudio.github.io/web-audio-api/",
        editors: [
              {   name:       "Paul Adenot",
                  company:    "Mozilla",
                  companyURL: "https://www.mozilla.org/",
                  mailto:     "padenot@mozilla.com",
                  w3cid:      "62410" },
              {
                  name:       "Raymond Toy",
                  company:    "Google, Inc.",
                  companyURL: "https://www.google.com/",
                  mailto:     "rtoy@google.com",
                  w3cid:      "66910" },
        ],

        license:      "w3c-software-doc",

        previousMaturity: "WD",
        previousPublishDate:  "2013-10-10",
        previousURI:  "http://www.w3.org/TR/2012/WD-webaudio-20121213/",
        wg:           "Audio Working Group",
        wgURI:        "http://www.w3.org/2011/audio/",
        wgPublicList: "public-audio",
        wgPatentURI:  "http://www.w3.org/2004/01/pp-impl/46884/status",
        tocIntroductory: true,
        copyrightStart: 2013,
        otherLinks: [
          {
            key: "Previous editors",
            data : [{value: "Chris Wilson (Until January 2016)"},
                    {value: "Chris Rogers (Until August 2013)"}] },
          {
            key: "Repository",
            href: "https://github.com/WebAudio/web-audio-api" },
          {
            key: "Bug tracker",
            href: "https://github.com/WebAudio/web-audio-api/issues?state=open" },
        ]
    };
    </script>
    <script>
    function findBadLink () {
      var old = document.querySelectorAll(".badLink");
      for (var i = 0 ; i < old.length; i++) {
        nodes[i].style.backgroundColor = "";
        nodes[i].classList.remove("badLink");
      }
      var nodes = document.querySelectorAll('a');

      for (var i = 0; i < nodes.length; i++) {
          if (nodes[i].href) {
              var id =  nodes[i].href.split("/");
              id = id[id.length - 1];
              if (id.length != 0) {
                  if (id[0] == "#") {
                      if (document.querySelectorAll(id).length == 0) {
                        nodes[i].style.backgroundColor = "red";
                        nodes[i].classList.add("badLink");
                        console.log(nodes[i].textContent);
                    }
                 }
              }
          } else {
            nodes[i].style.backgroundColor = "red";
            nodes[i].classList.add("badLink");
          }
      }
    }

    function findMissingLink() {
      var codetags = document.querySelectorAll("code");

      for (var i = 0; i < codetags.length; i++) {
        if (!(codetags[i].parentNode instanceof HTMLAnchorElement) ||
            codetags[i].parentNode.href == "") {
          codetags[i].style.backgroundColor = 'hotpink';
          codetags[i].style.color  = 'yellow';

          console.log(codetags[i].innerHTML);
        }
      }
    }

    window.addEventListener("DOMContentLoaded", function () {
      "use strict";
      // Don't run MathJax during spec generation
      if (navigator.userAgent === "respec2html") {
        return;
      }
      new Promise(function (resolve, reject) {
        var mathjax = document.createElement('script');
        var url = "https://www.w3.org/scripts/MathJax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
        // Safari doesn't (yet) support load event on scripts so we have to poll. So ðŸ˜¢.
        var id = setInterval(function () {
          if (window.MathJax) {
            clearInterval(id);
            resolve();
          }
        }, 100);
        mathjax.onload = function () {
          clearInterval(id);
          resolve();
        };
        mathjax.onerror = function (err) {
          var error = (err instanceof Event)  ? new Error(err.message) : err;
          reject(error);
        };
        // Time out waiting after 20 seconds and reject.
        setTimeout(function(){
          mathjax.onerror(new Error("Loading timed out."));
        }, 20000);
        mathjax.id = "mathjax";
        mathjax.src = url;
        document.body.appendChild(mathjax);
      }).then(function () {
        MathJax.Hub.Config({
          tex2jax: {
            skipTags: ["script", "noscript", "style", "textarea", "code"]
          }
        });
      }).catch(function (error) {
        console.error(error);
      });
    });
    </script>
    <style>
    .todo {
      border: 1px solid red;
      background-color: rgba(255, 0, 0, 0.3);
    }
    .todo:before {
      content: "TODO:";
    }
    .synchronous:before {
      content: "âŒ› ";
    }
    .synchronous:hover{
      border-bottom: 1px dotted gray;
    }
    body#respecDocument {
      max-width: 60em;
    }
        .seclist > p {
                font-style: italic;
        }
    </style>
  </head>
  <body>
    <section id="abstract">
      <p>
        This specification describes a high-level JavaScript <abbr title=
        "Application Programming Interface">API</abbr> for processing and
        synthesizing audio in web applications. The primary paradigm is of an
        audio routing graph, where a number of <a><code>AudioNode</code></a>
        objects are connected together to define the overall audio rendering.
        The actual processing will primarily take place in the underlying
        implementation (typically optimized Assembly / C / C++ code), but
        <a href="#AudioWorker">direct JavaScript processing and synthesis</a>
        is also supported.
      </p>
      <p>
        The <a href="#introduction">introductory</a> section covers the
        motivation behind this specification.
      </p>
      <p>
        This API is designed to be used in conjunction with other APIs and
        elements on the web platform, notably: XMLHttpRequest [[XHR]] (using
        the <code>responseType</code> and <code>response</code> attributes).
        For games and interactive applications, it is anticipated to be used
        with the <code>canvas</code> 2D [[2dcontext]] and WebGL [[WEBGL]] 3D
        graphics APIs.
      </p>
    </section>
    <section id="sotd"></section>
    <section class="introductory">
      <h2>
        Introduction
      </h2>
      <section>
        <p>
          Audio on the web has been fairly primitive up to this point and until
          very recently has had to be delivered through plugins such as Flash
          and QuickTime. The introduction of the <code>audio</code> element in
          HTML5 is very important, allowing for basic streaming audio playback.
          But, it is not powerful enough to handle more complex audio
          applications. For sophisticated web-based games or interactive
          applications, another solution is required. It is a goal of this
          specification to include the capabilities found in modern game audio
          engines as well as some of the mixing, processing, and filtering
          tasks that are found in modern desktop audio production applications.
        </p>
        <p>
          The APIs have been designed with a wide variety of use cases
          [[webaudio-usecases]] in mind. Ideally, it should be able to support
          <i>any</i> use case which could reasonably be implemented with an
          optimized C++ engine controlled via JavaScript and run in a browser.
          That said, modern desktop audio software can have very advanced
          capabilities, some of which would be difficult or impossible to build
          with this system. Apple's Logic Audio is one such application which
          has support for external MIDI controllers, arbitrary plugin audio
          effects and synthesizers, highly optimized direct-to-disk audio file
          reading/writing, tightly integrated time-stretching, and so on.
          Nevertheless, the proposed system will be quite capable of supporting
          a large range of reasonably complex games and interactive
          applications, including musical ones. And it can be a very good
          complement to the more advanced graphics features offered by WebGL.
          The API has been designed so that more advanced capabilities can be
          added at a later time.
        </p>
      </section>
      <section>
        <h2 id="Features">
          Features
        </h2>
        <p>
          The API supports these primary features:
        </p>
        <ul>
          <li>
            <a href="#ModularRouting">Modular routing</a> for simple or complex
            mixing/effect architectures, including <a href=
            "#mixer-gain-structure">multiple sends and submixes</a>.
          </li>
          <li>High dynamic range, using 32bits floats for internal processing.
          </li>
          <li>
            <a href="#AudioParam">Sample-accurate scheduled sound playback</a>
            with low <a href="#latency">latency</a> for musical applications
            requiring a very high degree of rhythmic precision such as drum
            machines and sequencers. This also includes the possibility of
            <a href="#DynamicLifetime">dynamic creation</a> of effects.
          </li>
          <li>Automation of audio parameters for envelopes, fade-ins /
          fade-outs, granular effects, filter sweeps, LFOs etc.
          </li>
          <li>Flexible handling of channels in an audio stream, allowing them
          to be split and merged.
          </li>
          <li>Processing of audio sources from an <code>audio</code> or <code>
            video</code> <a href="#MediaElementAudioSourceNode">media
            element</a>.
          </li>
          <li>Processing live audio input using a <a href=
          "#MediaStreamAudioSourceNode">MediaStream</a> from getUserMedia().
          </li>
          <li>Integration with WebRTC
            <ul>
              <li>Processing audio received from a remote peer using a
              <a><code>MediaStreamAudioSourceNode</code></a> and [[!webrtc]].
              </li>
              <li>Sending a generated or processed audio stream to a remote
              peer using a <a><code>MediaStreamAudioDestinationNode</code></a>
              and [[!webrtc]].
              </li>
            </ul>
          </li>
          <li>Audio stream synthesis and processing <a href=
          "#AudioWorker">directly in JavaScript</a>.
          </li>
          <li>
            <a href="#Spatialization">Spatialized audio</a> supporting a wide
            range of 3D games and immersive environments:
            <ul>
              <li>Panning models: equalpower, HRTF, pass-through
              </li>
              <li>Distance Attenuation
              </li>
              <li>Sound Cones
              </li>
              <li>Obstruction / Occlusion
              </li>
              <li>Source / Listener based
              </li>
            </ul>
          </li>
          <li>A <a href="convolution.html">convolution engine</a> for a wide
          range of linear effects, especially very high-quality room effects.
          Here are some examples of possible effects:
            <ul>
              <li>Small / large room
              </li>
              <li>Cathedral
              </li>
              <li>Concert hall
              </li>
              <li>Cave
              </li>
              <li>Tunnel
              </li>
              <li>Hallway
              </li>
              <li>Forest
              </li>
              <li>Amphitheater
              </li>
              <li>Sound of a distant room through a doorway
              </li>
              <li>Extreme filters
              </li>
              <li>Strange backwards effects
              </li>
              <li>Extreme comb filter effects
              </li>
            </ul>
          </li>
          <li>Dynamics compression for overall control and sweetening of the
          mix
          </li>
          <li>Efficient <a href="#the-analysernode-interface">real-time
          time-domain and frequency analysis / music visualizer support</a>
          </li>
          <li>Efficient biquad filters for lowpass, highpass, and other common
          filters.
          </li>
          <li>A Waveshaping effect for distortion and other non-linear effects
          </li>
          <li>Oscillators
          </li>
        </ul>
        <section>
          <h2 id="ModularRouting">
            Modular Routing
          </h2>
          <p>
            Modular routing allows arbitrary connections between different
            <a><code>AudioNode</code></a> objects. Each node can have
            <dfn>inputs</dfn> and/or <dfn>outputs</dfn>. A <dfn>source
            node</dfn> has no inputs and a single output. A <dfn>destination
            node</dfn> has one input and no outputs. Other nodes such as
            filters can be placed between the source and destination nodes. The
            developer doesn't have to worry about low-level stream format
            details when two objects are connected together; <a href=
            "#channel-up-mixing-and-down-mixing">the right thing just
            happens</a>. For example, if a mono audio stream is connected to a
            stereo input it should just mix to left and right channels <a href=
            "#channel-up-mixing-and-down-mixing">appropriately</a>.
          </p>
          <p>
            In the simplest case, a single source can be routed directly to the
            output. All routing occurs within an <a href=
            "#AudioContext"><code>AudioContext</code></a> containing a single
            <a href=
            "#AudioDestinationNode"><code>AudioDestinationNode</code></a>:
          </p>
          <figure>
            <img alt="modular routing" src="images/modular-routing1.png">
            <figcaption>
              A simple example of modular routing.
            </figcaption>
          </figure>
          <p>
            Illustrating this simple routing, here's a simple example playing a
            single sound:
          </p>
          <pre class="example">

var context = new AudioContext();

function playSound() {
    var source = context.createBufferSource();
    source.buffer = dogBarkingBuffer;
    source.connect(context.destination);
    source.start(0);
}
</pre>
          <p>
            Here's a more complex example with three sources and a convolution
            reverb send with a dynamics compressor at the final output stage:
          </p>
          <figure>
            <img alt="modular routing2" src="images/modular-routing2.png">
            <figcaption>
              A more complex example of modular routing.
            </figcaption>
          </figure>
          <pre class="example">

var context = 0;
var compressor = 0;
var reverb = 0;

var source1 = 0;
var source2 = 0;
var source3 = 0;

var lowpassFilter = 0;
var waveShaper = 0;
var panner = 0;

var dry1 = 0;
var dry2 = 0;
var dry3 = 0;

var wet1 = 0;
var wet2 = 0;
var wet3 = 0;

var masterDry = 0;
var masterWet = 0;

function setupRoutingGraph () {
    context = new AudioContext();

    // Create the effects nodes.
    lowpassFilter = context.createBiquadFilter();
    waveShaper = context.createWaveShaper();
    panner = context.createPanner();
    compressor = context.createDynamicsCompressor();
    reverb = context.createConvolver();

    // Create master wet and dry.
    masterDry = context.createGain();
    masterWet = context.createGain();

    // Connect final compressor to final destination.
    compressor.connect(context.destination);

    // Connect master dry and wet to compressor.
    masterDry.connect(compressor);
    masterWet.connect(compressor);

    // Connect reverb to master wet.
    reverb.connect(masterWet);

    // Create a few sources.
    source1 = context.createBufferSource();
    source2 = context.createBufferSource();
    source3 = context.createOscillator();

    source1.buffer = manTalkingBuffer;
    source2.buffer = footstepsBuffer;
    source3.frequency.value = 440;

    // Connect source1
    dry1 = context.createGain();
    wet1 = context.createGain();
    source1.connect(lowpassFilter);
    lowpassFilter.connect(dry1);
    lowpassFilter.connect(wet1);
    dry1.connect(masterDry);
    wet1.connect(reverb);

    // Connect source2
    dry2 = context.createGain();
    wet2 = context.createGain();
    source2.connect(waveShaper);
    waveShaper.connect(dry2);
    waveShaper.connect(wet2);
    dry2.connect(masterDry);
    wet2.connect(reverb);

    // Connect source3
    dry3 = context.createGain();
    wet3 = context.createGain();
    source3.connect(panner);
    panner.connect(dry3);
    panner.connect(wet3);
    dry3.connect(masterDry);
    wet3.connect(reverb);

    // Start the sources now.
    source1.start(0);
    source2.start(0);
    source3.start(0);
}
</pre>
          <p>
            Modular routing also permits the output of
            <a><code>AudioNode</code></a>s to be routed to an
            <a><code>AudioParam</code></a> parameter that controls the behavior
            of a different <a><code>AudioNode</code></a>. In this scenario, the
            output of a node can act as a modulation signal rather than an
            input signal.
          </p>
          <figure>
            <img alt="modular routing3" src="images/modular-routing3.png">
            <figcaption>
              Modular routing illustrating one Oscillator modulating the
              frequency of another.
            </figcaption>
          </figure>
          <pre class="example">
function setupRoutingGraph() {
  var context = new AudioContext();

  // Create the low frequency oscillator that supplies the modulation signal
  var lfo = context.createOscillator();
  lfo.frequency.value = 1.0;

  // Create the high frequency oscillator to be modulated
  var hfo = context.createOscillator();
  hfo.frequency.value = 440.0;

  // Create a gain node whose gain determines the amplitude of the modulation signal
  var modulationGain = context.createGain();
  modulationGain.gain.value = 50;

  // Configure the graph and start the oscillators
  lfo.connect(modulationGain);
  modulationGain.connect(hfo.detune);
  hfo.connect(context.destination);
  hfo.start(0);
  lfo.start(0);
}
</pre>
        </section>
      </section>
      <section>
        <h2 id="APIOverview">
          API Overview
        </h2>
        <p>
          The interfaces defined are:
        </p>
        <ul>
          <li>An <a class="dfnref" href="#AudioContext">AudioContext</a>
          interface, which contains an audio signal graph representing
          connections betweens <a><code>AudioNode</code></a>s.
          </li>
          <li>An <a><code>AudioNode</code></a> interface, which represents
          audio sources, audio outputs, and intermediate processing modules.
          <a><code>AudioNode</code></a>s can be dynamically connected together
          in a <a href="#ModularRouting">modular fashion</a>.
          <a><code>AudioNode</code></a>s exist in the context of an
          <a><code>AudioContext</code></a>
          </li>
          <li>An <a><code>AudioDestinationNode</code></a> interface, an
          <a><code>AudioNode</code></a> subclass representing the final
          destination for all rendered audio.
          </li>
          <li>An <a><code>AudioBuffer</code></a> interface, for working with
          memory-resident audio assets. These can represent one-shot sounds, or
          longer audio clips.
          </li>
          <li>An <a><code>AudioBufferSourceNode</code></a> interface, an
          <a><code>AudioNode</code></a> which generates audio from an
          AudioBuffer.
          </li>
          <li>A <a><code>MediaElementAudioSourceNode</code></a> interface, an
          <a><code>AudioNode</code></a> which is the audio source from an
          <code>audio</code>, <code>video</code>, or other media element.
          </li>
          <li>A <a><code>MediaStreamAudioSourceNode</code></a> interface, an
          <a><code>AudioNode</code></a> which is the audio source from a
          MediaStream such as live audio input, or from a remote peer.
          </li>
          <li>A <a><code>MediaStreamAudioDestinationNode</code></a> interface,
          an <a><code>AudioNode</code></a> which is the audio destination to a
          MediaStream sent to a remote peer.
          </li>
          <li>An <a><code>AudioWorker</code></a> interface representing a
          factory for creating custom nodes that can process audio directly in
          JavaScript.
          </li>
          <li>An <a><code>AudioWorkerNode</code></a> interface, an
          <a><code>AudioNode</code></a> representing a node processed in an
          AudioWorker.
          </li>
          <li>An <a><code>AudioWorkerGlobalScope</code></a> interface, the
          context in which AudioWorker processing scripts run.
          </li>
          <li>An <a><code>AudioWorkerNodeProcessor</code></a> interface,
          representing a single node instance inside an audio worker.
          </li>
          <li>An <a><code>AudioParam</code></a> interface, for controlling an
          individual aspect of an <a><code>AudioNode</code></a>'s functioning,
          such as volume.
          </li>
          <li>An <a><code>GainNode</code></a> interface, an
          <a><code>AudioNode</code></a> for explicit gain control. Because
          inputs to <a><code>AudioNode</code></a>s support multiple connections
          (as a unity-gain summing junction), mixers can be <a href=
          "#mixer-gain-structure">easily built</a> with GainNodes.
          </li>
          <li>A <a><code>BiquadFilterNode</code></a> interface, an
          <a><code>AudioNode</code></a> for common low-order filters such as:
            <ul>
              <li>Low Pass
              </li>
              <li>High Pass
              </li>
              <li>Band Pass
              </li>
              <li>Low Shelf
              </li>
              <li>High Shelf
              </li>
              <li>Peaking
              </li>
              <li>Notch
              </li>
              <li>Allpass
              </li>
            </ul>
          </li>
          <li>A <a><code>IIRFilterNode</code></a> interface, an
          <a><code>AudioNode</code></a> for a general IIR filter.
          </li>
          <li>A <a><code>DelayNode</code></a> interface, an
          <a><code>AudioNode</code></a> which applies a dynamically adjustable
          variable delay.
          </li>
          <li>A <a><code>StereoPannerNode</code></a> interface, an
          <a><code>AudioNode</code></a> for equal-power positioning of audio
          input in a stereo stream.
          </li>
          <li>A <a><code>ConvolverNode</code></a> interface, an
          <a><code>AudioNode</code></a> for applying a <a href=
          "convolution.html">real-time linear effect</a> (such as the sound of
          a concert hall).
          </li>
          <li>A <a><code>AnalyserNode</code></a> interface, an
          <a><code>AudioNode</code></a> for use with music visualizers, or
          other visualization applications.
          </li>
          <li>A <a><code>ChannelSplitterNode</code></a> interface, an <a><code>
            AudioNode</code></a> for accessing the individual channels of an
            audio stream in the routing graph.
          </li>
          <li>A <a><code>ChannelMergerNode</code></a> interface, an
          <a><code>AudioNode</code></a> for combining channels from multiple
          audio streams into a single audio stream.
          </li>
          <li>A <a><code>DynamicsCompressorNode</code></a> interface, an
          <a><code>AudioNode</code></a> for dynamics compression.
          </li>
          <li>A <a><code>WaveShaperNode</code></a> interface, an
          <a><code>AudioNode</code></a> which applies a non-linear waveshaping
          effect for distortion and other more subtle warming effects.
          </li>
          <li>A <a><code>OscillatorNode</code></a> interface, an
          <a><code>AudioNode</code></a> for generating a periodic waveform.
          </li>
        </ul>
        <p>
          There are also several features that have been deprecated from the
          Web Audio API but not yet removed, pending implementation experience
          of their replacements:
        </p>
        <ul>
          <li>A <a><code>ScriptProcessorNode</code></a> interface, an <a><code>
            AudioNode</code></a> for generating or processing audio directly in
            JavaScript.
          </li>
          <li>An <a><code>AudioProcessingEvent</code></a> interface, which is
          an event type used with <a><code>ScriptProcessorNode</code></a>
          objects.
          </li>
        </ul>
      </section>
    </section>
    <section id="conformance">
      <p>
        The following conformance classes are defined by this specification:
      </p>
      <dl>
        <dt>
          <dfn id="dfn-conforming-implementation">conforming
          implementation</dfn>
        </dt>
        <dd>
          <p>
            A user agent is considered to be a <a class="dfnref" href=
            "#dfn-conforming-implementation">conforming implementation</a> if
            it satisfies all of the MUST-, REQUIRED- and SHALL-level criteria
            in this specification that apply to implementations.
          </p>
        </dd>
      </dl>
      <p>
        User agents that use ECMAScript to implement the APIs defined in this
        specification must implement them in a manner consistent with the
        ECMAScript Bindings defined in the Web IDL specification [[!WEBIDL]] as
        this specification uses that specification and terminology.
      </p>
    </section>
    <section id="audioapi">
      <h2 id="API">
        The Audio API
      </h2>
      <section>
        <h2 id="BaseAudioContext">
          The BaseAudioContext Interface
        </h2>
        <p>
          This interface represents a set of <a><code>AudioNode</code></a>
          objects and their connections. It allows for arbitrary routing of
          signals to an <a><code>AudioDestinationNode</code></a>. Nodes are
          created from the context and are then <a href=
          "#ModularRouting">connected</a> together.
        </p>
        <p>
          <a><code>BaseAudioContext</code></a> is not instantiated directly,
          but is instead extended by the concrete interfaces
          <a><code>AudioContext</code></a> (for real-time rendering) and
          <a><code>OfflineAudioContext</code></a> (for offline rendering).
        </p>
        <dl title="enum AudioContextState" class="idl">
          <dt>
            suspended
          </dt>
          <dd>
            This context is currently suspended (context time is not
            proceeding, audio hardware may be powered down/released).
          </dd>
          <dt>
            running
          </dt>
          <dd>
            Audio is being processed.
          </dd>
          <dt>
            closed
          </dt>
          <dd>
            This context has been released, and can no longer be used to
            process audio. All system audio resources have been released.
            <span class="synchronous">Attempts to create new Nodes on the
            AudioContext will throw InvalidStateError</span>. (AudioBuffers may
            still be created, through <a href=
            "#widl-BaseAudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate">
            createBuffer</a> or <a href=
            "#widl-BaseAudioContext-decodeAudioData-Promise-AudioBuffer--ArrayBuffer-audioData-DecodeSuccessCallback-successCallback-DecodeErrorCallback-errorCallback">
            decodeAudioData</a>.)
          </dd>
        </dl>
        <dl title="enum AudioContextLatencyCategory" class="idl">
          <dt>
            balanced
          </dt>
          <dd>
            Balance audio output latency and power consumption.
          </dd>
          <dt>
            interactive
          </dt>
          <dd>
            Provide the lowest audio output latency possible without glitching.
            This is the default.
          </dd>
          <dt>
            playback
          </dt>
          <dd>
            Prioritize sustained playback without interruption over audio
            output latency. Lowest power consumption.
          </dd>
        </dl>
        <dl title="interface BaseAudioContext : EventTarget" class="idl"
        data-merge="DecodeSuccessCallback DecodeErrorCallback">
          <dt>
            readonly attribute AudioDestinationNode destination
          </dt>
          <dd>
            <p>
              An <a href=
              "#AudioDestinationNode"><code>AudioDestinationNode</code></a>
              with a single input representing the final destination for all
              audio. Usually this will represent the actual audio hardware. All
              <a><code>AudioNode</code></a>s actively rendering audio will
              directly or indirectly connect to <a href=
              "#widl-BaseAudioContext-destination"><code>destination</code></a>.
            </p>
          </dd>
          <dt>
            readonly attribute float sampleRate
          </dt>
          <dd>
            <p>
              The sample rate (in sample-frames per second) at which the
              <a><code>BaseAudioContext</code></a> handles audio. It is assumed
              that all <a><code>AudioNode</code></a>s in the context run at
              this rate. In making this assumption, sample-rate converters or
              "varispeed" processors are not supported in real-time processing.
              The <dfn>Nyquist frequency</dfn> is half this sample-rate value.
            </p>
          </dd>
          <dt>
            readonly attribute double currentTime
          </dt>
          <dd>
            <p>
              This is the time in seconds of the sample frame immediately
              following the last sample-frame in the block of audio most
              recently processed by the context's rendering graph. If the
              context's rendering graph has not yet processed a block of audio,
              then <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>
              has a value of zero.
            </p>
            <p>
              In the time coordinate system of <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>,
              the value of zero corresponds to the first sample-frame in the
              first block processed by the graph. Elapsed time in this system
              corresponds to elapsed time in the audio stream generated by the
              <a><code>BaseAudioContext</code></a>, which may not be
              synchronized with other clocks in the system. (For an
              <a><code>OfflineAudioContext</code></a>, since the stream is not
              being actively played by any device, there is not even an
              approximation to real time.)
            </p>
            <p>
              All scheduled times in the Web Audio API are relative to the
              value of <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>.
            </p>
            <p>
              When the <a><code>BaseAudioContext</code></a> is in the <a href=
              "#idl-def-AudioContextState.running"><code>running</code></a>
              state, the value of this attribute is monotonically increasing
              and is updated by the rendering thread in uniform increments,
              corresponding to the audio block size of 128 samples. Thus, for a
              running context, <code>currentTime</code> increases steadily as
              the system processes audio blocks, and always represents the time
              of the start of the next audio block to be processed. It is also
              the earliest possible time when any change scheduled in the
              current state might take effect.
            </p>
            <p>
              <code>currentTime</code> MUST be read <a data-lt=
              "atomic">atomically</a> on the control thread before being
              returned.
            </p>
          </dd>
          <dt>
            readonly attribute AudioListener listener
          </dt>
          <dd>
            <p>
              An <a href="#AudioListener"><code>AudioListener</code></a> which
              is used for 3D <a href="#Spatialization">spatialization</a>.
            </p>
          </dd>
          <dt>
            readonly attribute AudioContextState state
          </dt>
          <dd>
            <p>
              Describes the current state of the <a>AudioContext</a>, on the
              <a>control thread</a>.
            </p>
          </dd>
          <dt>
            readonly attribute double baseLatency
          </dt>
          <dd>
            <p>
              This represents the number of seconds of processing latency
              incurred by the <a>AudioContext</a> passing the audio from the
              <a>AudioDestinationNode</a> to the audio subsystem. It does not
              include any additional latency that might be caused by any other
              processing between the output of the <a>AudioDestinationNode</a>
              and the audio hardware and specifically does not include any
              latency incurred the the audio graph itself.
            </p>
            <p>
              For example, if the audio context is running at 44.1 kHz and the
              <a>AudioDestinationNode</a> implements double buffering
              internally and can process and output audio every 128 sample
              frames, then the processing latency is \((2\cdot128)/44100 =
              5.805 \mathrm{ ms}\), approximately.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; suspend()
          </dt>
          <dd>
            <p>
              Suspends the progression of
              <a><code>BaseAudioContext</code></a>'s <a href=
              "#widl-BaseAudioContext-currentTime">currentTime</a>, allows any
              current context processing blocks that are already processed to
              be played to the destination, and then allows the system to
              release its claim on audio hardware. This is generally useful
              when the application knows it will not need the
              <a>BaseAudioContext</a> for some time, and wishes to temporarily
              <a>release system resource</a> associated with the
              <a>BaseAudioContext</a>. The promise resolves when the frame
              buffer is empty (has been handed off to the hardware), or
              immediately (with no other effect) if the context is already
              <code>suspended</code>. The promise is rejected if the context
              has been closed.
            </p>
            <p>
              <span class="synchronous">When suspend is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>closed</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>suspended</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>suspended</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to suspend the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to suspend an
              <a>AudioContext</a> means running these steps on the <a>rendering
              thread</a>:
            </p>
            <ol>
              <li>Attempt to <a>release system resources</a>.
              </li>
              <li>Queue a task on the <a>control thread</a>'s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>suspended</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>suspended</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>statechange</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
            <p>
              While a <a>BaseAudioContext</a> is suspended,
              <code>MediaStream</code>s will have their output ignored; that
              is, data will be lost by the real time nature of media streams.
              <code>HTMLMediaElement</code>s will similarly have their output
              ignored until the system is resumed. <a>AudioWorker</a>s and
              <a>ScriptProcessorNode</a>s will simply not fire their
              <code>onaudioprocess</code> events while suspended, but will
              resume when resumed. For the purpose of <a>AnalyserNode</a>
              window functions, the data is considered as a continuous stream -
              i.e. the <code>resume()</code>/<code>suspend()</code> does not
              cause silence to appear in the <a>AnalyserNode</a>'s stream of
              data. In particular, calling <a>AnalyserNode</a> functions
              repeatedly when a <a>BaseAudioContext</a> is suspended MUST
              return the same data.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; resume()
          </dt>
          <dd>
            <p>
              Resumes the progression of the
              <a><code>AudioContext</code></a>'s' <a href=
              "#widl-AudioContext-currentTime">currentTime</a> when it has been
              suspended.
            </p>
            <p>
              <span class="synchronous">When resume is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>closed</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>running</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>running</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to resume the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to resume an <a>AudioContext</a>
              means running these steps on the <a>rendering thread</a>:
            </p>
            <ol>
              <li>Attempt to <a href="#acquiring">acquire system resources</a>.
              </li>
              <li>In case of failure, queue a task on the <a>control thread</a>
              to execute the following, and abort these steps
                <ol>
                  <li>Reject <em>promise</em>
                  </li>
                </ol>
              </li>
              <li>Queue a task on the <a>control thread</a>'s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>running</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>running</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>statechange</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            Promise&lt;void&gt; close()
          </dt>
          <dd>
            <p>
              Closes the <a>BaseAudioContext</a>, <a>releasing the system
              resources</a> it's using. This will not automatically release all
              <a>AudioContext</a>-created objects, but will suspend the
              progression of the <a><code>AudioContext</code></a>'s <a href=
              "#widl-AudioContext-currentTime">currentTime</a>, and stop
              processing audio data.
            </p>
            <p>
              <span class="synchronous">When close is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If this <a>BaseAudioContext</a> is an
              <a>OfflineAudioContext</a>, reject <em>promise</em> with
              <code>NotSupportedError</code>, return it, and abort these steps.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>closed</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>closed</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>closed</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to close an
              <a>BaseAudioContext</a> means running these steps on the
              <a>rendering thread</a>:
            </p>
            <ol>
              <li>Attempt to <a>release system resources</a>.
              </li>
              <li>Queue a task on the <a>control thread</a>'s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>closed</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>closed</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>statechange</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
            <p class="note">
              When an <a>BaseAudioContext</a> has been closed, implementation
              can choose to aggressively release more resources than when
              suspending.
            </p>
          </dd>
          <dt>
            attribute EventHandler onstatechange
          </dt>
          <dd>
            <p>
              A property used to set the <code>EventHandler</code> for an event
              that is dispatched to <a><code>BaseAudioContext</code></a> when
              the state of the AudioContext has changed (i.e. when the
              corresponding promise would have resolved). An event of type
              <a><code>Event</code></a> will be dispatched to the event
              handler, which can query the AudioContext's state directly. A
              newly-created AudioContext will always begin in the
              <code>suspended</code> state, and a state change event will be
              fired whenever the state changes to a different state. This event
              is fired before the <a><code>oncomplete</code></a> event is
              fired.
            </p>
          </dd>
          <dt>
            AudioBuffer createBuffer()
          </dt>
          <dd>
            <p>
              Creates an AudioBuffer of the given size. The audio data in the
              buffer will be zero-initialized (silent). <span class=
              "synchronous">A NotSupportedError exception MUST be thrown if any
              of the arguments is negative, zero, or outside its nominal
              range.</span>
            </p>
            <dl class="parameters">
              <dt>
                unsigned long numberOfChannels
              </dt>
              <dd>
                Determines how many channels the buffer will have. An
                implementation must support at least 32 channels.
              </dd>
              <dt>
                unsigned long length
              </dt>
              <dd>
                Determines the size of the buffer in sample-frames.
              </dd>
              <dt>
                float sampleRate
              </dt>
              <dd>
                Describes the sample-rate of the linear PCM audio data in the
                buffer in sample-frames per second. An implementation must
                support sample rates in at least the range 8000 to 96000.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;AudioBuffer&gt; decodeAudioData()
          </dt>
          <dd>
            <p>
              Asynchronously decodes the audio file data contained in the
              <code>ArrayBuffer</code>. The <code>ArrayBuffer</code> can, for
              example, be loaded from an <code>XMLHttpRequest</code>'s
              <code>response</code> attribute after setting the
              <code>responseType</code> to <code>"arraybuffer"</code>. Audio
              file data can be in any of the formats supported by the
              <code>audio</code> element. The buffer passed to <a href=
              "#widl-AudioContext-decodeAudioData-Promise-AudioBuffer--ArrayBuffer-audioData-DecodeSuccessCallback-successCallback-DecodeErrorCallback-errorCallback">
              decodeAudioData</a> has its content-type determined by sniffing,
              as described in [[mimesniff]].
            </p>
            <dl class="parameters">
              <dt>
                ArrayBuffer audioData
              </dt>
              <dd>
                An ArrayBuffer containing compressed audio data
              </dd>
              <dt>
                optional DecodeSuccessCallback successCallback
              </dt>
              <dd>
                A callback function which will be invoked when the decoding is
                finished. The single argument to this callback is an
                AudioBuffer representing the decoded PCM audio data.
              </dd>
              <dt>
                optional DecodeErrorCallback errorCallback
              </dt>
              <dd>
                A callback function which will be invoked if there is an error
                decoding the audio file.
              </dd>
            </dl>
            <p>
              Although the primary method of interfacing with this function is
              via its promise return value, the callback parameters are
              provided for legacy reasons. The system shall ensure that the
              <a>AudioContext</a> is not garbage collected before the promise
              is resolved or rejected and any callback function is called and
              completes.
            </p>
            <p>
              <span class="synchronous">When <code>decodeAudioData</code> is
              called, the following steps must be performed on the control
              thread:</span>
            </p>
            <ol>
              <li>Let <var>promise</var> be a new promise.
              </li>
              <li>If the operation <a href=
              "https://tc39.github.io/ecma262/#sec-isdetachedbuffer"><code>IsDetachedBuffer</code></a>
              (described in [[!ECMASCRIPT]]) on <a>audioData</a> is
              <code>false</code>, execute the following steps:
                <ol>
                  <li>
                    <a href=
                    "https://tc39.github.io/ecma262/#sec-detacharraybuffer">Detach</a>
                    the <a>audioData</a> <code>ArrayBuffer</code>. This
                    operation is described in [[!ECMASCRIPT]].
                  </li>
                  <li>Queue a decoding operation to be performed on another
                  thread.
                  </li>
                </ol>
              </li>
              <li>Else, execute the following steps:
                <ol>
                  <li>Let <var>error</var> be a <code>DOMException</code> whose
                  name is <code>"TypeError"</code>.
                  </li>
                  <li>Reject <var>promise</var> with <var>error</var>.
                  </li>
                  <li>Queue a task to invoke <a>errorCallback</a> with
                  <var>error</var>.
                  </li>
                </ol>
              </li>
              <li>Return <var>promise</var>.
              </li>
            </ol>
            <p>
              When queuing a decoding operation to be performed on another
              thread, the following steps MUST happen on a thread that is not
              the <a>control thread</a> nor the <a>rendering thread</a>, called
              the <em>decoding thread</em>.
            </p>
            <div class="note">
              Multiple <em>decoding threads</em> can run in parallel to service
              multiple calls to <code>decodeAudioData</code>.
            </div>
            <ol>
              <li>Attempt to decode the encoded <a>audioData</a> into linear
              PCM.
              </li>
              <li>If a decoding error is encountered due to the audio format
              not being recognized or supported, or because of
              corrupted/unexpected/inconsistent data, then queue a task to
              execute the following step, on the <a>control thread</a>'s event
              loop:
                <ol>
                  <li>Let <var>error</var> be a <code>DOMException</code> whose
                  name is <code>"EncodingError"</code>.
                  </li>
                  <li>Reject <var>promise</var> with <var>error</var>.
                  </li>
                  <li>If <dfn>errorCallback</dfn> is not missing, invoke
                  <a>errorCallback</a> with <var>error</var>.
                  </li>
                </ol>
              </li>
              <li>Otherwise:
                <ol>
                  <li>Take the result, representing the decoded linear PCM
                  audio data, and resample it to the sample-rate of the
                  <a><code>AudioContext</code></a> if it is different from the
                  sample-rate of <a>audioData</a>.
                  </li>
                  <li>Queue a task on the <a>control thread</a>'s event loop to
                  execute the following steps:
                    <ol>
                      <li>Let <var>buffer</var> be an <code>AudioBuffer</code>
                      containing the final result (after possibly sample-rate
                      conversion).
                      </li>
                      <li>Resolve <var>promise</var> with <var>buffer</var>.
                      </li>
                      <li>If <dfn>successCallback</dfn> is not missing, invoke
                      <a>successCallback</a> with <var>buffer</var>.
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            AudioBufferSourceNode createBufferSource()
          </dt>
          <dd>
            <p>
              Creates an <a><code>AudioBufferSourceNode</code></a>.
              <a>AudioBufferSourceNode</a> are created with an internal flag
              <code>buffer set</code>, initially set to false.
            </p>
          </dd>
          <dt>
            Promise&lt;AudioWorker&gt; createAudioWorker()
          </dt>
          <dd>
            <p>
              Creates an <a><code>AudioWorker</code></a> object and loads the
              associated script into an
              <a><code>AudioWorkerGlobalScope</code></a>, then resolves the
              returned Promise.
            </p>
            <dl class="parameters">
              <dt>
                DOMString scriptURL
              </dt>
              <dd>
                This parameter represents the URL of the script to be loaded as
                an <a>AudioWorker</a> node factory. See <a>AudioWorker</a>
                section for more detail.
              </dd>
            </dl>
          </dd>
          <dt>
            ScriptProcessorNode createScriptProcessor()
          </dt>
          <dd>
            <p>
              This method is DEPRECATED, as it is intended to be replaced by
              createAudioWorker. Creates a
              <a><code>ScriptProcessorNode</code></a> for direct audio
              processing using JavaScript. <span class="synchronous">An
              IndexSizeError exception MUST be thrown if
              <a><code>bufferSize</code></a> or
              <a><code>numberOfInputChannels</code></a> or
              <a><code>numberOfOutputChannels</code></a> are outside the valid
              range.</span>
            </p>
            <dl class="parameters">
              <dt>
                optional unsigned long bufferSize = 0
              </dt>
              <dd>
                The <a><code>bufferSize</code></a> parameter determines the
                buffer size in units of sample-frames. If it's not passed in,
                or if the value is 0, then the implementation will choose the
                best buffer size for the given environment, which will be
                constant power of 2 throughout the lifetime of the node.
                Otherwise if the author explicitly specifies the bufferSize, it
                must be one of the following values: 256, 512, 1024, 2048,
                4096, 8192, 16384. This value controls how frequently the
                <code>audioprocess</code> event is dispatched and how many
                sample-frames need to be processed each call. Lower values for
                <a><code>bufferSize</code></a> will result in a lower (better)
                <a href="#latency">latency</a>. Higher values will be necessary
                to avoid audio breakup and <a href=
                "#audio-glitching">glitches</a>. It is recommended for authors
                to not specify this buffer size and allow the implementation to
                pick a good buffer size to balance between <a href=
                "#latency">latency</a> and audio quality. If the value of this
                parameter is not one of the allowed power-of-2 values listed
                above, an IndexSizeError MUST be thrown.
              </dd>
              <dt>
                optional unsigned long numberOfInputChannels = 2
              </dt>
              <dd>
                This parameter determines the number of channels for this
                node's input. Values of up to 32 must be supported.
              </dd>
              <dt>
                optional unsigned long numberOfOutputChannels = 2
              </dt>
              <dd>
                This parameter determines the number of channels for this
                node's output. Values of up to 32 must be supported.
              </dd>
            </dl>
            <p>
              It is invalid for both <a><code>numberOfInputChannels</code></a>
              and <a><code>numberOfOutputChannels</code></a> to be zero. In
              this case an IndexSizeError MUST be thrown.
            </p>
          </dd>
          <dt>
            AnalyserNode createAnalyser()
          </dt>
          <dd>
            <p>
              Create an <a><code>AnalyserNode</code></a>.
            </p>
          </dd>
          <dt>
            GainNode createGain()
          </dt>
          <dd>
            <p>
              Create an <a><code>GainNode</code></a>.
            </p>
          </dd>
          <dt>
            DelayNode createDelay()
          </dt>
          <dd>
            <p>
              Creates a <a><code>DelayNode</code></a> representing a variable
              delay line. The initial default delay time will be 0 seconds.
            </p>
            <dl class="parameters">
              <dt>
                optional double maxDelayTime = 1.0
              </dt>
              <dd>
                The <dfn>maxDelayTime</dfn> parameter is optional and specifies
                the maximum delay time in seconds allowed for the delay line.
                <span class="synchronous">If specified, this value MUST be
                greater than zero and less than three minutes or a
                NotSupportedError exception MUST be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            BiquadFilterNode createBiquadFilter()
          </dt>
          <dd>
            <p>
              Creates a <a><code>BiquadFilterNode</code></a> representing a
              second order filter which can be configured as one of several
              common filter types.
            </p>
          </dd>
          <dt>
            IIRFilterNode createIIRFilter(sequence&lt;double&gt; b,
            sequence&lt;double&gt; a)
          </dt>
          <dd>
            <p>
              Creates an <a><code>IIRFilterNode</code></a> representing a
              general IIR Filter.
            </p>
            <dl class="parameters">
              <dt>
                sequence&lt;double&gt; feedforward
              </dt>
              <dd>
                An array of the feedforward (numerator) coefficients for the
                transfer function of the IIR filter. The maximum length of this
                array is 20. If all of the values are zero, an
                InvalidStateError MUST be thrown. A NotSupportedError MUST be
                thrown if the array length is 0 or greater than 20.
              </dd>
              <dt>
                sequence&lt;double&gt; feedback
              </dt>
              <dd>
                An array of the feedback (denominator) coefficients for the
                tranfer function of the IIR filter. The maximum length of this
                array is 20. If the first element of the array is 0, an
                InvalidStateError MUST be thrown. A NotSupportedError MUST be
                thrown if the array length is 0 or greater than 20.
              </dd>
            </dl>
          </dd>
          <dt>
            WaveShaperNode createWaveShaper()
          </dt>
          <dd>
            <p>
              Creates a <a><code>WaveShaperNode</code></a> representing a
              non-linear distortion.
            </p>
          </dd>
          <dt>
            PannerNode createPanner()
          </dt>
          <dd>
            <p>
              Creates a <a><code>PannerNode</code></a>.
            </p>
          </dd>
          <dt>
            StereoPannerNode createStereoPanner()
          </dt>
          <dd>
            <p>
              Creates a <a><code>StereoPannerNode</code></a>.
            </p>
          </dd>
          <dt>
            ConvolverNode createConvolver()
          </dt>
          <dd>
            <p>
              Creates a <a><code>ConvolverNode</code></a>.
            </p>
          </dd>
          <dt>
            ChannelSplitterNode createChannelSplitter()
          </dt>
          <dd>
            <p>
              Creates an <a><code>ChannelSplitterNode</code></a> representing a
              channel splitter. <span class="synchronous">An IndexSizeError
              exception MUST be thrown for invalid parameter values.</span>
            </p>
            <dl class="parameters">
              <dt>
                optional unsigned long numberOfOutputs = 6
              </dt>
              <dd>
                The number of outputs. Values of up to 32 must be supported. If
                not specified, then 6 will be used.
              </dd>
            </dl>
          </dd>
          <dt>
            ChannelMergerNode createChannelMerger()
          </dt>
          <dd>
            <p>
              Creates a <a><code>ChannelMergerNode</code></a> representing a
              channel merger. <span class="synchronous">An IndexSizeError
              exception MUST be thrown for invalid parameter values.</span>
            </p>
            <dl class="parameters">
              <dt>
                optional unsigned long numberOfInputs = 6
              </dt>
              <dd>
                The <dfn>numberOfInputs</dfn> parameter determines the number
                of inputs. Values of up to 32 must be supported. If not
                specified, then 6 will be used.
              </dd>
            </dl>
          </dd>
          <dt>
            DynamicsCompressorNode createDynamicsCompressor()
          </dt>
          <dd>
            <p>
              Creates a <a><code>DynamicsCompressorNode</code></a>
            </p>
          </dd>
          <dt>
            OscillatorNode createOscillator()
          </dt>
          <dd>
            <p>
              Creates an <a><code>OscillatorNode</code></a>
            </p>
          </dd>
          <dt>
            PeriodicWave createPeriodicWave()
          </dt>
          <dd>
            <p>
              Creates a <a><code>PeriodicWave</code></a> representing a
              waveform containing arbitrary harmonic content. <span class=
              "synchronous">The <code>real</code> and <code>imag</code>
              parameters must be of type <code>Float32Array</code> (described
              in [[!TYPED-ARRAYS]]) of equal lengths greater than zero or an
              IndexSizeError exception MUST be thrown.</span> All
              implementations must support arrays up to at least 8192. These
              parameters specify the Fourier coefficients of a <a href=
              "https://en.wikipedia.org/wiki/Fourier_series">Fourier series</a>
              representing the partials of a periodic waveform. The created
              <a><code>PeriodicWave</code></a> will be used with an
              <a><code>OscillatorNode</code></a> and, by default, will
              represent a <em>normalized</em> time-domain waveform having
              maximum absolute peak value of 1. Another way of saying this is
              that the generated waveform of an
              <a><code>OscillatorNode</code></a> will have maximum peak value
              at 0dBFS. Conveniently, this corresponds to the full-range of the
              signal values used by the Web Audio API. Because the PeriodicWave
              is normalized by default on creation, the <code>real</code> and
              <code>imag</code> parameters represent <em>relative</em> values.
              If normalization is disabled via the
              <code>disableNormalization</code> parameter, this normalization
              is disabled, and the time-domain waveform has the amplitudes as
              given by the Fourier coefficients.
            </p>
            <p>
              As <a>PeriodicWave</a> objects maintain their own copies of these
              arrays, any modification of the arrays uses as the
              <code>real</code> and <code>imag</code> parameters after the call
              to <a href=
              "#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
              createPeriodicWave()</a> will have no effect on the
              <a>PeriodicWave</a> object.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array real
              </dt>
              <dd>
                The <dfn id="dfn-real">real</dfn> parameter represents an array
                of <code>cosine</code> terms (traditionally the A terms). In
                audio terminology, the first element (index 0) is the DC-offset
                of the periodic waveform. The second element (index 1)
                represents the fundamental frequency. The third element
                represents the first overtone, and so on. The first element is
                ignored and implementations must set it to zero internally.
              </dd>
              <dt>
                Float32Array imag
              </dt>
              <dd>
                The <dfn id="dfn-imag">imag</dfn> parameter represents an array
                of <code>sine</code> terms (traditionally the B terms). The
                first element (index 0) should be set to zero (and will be
                ignored) since this term does not exist in the Fourier series.
                The second element (index 1) represents the fundamental
                frequency. The third element represents the first overtone, and
                so on.
              </dd>
              <dt>
                optional PeriodicWaveConstraints constraints
              </dt>
              <dd>
                If not given, the waveform is normalized. Otherwise, the
                waveform is normalized according the value given by
                <code>constraints</code>.
              </dd>
            </dl>
          </dd>
        </dl>
        <dl title="callback DecodeSuccessCallback = void" class="idl">
          <dt>
            AudioBuffer decodedData
          </dt>
          <dd>
            The AudioBuffer containing the decoded audio data.
          </dd>
        </dl>
        <dl title="callback DecodeErrorCallback = void" class="idl">
          <dt>
            DOMException error
          </dt>
          <dd>
            The error that occurred while decoding.
          </dd>
        </dl>
        <section>
          <h3 id="lifetime-AudioContext" class="informative">
            Lifetime
          </h3>
          <p>
            Once created, an <code>AudioContext</code> will continue to play
            sound until it has no more sound to play, or the page goes away.
          </p>
        </section>
        <section class="informative">
          <h3>
            Lack of introspection or serialization primitives
          </h3>
          <p>
            The Web Audio API takes a <em>fire-and-forget</em> approach to
            audio source scheduling. That is, <a>source nodes</a> are created
            for each note during the lifetime of the <a>AudioContext</a>, and
            never explicitely removed from the graph. This is incompatible with
            a serialization API, since there is no stable set of nodes that
            could be serialized.
          </p>
          <p>
            Moreover, having an introspection API would allow content script to
            be able to observe garbage collections.
          </p>
        </section>
        <section>
          <h3>
            System resources associated with an AudioContext
          </h3>
          <p>
            <a>AudioContext</a>s should be considered expensive objects.
            Creating an <a>AudioContext</a> often involves creating a
            high-priority thread, and using a low-latency system audio stream,
            both having an impact on energy consumption. Creating more than one
            <a>AudioContext</a> in a document is most of the time unnecessary.
          </p>
          <p>
            Additionally, a user-agent can have an implementation-defined
            maximum number of <a>AudioContext</a>, after which any attempt to
            create a new <a>AudioContext</a> will fail, <span class=
            "synchronous">throwing <code>NotSupportedError</code></span>.
          </p>
          <p>
            <a href="#widl-AudioContext-suspend-Promise-void">suspend</a> and
            <a href="#widl-AudioContext-close-Promise-void">close</a> allow
            authors to <dfn id="releasing">release system resources</dfn>.
            <a href="#releasing">Releasing system resources</a> means releasing
            the system resources such as threads, processes, audio streams, but
            conserving the state of the <a>AudioContext</a> such that it can
            continue to operate later after resuming if needed.
          </p>
          <p>
            Constructing or resuming an <a>AudioContext</a> involves <dfn id=
            "acquiring">acquiring system resources</dfn>. This means opening a
            system audio stream. This operation returns when the audio stream
            is ready.
          </p>
          <p class="note">
            For example, this can involve waiting for the audio callbacks to
            fire regularly, or to wait for the hardware to be ready for
            processing.
          </p>
        </section>
      </section>
      <section>
        <h2 id="AudioContext">
          The AudioContext Interface
        </h2>
        <p>
          This interface represents an audio graph whose
          <a><code>AudioDestinationNode</code></a> is routed to a real-time
          output device that produces a signal directed at the user. In most
          use cases, only a single <a><code>AudioContext</code></a> is used per
          document.
        </p>
        <dl title=
        '[Constructor(optional AudioContextOptions contextOptions)] interface AudioContext : BaseAudioContext'
        class='idl'>
          <dt>
            Constructor
          </dt>
          <dd>
            <p>
              <span class="synchronous">When creating an <a>AudioContext</a>,
              execute these steps:</span>
            </p>
            <ol>
              <li>Set a <code>control thread state</code> to
              <code>suspended</code> on the <a>AudioContext</a>.
              </li>
              <li>Send a <a>control message</a> to start processing.
              </li>
            </ol>
            <p>
              Sending a <a>control message</a> to start processing means
              executing the following steps:
            </p>
            <ol>
              <li>Attempt to <a href="#acquiring">Acquire system resources</a>.
              </li>
              <li>In case of failure, abort these steps.
              </li>
              <li>Queue a task on the <a>control thread</a> event loop, to
              execute these steps:
                <ol>
                  <li>Set the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> to <code>running</code>.
                  </li>
                  <li>Queue a task to fire a simple event named
                  <code>statechange</code> at the <a>AudioContext</a>.
                  </li>
                </ol>
              </li>
            </ol>
            <p class="note">
              It is unfortunately not possible to programatically notify
              authors that the creation of the <a>AudioContext</a> failed.
              User-Agents are encouraged to log an informative message if they
              have access to a logging mechanism, such as a developer tools
              console.
            </p>
          </dd>
          <dt>
            readonly attribute double outputLatency
          </dt>
          <dd>
            <p>
              The estimation in seconds of audio output latency, i.e., the
              interval between the time the UA requests the host system to play
              a buffer and the time at which the first sample in the buffer is
              actually processed by the audio output device. For devices such
              as speakers or headphones that produce an acoustic signal, this
              latter time refers to the time when a sample's sound is produced.
            </p>
            <p>
              The <a><code>outputLatency</code></a> attribute value depends on
              the platform and the connected hardware audio output device. The
              <a><code>outputLatency</code></a> attribute value does not change
              for the context's lifetime as long as the connected audio output
              device remains the same. If the audio output device is changed
              the <a><code>outputLatency</code></a> attribute value will be
              updated accordingly.
            </p>
          </dd>
          <dt>
            AudioTimestamp getOutputTimestamp()
          </dt>
          <dd>
            <p>
              Returns a new <a><code>AudioTimestamp</code></a> instance
              containing two correlated context's audio stream position values:
              the <a href=
              "#widl-AudioTimestamp-contextTime"><code>contextTime</code></a>
              member contains the time of the sample frame which is currently
              being rendered by the audio output device (i.e., output audio
              stream position), in the same units and origin as context's
              <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>;
              the <a href=
              "#widl-AudioTimestamp-performanceTime"><code>performanceTime</code></a>
              member contains the time estimating the moment when the sample
              frame corresponding to the stored <code>contextTime</code> value
              was rendered by the audio output device, in the same units and
              origin as <code>performance.now()</code> (described in
              [[!hr-time-2]]).
            </p>
            <p>
              If the context's rendering graph has not yet processed a block of
              audio, then <a><code>getOutputTimestamp</code></a> call returns
              an <code>AudioTimestamp</code> instance with both members
              containing zero.
            </p>
            <p>
              After the context's rendering graph has started processing of
              blocks of audio, its <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>
              attribute value always exceeds the <a href=
              "#widl-AudioTimestamp-contextTime"><code>contextTime</code></a>
              value obtained from <a href=
              "#widl-AudioContext-getOutputTimestamp-AudioTimestamp"><code>getOutputTimestamp</code></a>
              method call.
            </p>
            <p>
              The value returned from <a><code>getOutputTimestamp</code></a>
              method can be used to get performance time estimation for the
              slightly later context's time value:
            </p>
            <pre class="example">
            function outputPerformanceTime(contextTime) {
                var timestamp = context.getOutputTimestamp();
                var elapsedTime = contextTime - timestamp.contextTime;
                return timestamp.performanceTime + elapsedTime * 1000;
            }
            </pre>
            <p>
              In the above example the accuracy of the estimation depends on
              how close the argument value is to the current output audio
              stream position: the closer the given <code>contextTime</code> is
              to <code>timestamp.contextTime</code>, the better the accuracy of
              the obtained estimation.
            </p>
            <p class="note">
              The difference between the values of context's <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>
              and the <a href=
              "#widl-AudioTimestamp-contextTime"><code>contextTime</code></a>
              obtained from <a href=
              "#widl-AudioContext-getOutputTimestamp-AudioTimestamp"><code>getOutputTimestamp</code></a>
              method call cannot be considered as a reliable output latency
              estimation because <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>
              may be incremented at non-uniform time intervals, so <a href=
              "#widl-AudioContext-outputLatency"><code>outputLatency</code></a>
              attribute should be used instead.
            </p>
          </dd>
          <dt>
            MediaElementAudioSourceNode createMediaElementSource()
          </dt>
          <dd>
            <p>
              Creates a <a href=
              "#MediaElementAudioSourceNode">MediaElementAudioSourceNode</a>
              given an HTMLMediaElement. As a consequence of calling this
              method, audio playback from the HTMLMediaElement will be
              re-routed into the processing graph of the
              <a><code>AudioContext</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                HTMLMediaElement mediaElement
              </dt>
              <dd>
                The media element that will be re-routed.
              </dd>
            </dl>
          </dd>
          <dt>
            MediaStreamAudioSourceNode createMediaStreamSource()
          </dt>
          <dd>
            <p>
              Creates a <a><code>MediaStreamAudioSourceNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                MediaStream mediaStream
              </dt>
              <dd>
                The media stream that will act as source.
              </dd>
            </dl>
          </dd>
          <dt>
            MediaStreamAudioDestinationNode createMediaStreamDestination()
          </dt>
          <dd>
            <p>
              Creates a <a><code>MediaStreamAudioDestinationNode</code></a>
            </p>
          </dd>
        </dl>
        <section>
          <h3>
            AudioContextOptions
          </h3>
          <p>
            The <a><code>AudioContextOptions</code></a> dictionary is used to
            specify a requested latency for an <a>AudioContext</a>.
          </p>
          <dl title="dictionary AudioContextOptions" class="idl">
            <dt>
              (AudioContextLatencyCategory or double) latencyHint =
              "interactive"
            </dt>
            <dd>
              <p>
                Identify the type of playback, which affects tradeoffs between
                audio output latency and power consumption.
              </p>
              <p>
                The preferred value of the <code>latencyHint</code> is a value
                from <a>AudioContextLatencyCategory</a>. However, a double can
                also be specified for the number of seconds of latency for
                finer control to balance latency and power consumption. It is
                at the browser's discretion to interpret the number
                appropriately. The actual latency used is given by
                AudioContext's <a href=
                "#widl-BaseAudioContext-baseLatency">baseLatency</a> attribute.
              </p>
            </dd>
          </dl>
        </section>
        <section>
          <h3>
            AudioTimestamp
          </h3>
          <dl title="dictionary AudioTimestamp" class="idl">
            <dt>
              double contextTime
            </dt>
            <dd>
              Represents a point in the time coordinate system of
              BaseAudioContext's <a href=
              "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>.
            </dd>
            <dt>
              DOMHighResTimeStamp performanceTime
            </dt>
            <dd>
              Represents a point in the time coordinate system of a
              <code>Performance</code> interface implementation (described in
              [[!hr-time-2]]).
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2 id="OfflineAudioContext">
          The OfflineAudioContext Interface
        </h2>
        <p>
          <a><code>OfflineAudioContext</code></a> is a particular type of
          <a><code>AudioContext</code></a> for rendering/mixing-down
          (potentially) faster than real-time. It does not render to the audio
          hardware, but instead renders as quickly as possible, fulfilling the
          returned promise with the rendered result as an
          <code>AudioBuffer</code>.
        </p>
        <p>
          The OfflineAudioContext is constructed with the same arguments as
          AudioContext.createBuffer. <span class="synchronous">A
          NotSupportedException exception MUST be thrown if any of the
          arguments is negative, zero, or outside its nominal range.</span>
        </p>
        <dl class="parameters">
          <dt>
            unsigned long numberOfChannels
          </dt>
          <dd>
            Determines how many channels the buffer will have. See <a href=
            "#widl-BaseAudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate">
            createBuffer</a> for the supported number of channels.
          </dd>
          <dt>
            unsigned long length
          </dt>
          <dd>
            Determines the size of the buffer in sample-frames.
          </dd>
          <dt>
            float sampleRate
          </dt>
          <dd>
            Describes the sample-rate of the linear PCM audio data in the
            buffer in sample-frames per second. See <a href=
            "#widl-BaseAudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate">
            createBuffer</a> for valid sample rates.
          </dd>
        </dl>
        <dl title=
        '[Constructor(unsigned long numberOfChannels, unsigned long length, float sampleRate)] interface OfflineAudioContext : BaseAudioContext'
        class='idl'>
          <dt>
            Promise&lt;AudioBuffer&gt; startRendering()
          </dt>
          <dd>
            <p>
              Given the current connections and scheduled changes, starts
              rendering audio. The system shall ensure that the
              <code>OfflineAudioContext</code> is not garbage collected until
              either the promise is resolved and any callback function is
              called and completes, or until the <code>suspend</code> function
              is called.
            </p>
            <p>
              Although the primary method of getting the rendered audio data is
              via its promise return value, the instance will also fire an
              event named <code>complete</code> for legacy reasons.
            </p>
            <p>
              <span class="synchronous">When <code>startRendering</code> is
              called, the following steps must be performed on the <a>control
              thread</a>:</span>
            </p>
            <ol>
              <li>Set a flag called <var>renderingStarted</var> on the
              <a>OfflineAudioContext</a> to <em>true</em>.
              </li>
              <li>If the <em>renderingStarted</em> flag on the
              <a>OfflineAudioContext</a> is <em>true</em>, return a rejected
              promise with <code>InvalidStateError</code>, and abort these
              steps.
              </li>
              <li>Let <var>promise</var> be a new promise.
              </li>
              <li>Start to render the audio graph on another thread.
              </li>
              <li>Return <var>promise</var>
              </li>
            </ol>
            <p>
              When rendering an audio graph on another thread, the following
              steps MUST happen on a <a>rendering thread</a> that is created
              for the occasion.
            </p>
            <ol>
              <li>Let <var>buffer</var> be a new <code>AudioBuffer</code>, with
              a number of channels, length and sample rate equal respectively
              to the <code>numberOfChannels</code>, <code>length</code> and
              <code>sampleRate</code> parameters used when this instance's
              constructor was called.
              </li>
              <li>Given the current connections and scheduled changes, start
              rendering <code>length</code> sample-frames of audio into
              <var>buffer</var>.
              </li>
              <li>For every <a>render quantum</a>, check and suspend the
              rendering if necessary.
              </li>
              <li>If a suspended context is resumed, continue to render the
              buffer.
              </li>
              <li>Once the rendering is complete, queue a task on the
              <a>control thread</a>'s event loop to perform the following
              steps:
                <ol>
                  <li>Resolve <var>promise</var> with <var>buffer</var>.
                  </li>
                  <li>If a suspended context is resumed, continue to render the
                  buffer.
                  </li>
                  <li>Once the rendering is complete,
                    <ol>
                      <li>Resolve <var>promise</var> with <var>buffer</var>.
                      </li>
                      <li>Queue a task to fire an event named
                      <code>complete</code> at this instance, using an instance
                      of <a><code>OfflineAudioCompletionEvent</code></a> whose
                      <code>renderedBuffer</code> property is set to
                      <var>buffer</var>.
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            Promise&lt;void&gt; resume()
          </dt>
          <dd>
            <p>
              Resumes the progression of time in an audio context that has been
              suspended. The promise resolves immediately because the
              <a><code>OfflineAudioContext</code></a> does not require the
              audio hardware. If the context is not currently suspended or the
              rendering has not started, the promise is rejected with
              <code>InvalidStateError</code>.
            </p>
            <p>
              In contrast to a live <a><code>AudioContext</code></a>, the value
              of <a href="#widl-BaseAudioContext-currentTime">currentTime</a>
              always reflects the start time of the next block to be rendered
              by the audio graph, since the context's audio stream does not
              advance in time during suspension.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; suspend()
          </dt>
          <dd>
            <p>
              Schedules a suspension of the time progression in the audio
              context at the specified time and returns a promise. This is
              generally useful when manipulating the audio graph synchronously
              on <a><code>OfflineAudioContext</code></a>.
            </p>
            <p>
              Note that the maximum precision of suspension is the size of the
              <a>render quantum</a> and the specified suspension time will be
              rounded down to the nearest <a>render quantum</a> boundary. For
              this reason, it is not allowed to schedule multiple suspends at
              the same quantized frame. Also scheduling should be done while
              the context is not running to ensure the precise suspension.
            </p>
            <dl class="parameters">
              <dt>
                double suspendTime
              </dt>
              <dd>
                Schedules a suspension of the rendering at the specified time,
                which is quantized and rounded down to the <a>render
                quantum</a> size. If the quantized frame number
                <ol>
                  <li>is negative or
                  </li>
                  <li>is less than or equal to the current time or
                  </li>
                  <li>is greater than or equal to the total render duration or
                  </li>
                  <li>is scheduled by another suspend for the same time,
                  </li>
                </ol>then the promise is rejected with
                <code>InvalidStateError</code>.
              </dd>
            </dl>
          </dd>
          <dt>
            readonly attribute unsigned long length
          </dt>
          <dd>
            <p>
              The size of the buffer in sample-frames. This is the same as the
              value of the <code>length</code> parameter for the constructor.
            </p>
          </dd>
          <dt>
            attribute EventHandler oncomplete
          </dt>
          <dd>
            <p>
              An EventHandler of type <a href=
              "#OfflineAudioCompletionEvent">OfflineAudioCompletionEvent</a>.
              It is the last event fired on an <a>OfflineAudioContext</a>.
            </p>
          </dd>
        </dl>
        <dl title="[Constructor] interface AudioContext : BaseAudioContext"
        class="idl"></dl>
        <section>
          <h2 id="OfflineAudioCompletionEvent">
            The OfflineAudioCompletionEvent Interface
          </h2>
          <p>
            This is an <code>Event</code> object which is dispatched to
            <a><code>OfflineAudioContext</code></a> for legacy reasons.
          </p>
          <dl title="interface OfflineAudioCompletionEvent : Event" class=
          "idl">
            <dt>
              readonly attribute AudioBuffer renderedBuffer
            </dt>
            <dd>
              <p>
                An <code>AudioBuffer</code> containing the rendered audio data.
              </p>
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The <dfn>AudioNode</dfn> Interface
        </h2>
        <p>
          <a>AudioNode</a>s are the building blocks of an <a href=
          "#AudioContext"><code>AudioContext</code></a>. This interface
          represents audio sources, the audio destination, and intermediate
          processing modules. These modules can be connected together to form
          <a href="#ModularRouting">processing graphs</a> for rendering audio
          to the audio hardware. Each node can have <a>inputs</a> and/or
          <a>outputs</a>. A <a>source node</a> has no inputs and a single
          output. Most processing nodes such as filters will have one input and
          one output. Each type of <a><code>AudioNode</code></a> differs in the
          details of how it processes or synthesizes audio. But, in general, an
          <a><code>AudioNode</code></a> will process its inputs (if it has
          any), and generate audio for its outputs (if it has any).
        </p>
        <p>
          Each output has one or more channels. The exact number of channels
          depends on the details of the specific <a><code>AudioNode</code></a>.
        </p>
        <p>
          An output may connect to one or more <a><code>AudioNode</code></a>
          inputs, thus <em>fan-out</em> is supported. An input initially has no
          connections, but may be connected from one or more <a>AudioNode</a>
          outputs, thus <em>fan-in</em> is supported. When the
          <code>connect()</code> method is called to connect an output of an
          <a>AudioNode</a> to an input of an <a>AudioNode</a>, we call that a
          <dfn>connection</dfn> to the input.
        </p>
        <p>
          Each <a>AudioNode</a> <dfn>input</dfn> has a specific number of
          channels at any given time. This number can change depending on the
          <a>connection</a>(s) made to the input. If the input has no
          connections then it has one channel which is silent.
        </p>
        <p>
          For each <a>input</a>, an <a><code>AudioNode</code></a> performs a
          mixing (usually an up-mixing) of all connections to that input.
          Please see <a href="#mixer-gain-structure"></a> for more informative
          details, and the <a href="#channel-up-mixing-and-down-mixing"></a>
          section for normative requirements.
        </p>
        <p>
          The processing of inputs and the internal operations of an
          <a>AudioNode</a> take place continuously with respect to
          <a>AudioContext</a> time, regardless of whether the node has
          connected outputs, and regardless of whether these outputs ultimately
          reach an <a>AudioContext</a>'s <a>AudioDestinationNode</a>.
        </p>
        <p>
          For performance reasons, practical implementations will need to use
          block processing, with each <a><code>AudioNode</code></a> processing
          a fixed number of sample-frames of size <em>block-size</em>. In order
          to get uniform behavior across implementations, we will define this
          value explicitly. <em>block-size</em> is defined to be 128
          sample-frames which corresponds to roughly 3ms at a sample-rate of
          44.1KHz.
        </p>
        <p>
          <a>AudioNode</a>s are <em>EventTarget</em>s, as described in
          <cite><a href="https://dom.spec.whatwg.org/">DOM</a></cite> [[!DOM]].
          This means that it is possible to dispatch events to
          <a><code>AudioNode</code></a>s the same way that other EventTargets
          accept events.
        </p>
        <dl title="enum ChannelCountMode" class="idl">
          <dt>
            max
          </dt>
          <dd>
            <a><code>computedNumberOfChannels</code></a> is computed as the
            maximum of the number of channels of all connections. In this mode
            channelCount is ignored
          </dd>
          <dt>
            clamped-max
          </dt>
          <dd>
            Same as â€œmaxâ€ up to a limit of the channelCount
          </dd>
          <dt>
            explicit
          </dt>
          <dd>
            <a><code>computedNumberOfChannels</code></a> is the exact value as
            specified in channelCount
          </dd>
        </dl>
        <dl title="enum ChannelInterpretation" class="idl">
          <dt>
            speakers
          </dt>
          <dd>
            use <a href="#ChannelLayouts">up-down-mix equations for
            mono/stereo/quad/5.1</a>. In cases where the number of channels do
            not match any of these basic speaker layouts, revert to "discrete".
          </dd>
          <dt>
            discrete
          </dt>
          <dd>
            Up-mix by filling channels until they run out then zero out
            remaining channels. down-mix by filling as many channels as
            possible, then dropping remaining channels.
          </dd>
        </dl>
        <dl title="interface AudioNode : EventTarget" class="idl">
          <dt>
            AudioNode connect()
          </dt>
          <dd>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to connect to. If the
                <code>destination</code> parameter is an
                <a><code>AudioNode</code></a> that has been created using
                another <a><code>AudioContext</code></a>, an InvalidAccessError
                MUST be thrown. That is, <a><code>AudioNodes</code></a> cannot
                be shared between <a><code>AudioContext</code></a>s.
              </dd>
              <dt>
                optional unsigned long output = 0
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                connect. <span class="synchronous">If this paremeter is
                out-of-bound, an IndexSizeError exception MUST be
                thrown.</span> It is possible to connect an
                <a><code>AudioNode</code></a> output to more than one input
                with multiple calls to connect(). Thus, "fan-out" is supported.
              </dd>
              <dt>
                optional unsigned long input = 0
              </dt>
              <dd>
                The <code>input</code> parameter is an index describing which
                input of the destination <a><code>AudioNode</code></a> to
                connect to. <span class="synchronous">If this parameter is
                out-of-bounds, an IndexSizeError exception MUST be
                thrown.</span> It is possible to connect an
                <a><code>AudioNode</code></a> to another
                <a><code>AudioNode</code></a> which creates a <dfn>cycle</dfn>:
                an <a><code>AudioNode</code></a> may connect to another
                <a><code>AudioNode</code></a>, which in turn connects back to
                the first <a><code>AudioNode</code></a>. This is allowed only
                if there is at least one <a><code>DelayNode</code></a> in the
                <em>cycle</em> <span class="synchronous">or a NotSupportedError
                exception MUST be thrown.</span>
              </dd>
            </dl>
            <p>
              There can only be one connection between a given output of one
              specific node and a given input of another specific node.
              Multiple connections with the same termini are ignored. For
              example:
            </p>
            <pre class="example">
    nodeA.connect(nodeB);
    nodeA.connect(nodeB);
    
</pre>
            <p>
              will have the same effect as
            </p>
            <pre class="example">
      nodeA.connect(nodeB);
    
</pre>
            <p>
              This method returns <code>destination</code>
              <a><code>AudioNode</code></a> object.
            </p>
          </dd>
          <dt>
            void connect()
          </dt>
          <dd>
            <p>
              Connects the <a><code>AudioNode</code></a> to an
              <a><code>AudioParam</code></a>, controlling the parameter value
              with an audio-rate signal.
            </p>
            <dl class="parameters">
              <dt>
                AudioParam destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioParam</code></a> to connect to. This method does
                not return <code>destination</code>
                <a><code>AudioParam</code></a> object.
              </dd>
              <dt>
                optional unsigned long output = 0
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                connect. <span class="synchronous">If the
                <code>parameter</code> is out-of-bound, an IndexSizeError
                exception MUST be thrown.</span>
              </dd>
            </dl>
            <p>
              It is possible to connect an <a><code>AudioNode</code></a> output
              to more than one <a><code>AudioParam</code></a> with multiple
              calls to connect(). Thus, "fan-out" is supported.
            </p>
            <p>
              It is possible to connect more than one
              <a><code>AudioNode</code></a> output to a single
              <a><code>AudioParam</code></a> with multiple calls to connect().
              Thus, "fan-in" is supported.
            </p>
            <p>
              An <a><code>AudioParam</code></a> will take the rendered audio
              data from any <a><code>AudioNode</code></a> output connected to
              it and <a href="#down-mix">convert it to mono</a> by down-mixing
              if it is not already mono, then mix it together with other such
              outputs and finally will mix with the <em>intrinsic</em>
              parameter value (the <code>value</code> the
              <a><code>AudioParam</code></a> would normally have without any
              audio connections), including any timeline changes scheduled for
              the parameter.
            </p>
            <p>
              There can only be one connection between a given output of one
              specific node and a specific <a><code>AudioParam</code></a>.
              Multiple connections with the same termini are ignored. For
              example:
            </p>
            <pre>
      nodeA.connect(param);
      nodeA.connect(param);
    
</pre>will have the same effect as
            <pre>
      nodeA.connect(param);
    
</pre>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects all outgoing connections from the
              <a><code>AudioNode</code></a>.
            </p>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a single output of the <a><code>AudioNode</code></a>
              from any other <a><code>AudioNode</code></a> or
              <a><code>AudioParam</code></a> objects to which it is connected.
            </p>
            <dl class="parameters">
              <dt>
                unsigned long output
              </dt>
              <dd>
                This parameter is an index describing which output of the
                <a><code>AudioNode</code></a> to disconnect. It disconnects all
                outgoing connections from the given output. <span class=
                "synchronous">If this parameter is out-of-bounds, an
                IndexSizeError exception MUST be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects all outputs of the <a><code>AudioNode</code></a> that
              go to a specific destination <a><code>AudioNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to disconnect. It disconnects all
                outgoing connections to the given <code>destination</code>.
                <span class="synchronous">If there is no connection to
                <code>destination</code>, an InvalidAccessError exception MUST
                be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a specific output of the
              <a><code>AudioNode</code></a> from a specific destination
              <a><code>AudioNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to disconnect. <span class=
                "synchronous">If there is no connection to the
                <code>destination</code> from the given output, an
                InvalidAccessError exception MUST be thrown.</span>
              </dd>
              <dt>
                unsigned long output
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                disconnect. <span class="synchronous">If this parameter is
                out-of-bound, an IndexSizeError exception MUST be
                thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a specific output of the
              <a><code>AudioNode</code></a> from a specific input of some
              destination <a><code>AudioNode</code></a>.
            </p>
            <dl class="parameters">
              <dt>
                AudioNode destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioNode</code></a> to disconnect. <span class=
                "synchronous">If there is no connection to the
                <code>destination</code> from the given output to the given
                input, an InvalidAccessError exception MUST be thrown.</span>
              </dd>
              <dt>
                unsigned long output
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                disconnect. <span class="synchronous">If this parameter is
                out-of-bound, an IndexSizeError exception MUST be
                thrown.</span>
              </dd>
              <dt>
                unsigned long input
              </dt>
              <dd>
                The <code>input</code> parameter is an index describing which
                input of the destination <a><code>AudioNode</code></a> to
                disconnect. <span class="synchronous">If this parameter is
                out-of-bounds, an IndexSizeError exception MUST be
                thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects all outputs of the <a><code>AudioNode</code></a> that
              go to a specific destination <a><code>AudioParam</code></a>. The
              contribution of this <a><code>AudioNode</code></a> to the
              computed parameter value goes to 0 when this operation takes
              effect. The intrinsic parameter value is not affected by this
              operation.
            </p>
            <dl class="parameters">
              <dt>
                AudioParam destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioParam</code></a> to disconnect. <span class=
                "synchronous">If there is no connection to the
                <code>destination</code>, an InvalidAccessError exception MUST
                be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void disconnect()
          </dt>
          <dd>
            <p>
              Disconnects a specific output of the
              <a><code>AudioNode</code></a> from a specific destination
              <a><code>AudioParam</code></a>. The contribution of this
              <a><code>AudioNode</code></a> to the computed parameter value
              goes to 0 when this operation takes effect. The intrinsic
              parameter value is not affected by this operation.
            </p>
            <dl class="parameters">
              <dt>
                AudioParam destination
              </dt>
              <dd>
                The <code>destination</code> parameter is the
                <a><code>AudioParam</code></a> to disconnect. <span class=
                "synchronous">If there is no connection to the
                <code>destination</code>, an InvalidAccessError exception MUST
                be thrown.</span>
              </dd>
              <dt>
                unsigned long output
              </dt>
              <dd>
                The <code>output</code> parameter is an index describing which
                output of the <a><code>AudioNode</code></a> from which to
                disconnect. <span class="synchronous">If the
                <code>parameter</code> is out-of-bound, an IndexSizeError
                exception MUST be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            readonly attribute AudioContext context
          </dt>
          <dd>
            <p>
              The <a><code>AudioContext</code></a> which owns this
              <a><code>AudioNode</code></a>.
            </p>
          </dd>
          <dt>
            readonly attribute unsigned long numberOfInputs
          </dt>
          <dd>
            <p>
              The number of inputs feeding into the
              <a><code>AudioNode</code></a>. For <dfn>source nodes</dfn>, this
              will be 0. This attribute is predetermined for many
              <a><code>AudioNode</code></a> types, but some
              <a><code>AudioNode</code></a>, like the
              <a><code>ChannelMergerNode</code></a> and the
              <a><code>AudioWorkerNode</code></a> have variable number of
              inputs.
            </p>
          </dd>
          <dt>
            readonly attribute unsigned long numberOfOutputs
          </dt>
          <dd>
            <p>
              The number of outputs coming out of the
              <a><code>AudioNode</code></a>. This attribute is predetermined
              for some <a><code>AudioNode</code></a> types, but can be
              variable, like for the <a><code>ChannelSplitterNode</code></a>
              and the <a><code>AudioWorkerNode</code></a>.
            </p>
          </dd>
          <dt>
            attribute unsigned long channelCount
          </dt>
          <dd>
            <p>
              <dfn>channelCount</dfn> is the number of channels used when
              up-mixing and down-mixing connections to any inputs to the node.
              The default value is 2 except for specific nodes where its value
              is specially determined. This attribute has no effect for nodes
              with no inputs. <span class="synchronous">If this value is set to
              zero or to a value greater than the implementation's maximum
              number of channels the implementation MUST throw a
              NotSupportedError exception.</span>
            </p>
            <p>
              See the <a href="#channel-up-mixing-and-down-mixing"></a> section
              for more information on this attribute.
            </p>
          </dd>
          <dt>
            attribute ChannelCountMode channelCountMode
          </dt>
          <dd>
            <p>
              <dfn>channelCountMode</dfn> determines how channels will be
              counted when up-mixing and down-mixing connections to any inputs
              to the node. This attribute has no effect for nodes with no
              inputs.
            </p>
            <p>
              See the <a href="#channel-up-mixing-and-down-mixing"></a> section
              for more information on this attribute.
            </p>
          </dd>
          <dt>
            attribute ChannelInterpretation channelInterpretation
          </dt>
          <dd>
            <p>
              <dfn>channelInterpetation</dfn> determines how individual
              channels will be treated when up-mixing and down-mixing
              connections to any inputs to the node. This attribute has no
              effect for nodes with no inputs.
            </p>
            <p>
              See the <a href="#channel-up-mixing-and-down-mixing"></a> section
              for more information on this attribute.
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            Dictionaries
          </h2>
          <dl title="dictionary AudioNodeOptions" class="idl">
            <dt>
              unsigned long channelCount
            </dt>
            <dd>
              Desired number of channels for the <a>channelCount</a> attribute.
            </dd>
            <dt>
              ChannelCountMode channelCountMode
            </dt>
            <dd>
              Desired mode for the <a>channelCountMode</a> attribute.
            </dd>
            <dt>
              ChannelIntepretation channelInterpretation
            </dt>
            <dd>
              Desired mode for the <a>channelCountMode</a> attribute.
            </dd>
          </dl>
        </section>
        <section>
          <h3 id="lifetime-AudioNode" class="informative">
            Lifetime
          </h3>
          <p>
            <i>This section is informative.</i>
          </p>
          <p>
            An implementation may choose any method to avoid unnecessary
            resource usage and unbounded memory growth of unused/finished
            nodes. The following is a description to help guide the general
            expectation of how node lifetime would be managed.
          </p>
          <p>
            An <a><code>AudioNode</code></a> will live as long as there are any
            references to it. There are several types of references:
          </p>
          <ol>
            <li>A <em>normal</em> JavaScript reference obeying normal garbage
            collection rules.
            </li>
            <li>A <em>playing</em> reference for both
            <a><code>AudioBufferSourceNode</code></a>s and
            <a><code>OscillatorNode</code></a>s. These nodes maintain a
            <em>playing</em> reference to themselves while they are currently
            playing.
            </li>
            <li>A <em>connection</em> reference which occurs if another
            <a><code>AudioNode</code></a> is connected to it.
            </li>
            <li>A <em>tail-time</em> reference which an
            <a><code>AudioNode</code></a> maintains on itself as long as it has
            any internal processing state which has not yet been emitted. For
            example, a <a><code>ConvolverNode</code></a> has a tail which
            continues to play even after receiving silent input (think about
            clapping your hands in a large concert hall and continuing to hear
            the sound reverberate throughout the hall). Some
            <a><code>AudioNode</code></a>s have this property. Please see
            details for specific nodes.
            </li>
          </ol>
          <p>
            Any <a><code>AudioNode</code></a>s which are connected in a cycle
            <em>and</em> are directly or indirectly connected to the
            <a><code>AudioDestinationNode</code></a> of the
            <a><code>AudioContext</code></a> will stay alive as long as the
            <a><code>AudioContext</code></a> is alive.
          </p>
          <p class="note">
            The uninterrupted operation of <a>AudioNode</a>s implies that as
            long as live references exist to a node, the node will continue
            processing its inputs and evolving its internal state even if it is
            disconnected from the audio graph. Since this processing will
            consume CPU and power, developers should carefully consider the
            resource usage of disconnected nodes. In particular, it is a good
            idea to minimize resource consumption by explicitly putting
            disconnected nodes into a stopped state when possible.
          </p>
          <p>
            When an <a><code>AudioNode</code></a> has no references it will be
            deleted. Before it is deleted, it will disconnect itself from any
            other <a><code>AudioNode</code></a>s which it is connected to. In
            this way it releases all connection references (3) it has to other
            nodes.
          </p>
          <p>
            Regardless of any of the above references, it can be assumed that
            the <a><code>AudioNode</code></a> will be deleted when its
            <a><code>AudioContext</code></a> is deleted.
          </p>
        </section>
      </section>
      <section>
        <h2 id="AudioDestinationNode">
          The AudioDestinationNode Interface
        </h2>
        <p>
          This is an <a><code>AudioNode</code></a> representing the final audio
          destination and is what the user will ultimately hear. It can often
          be considered as an audio output device which is connected to
          speakers. All rendered audio to be heard will be routed to this node,
          a "terminal" node in the <a><code>AudioContext</code></a>'s routing
          graph. There is only a single AudioDestinationNode per
          <a><code>AudioContext</code></a>, provided through the
          <code>destination</code> attribute of
          <a><code>AudioContext</code></a>.
        </p>
        <p>
          The output of a <a><code>AudioDestinationNode</code></a> is produced
          by <a href="#SummingJunction">summing its input</a>, allowing to
          capture the output of an <a><code>AudioContext</code></a> into, for
          example, a <a><code>MediaStreamAudioDestinationNode</code></a>, or a
          <code>MediaRecorder</code> (described in [[mediastream-recording]]).
        </p>
        <pre>
      numberOfInputs  : 1
      numberOfOutputs : 1

      channelCount = 2;
      channelCountMode = "explicit";
      channelInterpretation = "speakers";
</pre>
        <dl title="interface AudioDestinationNode : AudioNode" class="idl">
          <dt>
            readonly attribute unsigned long maxChannelCount
          </dt>
          <dd>
            <p>
              The maximum number of channels that the <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a>
              attribute can be set to. An
              <a><code>AudioDestinationNode</code></a> representing the audio
              hardware end-point (the normal case) can potentially output more
              than 2 channels of audio if the audio hardware is multi-channel.
              <code>maxChannelCount</code> is the maximum number of channels
              that this hardware is capable of supporting. If this value is 0,
              then this indicates that <a href=
              "#widl-AudioNode-channelCount">channelCount</a> may not be
              changed. This will be the case for an
              <a><code>AudioDestinationNode</code></a> in an
              <a><code>OfflineAudioContext</code></a> and also for basic
              implementations with hardware support for stereo output only.
            </p>
            <p>
              <a href="#widl-AudioNode-channelCount">channelCount</a> defaults
              to 2 for a destination in a normal
              <a><code>AudioContext</code></a>, and may be set to any non-zero
              value less than or equal to <code>maxChannelCount</code>.
              <span class="synchronous">An <a><code>IndexSizeError</code></a>
              exception MUST be thrown if this value is not within the valid
              range.</span> Giving a concrete example, if the audio hardware
              supports 8-channel output, then we may set <a href=
              "#widl-AudioNode-channelCount">channelCount</a> to 8, and render
              8-channels of output.
            </p>
            <p>
              For an<a><code>AudioDestinationNode</code></a> in an
              <a><code>OfflineAudioContext</code></a>, the <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a> is
              determined when the offline context is created and this value may
              not be changed.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="AudioParam">
          The AudioParam Interface
        </h2>
        <p>
          <a><code>AudioParam</code></a> controls an individual aspect of an
          <a><code>AudioNode</code></a>'s functioning, such as volume. The
          parameter can be set immediately to a particular value using the
          <code>value</code> attribute. Or, value changes can be scheduled to
          happen at very precise times (in the coordinate system of
          <a><code>AudioContext</code></a>'s <a href=
          "#widl-BaseAudioContext-currentTime">currentTime</a> attribute), for
          envelopes, volume fades, LFOs, filter sweeps, grain windows, etc. In
          this way, arbitrary timeline-based automation curves can be set on
          any <a><code>AudioParam</code></a>. Additionally, audio signals from
          the outputs of <a><code>AudioNode</code></a>s can be connected to an
          <a><code>AudioParam</code></a>, summing with the <em>intrinsic</em>
          parameter value.
        </p>
        <p>
          Some synthesis and processing <a><code>AudioNode</code></a>s have
          <code>AudioParams</code> as attributes whose values must be taken
          into account on a per-audio-sample basis. For other
          <code>AudioParams</code>, sample-accuracy is not important and the
          value changes can be sampled more coarsely. Each individual
          <code>AudioParam</code> will specify that it is either an
          <a>a-rate</a> parameter which means that its values must be taken
          into account on a per-audio-sample basis, or it is a <a>k-rate</a>
          parameter.
        </p>
        <p>
          Implementations must use block processing, with each
          <a><code>AudioNode</code></a> processing 128 sample-frames in each
          block.
        </p>
        <p>
          For each 128 sample-frame block, the value of a <dfn id=
          "k-rate">k-rate</dfn> parameter must be sampled at the time of the
          very first sample-frame, and that value must be used for the entire
          block. <dfn id="a-rate">a-rate</dfn> parameters must be sampled for
          each sample-frame of the block.
        </p>
        <p>
          Each <code>AudioParam</code> includes <code>min</code> and
          <code>max</code> attributes that together form the <a>nominal
          range</a> for the parameter. In effect, value of the parameter is
          clamped to the range \([\mathrm{minValue}, \mathrm{maxValue}]\). See
          the section <a href="#computation-of-value">Computation of Value</a>
          for full details.
        </p>
        <p>
          An <code>AudioParam</code> maintains a time-ordered event list which
          is initially empty. The times are in the time coordinate system of
          the <a><code>AudioContext</code></a>'s <a href=
          "#widl-BaseAudioContext-currentTime">currentTime</a> attribute. The
          events define a mapping from time to value. The following methods can
          change the event list by adding a new event into the list of a type
          specific to the method. Each event has a time associated with it, and
          the events will always be kept in time-order in the list. These
          methods will be called <dfn>automation methods</dfn>:
        </p>
        <ul>
          <li>
            <a href=
            "#widl-AudioParam-setValueAtTime-AudioParam-float-value-double-startTime">
            setValueAtTime()</a> - <em>SetValue</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-linearRampToValueAtTime-AudioParam-float-value-double-endTime">
            linearRampToValueAtTime()</a> - <em>LinearRampToValue</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-exponentialRampToValueAtTime-AudioParam-float-value-double-endTime">
            exponentialRampToValueAtTime()</a> -
            <em>ExponentialRampToValue</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-setTargetAtTime-AudioParam-float-target-double-startTime-float-timeConstant">
            setTargetAtTime()</a> - <em>SetTarget</em>
          </li>
          <li>
            <a href=
            "#widl-AudioParam-setValueCurveAtTime-AudioParam-Float32Array-values-double-startTime-double-duration">
            setValueCurveAtTime()</a> - <em>SetValueCurve</em>
          </li>
        </ul>
        <p>
          The following rules will apply when calling these methods:
        </p>
        <ul>
          <li>If one of these events is added at a time where there is already
          an event of the exact same type, then the new event will replace the
          old one.
          </li>
          <li>If one of these events is added at a time where there is already
          one or more events of a different type, then it will be placed in the
          list after them, but before events whose times are after the event.
          </li>
          <li>
            <span class="synchronous">If setValueCurveAtTime() is called for
            time \(T\) and duration \(D\) and there are any events having a
            time greater than \(T\), but less than \(T + D\), then a
            NotSupportedError exception MUST be thrown.</span> In other words,
            it's not ok to schedule a value curve during a time period
            containing other events.
          </li>
          <li>
            <span class="synchronous">Similarly a NotSupportedError exception
            MUST be thrown if any <em>automation</em> method is called at a
            time which is inside of the time interval of a
            <em>SetValueCurve</em> event at time T and duration D.</span>
          </li>
        </ul>
        <dl title="interface AudioParam" class="idl">
          <dt>
            attribute float value
          </dt>
          <dd>
            <p>
              The parameter's floating-point value. This attribute is
              initialized to the <code>defaultValue</code>.
            </p>
            <p>
              The effect of setting this attribute is equivalent to calling
              <code>setValueAtTime()</code> with the current
              <code>AudioContext</code>'s <code>currentTime</code> and the
              requested value. Subsequent accesses to this attribute's getter
              will return the same value.
            </p>
          </dd>
          <dt>
            readonly attribute float defaultValue
          </dt>
          <dd>
            <p>
              Initial value for the <code>value</code> attribute.
            </p>
          </dd>
          <dt>
            readonly attribute float minValue
          </dt>
          <dd>
            <p>
              The nominal minimum value that the parameter can take. Together
              with <code>maxValue</code>, this forms the <a>nominal range</a>
              for this parameter.
            </p>
          </dd>
          <dt>
            readonly attribute float maxValue
          </dt>
          <dd>
            <p>
              The nominal maximum value that the parameter can take. Together
              with <code>minValue</code>, this forms the <a>nominal range</a>
              for this parameter.
            </p>
          </dd>
          <dt>
            AudioParam setValueAtTime(float value, double startTime)
          </dt>
          <dd>
            <p>
              Schedules a parameter value change at the given time.
            </p>
            <dl class="parameters">
              <dt>
                float value
              </dt>
              <dd>
                The value the parameter will change to at the given time.
              </dd>
              <dt>
                double startTime
              </dt>
              <dd>
                The time in the same time coordinate system as the
                <a><code>BaseAudioContext</code></a>'s <a href=
                "#widl-AudioContext-currentTime">currentTime</a> attribute at
                which the parameter changes to the given value. <span class=
                "synchronous">A TypeError exception MUST be thrown if
                <code>startTime</code> is negative or is not a finite
                number.</span>
              </dd>
            </dl>
            <p>
              If there are no more events after this <em>SetValue</em> event,
              then for \(t \geq T_0\), \(v(t) = V\), where \(T_0\) is the
              <code>startTime</code> parameter and \(V\) is the
              <code>value</code> parameter. In other words, the value will
              remain constant.
            </p>
            <p>
              If the next event (having time \(T_1\)) after this
              <em>SetValue</em> event is not of type <em>LinearRampToValue</em>
              or <em>ExponentialRampToValue</em>, then, for \(T_0 \leq t &lt;
              T_1\):
            </p>
            <pre class="nohighlight">
              $$
                v(t) = V
              $$
            
</pre>
            <p>
              In other words, the value will remain constant during this time
              interval, allowing the creation of "step" functions.
            </p>
            <p>
              If the next event after this <em>SetValue</em> event is of type
              <em>LinearRampToValue</em> or <em>ExponentialRampToValue</em>
              then please see <code><a href=
              "#widl-AudioParam-linearRampToValueAtTime-AudioParam-float-value-double-endTime">
              linearRampToValueAtTime</a></code> or <code><a href=
              "#widl-AudioParam-exponentialRampToValueAtTime-AudioParam-float-value-double-endTime">
              exponentialRampToValueAtTime</a></code>, respectively.
            </p>
          </dd>
          <dt>
            AudioParam linearRampToValueAtTime(float value, double endTime)
          </dt>
          <dd>
            <p>
              Schedules a linear continuous change in parameter value from the
              previous scheduled parameter value to the given value.
            </p>
            <dl class="parameters">
              <dt>
                float value
              </dt>
              <dd>
                The value the parameter will linearly ramp to at the given
                time.
              </dd>
              <dt>
                double endTime
              </dt>
              <dd>
                The time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute
                at which the automation ends. <span class="synchronous">A
                TypeError exception MUST be thrown if <code>endTime</code> is
                negative or is not a finite number.</span>
              </dd>
            </dl>
            <p>
              The value during the time interval \(T_0 \leq t &lt; T_1\) (where
              \(T_0\) is the time of the previous event and \(T_1\) is the
              <code>endTime</code> parameter passed into this method) will be
              calculated as:
            </p>
            <pre class="nohighlight">
              $$
                v(t) = V_0 + (V_1 - V_0) \frac{t - T_0}{T_1 - T_0}
              $$
            </pre>
            <p>
              Where \(V_0\) is the value at the time \(T_0\) and \(V_1\) is the
              <code>value</code> parameter passed into this method.
            </p>
            <p>
              If there are no more events after this LinearRampToValue event
              then for \(t \geq T_1\), \(v(t) = V_1\).
            </p>
            <p>
              If there is no event preceding this event, the linear ramp
              behaves as if <code>setValueAtTime(value, currentTime)</code>
              were called where <code>value</code> is the current value of the
              attribute and <code>currentTime</code> is the context <a href=
              "#widl-AudioContext-currentTime">currentTime</a> at the time
              <code>linearRampToValueAtTime</code> is called.
            </p>
            <p>
              If the preceding event is a <em>SetTarget</em> event, \(T_0\) and
              \(V_0\) are chosen from the current time and value of
              <em>SetTarget</em> automation. That is, if the <em>SetTarget</em>
              event has not started, \(T_0\) is the start time of the event,
              and \(V_0\) is the value just before the <em>SetTarget</em> event
              starts. In this case, the <em>LinearRampToValue</em> event
              effectively replaces the <em>SetTarget</em> event. If the
              <em>SetTarget</em> event has already started, \(T_0\) is the
              current context time, and \(V_0\) is the current
              <em>SetTarget</em> automation value at time \(T_0\). In both
              cases, the automation curve is continuous.
            </p>
          </dd>
          <dt>
            AudioParam exponentialRampToValueAtTime(float value, double
            endTime)
          </dt>
          <dd>
            <p>
              Schedules an exponential continuous change in parameter value
              from the previous scheduled parameter value to the given value.
              Parameters representing filter frequencies and playback rate are
              best changed exponentially because of the way humans perceive
              sound.
            </p>
            <p>
              The value during the time interval \(T_0 \leq t &lt; T_1\) (where
              \(T_0\) is the time of the previous event and \(T_1\) is the
              <code>endTime</code> parameter passed into this method) will be
              calculated as:
            </p>
            <pre class="nohighlight">
              $$
                v(t) = V_0 \left(\frac{V_1}{V_0}\right)^\frac{t - T_0}{T_1 - T_0}
              $$
            </pre>
            <p>
              where \(V_0\) is the value at the time \(T_0\) and \(V_1\) is the
              <code>value</code> parameter passed into this method. If \(V_0\)
              and \(V_1\) have opposite signs or if \(V_0\) is zero, then
              \(v(t) = V_0\) for \(T_0 \le t \lt T_1\).
            </p>
            <p>
              This also implies an exponential ramp to 0 is not possible. A
              good approximation can be achieved using <a href=
              "#widl-AudioParam-setTargetAtTime-AudioParam-float-target-double-startTime-float-timeConstant">
              setTargetAtTime</a> with an appropriately chosen time constant.
            </p>
            <p>
              If there are no more events after this ExponentialRampToValue
              event then for \(t \geq T_1\), \(v(t) = V_1\).
            </p>
            <p>
              If there is no event preceding this event, the exponential ramp
              behaves as if <code>setValueAtTime(value, currentTime)</code>
              were called where <code>value</code> is the current value of the
              attribute and <code>currentTime</code> is the context <a href=
              "#widl-AudioContext-currentTime">currentTime</a> at the time
              <code>exponentialRampToValueAtTime</code> is called.
            </p>
            <p>
              If the preceding event is a <em>SetTarget</em> event, \(T_0\) and
              \(V_0\) are chosen from the current time and value of
              <em>SetTarget</em> automation. That is, if the <em>SetTarget</em>
              event has not started, \(T_0\) is the start time of the event,
              and \(V_0\) is the value just before the <em>SetTarget</em> event
              starts. In this case, the <em>ExponentialRampToValue</em> event
              effectively replaces the <em>SetTarget</em> event. If the
              <em>SetTarget</em> event has already started, \(T_0\) is the
              current context time, and \(V_0\) is the current
              <em>SetTarget</em> automation value at time \(T_0\). In both
              cases, the automation curve is continuous.
            </p>
            <dl class="parameters">
              <dt>
                float value
              </dt>
              <dd>
                The value the parameter will exponentially ramp to at the given
                time. <span class="synchronous">A NotSupportedError exception
                MUST be thrown if this value is equal to 0.</span>
              </dd>
              <dt>
                double endTime
              </dt>
              <dd>
                The time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute
                where the exponential ramp ends. <span class="synchronous">A
                TypeError exception MUST be thrown if <code>endTime</code> is
                negative or is not a finite number.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            AudioParam setTargetAtTime(float target, double startTime, float
            timeConstant)
          </dt>
          <dd>
            <p>
              Start exponentially approaching the target value at the given
              time with a rate having the given time constant. Among other
              uses, this is useful for implementing the "decay" and "release"
              portions of an ADSR envelope. Please note that the parameter
              value does not immediately change to the target value at the
              given time, but instead gradually changes to the target value.
            </p>
            <dl class="parameters">
              <dt>
                float target
              </dt>
              <dd>
                The value the parameter will <em>start</em> changing to at the
                given time.
              </dd>
              <dt>
                double startTime
              </dt>
              <dd>
                The time at which the exponential approach will begin, in the
                same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute.
                <span class="synchronous">A TypeError exception MUST be thrown
                if <code>start</code> is negative or is not a finite
                number.</span> If <var>startTime</var> is less than <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a>, it is
                clamped to <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a>.
              </dd>
              <dt>
                float timeConstant
              </dt>
              <dd>
                The time-constant value of first-order filter (exponential)
                approach to the target value. The larger this value is, the
                slower the transition will be. <span class="synchronous">The
                value must be strictly positive or a TypeError exception MUST
                be thrown.</span>
                <p>
                  More precisely, <em>timeConstant</em> is the time it takes a
                  first-order linear continuous time-invariant system to reach
                  the value \(1 - 1/e\) (around 63.2%) given a step input
                  response (transition from 0 to 1 value).
                </p>
              </dd>
            </dl>
            <p>
              During the time interval: \(T_0 \leq t\), where \(T_0\) is the
              <code>startTime</code> parameter:
            </p>
            <pre class="nohighlight">
              $$
                v(t) = V_1 + (V_0 - V_1)\, e^{-\left(\frac{t - T_0}{\tau}\right)}
              $$
            
</pre>
            <p>
              where \(V_0\) is the initial value (the <code>.value</code>
              attribute) at \(T_0\) (the <code>startTime</code> parameter),
              \(V_1\) is equal to the <code>target</code> parameter, and
              \(\tau\) is the <code>timeConstant</code> parameter.
            </p>
            <p>
              If a <em>LinearRampToValue</em> or
              <em>ExponentialRampToValue</em> event follows this event, the
              behavior is described in <code><a href=
              "#widl-AudioParam-linearRampToValueAtTime-AudioParam-float-value-double-endTime">
              linearRampToValueAtTime</a></code> or <code><a href=
              "#widl-AudioParam-exponentialRampToValueAtTime-AudioParam-float-value-double-endTime">
              exponentialRampToValueAtTime</a></code>, respectively. For all
              other events, the <em>SetTarget</em> event ends at the time of
              the next event.
            </p>
          </dd>
          <dt>
            AudioParam setValueCurveAtTime(Float32Array values, double
            startTime, double duration)
          </dt>
          <dd>
            <p>
              Sets an array of arbitrary parameter values starting at the given
              time for the given duration. The number of values will be scaled
              to fit into the desired duration.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array values
              </dt>
              <dd>
                <p>
                  A Float32Array representing a parameter value curve. These
                  values will apply starting at the given time and lasting for
                  the given duration. When this method is called, an internal
                  copy of the curve is created for automation purposes.
                  Subsequent modifications of the contents of the passed-in
                  array therefore have no effect on the <a>AudioParam</a>.
                </p>
                <p>
                  An <code>InvalidStateError</code> MUST be thrown if this
                  attribute is with a Float32Array that has a length less than
                  2.
                </p>
              </dd>
              <dt>
                double startTime
              </dt>
              <dd>
                The start time in the same time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute
                at which the value curve will be applied. <span class=
                "synchronous">A TypeError exception MUST be thrown if
                <code>startTime</code> is negative or is not a finite
                number.</span>. If <var>startTime</var> is less than <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a>, it is
                clamped to <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a>.
              </dd>
              <dt>
                double duration
              </dt>
              <dd>
                The amount of time in seconds (after the <em>time</em>
                parameter) where values will be calculated according to the
                <em>values</em> parameter. A <code>TypeError</code> exception
                MUST be thrown if <code>duration</code> is not strictly
                positive or is not a finite number.
              </dd>
            </dl>
            <p>
              Let \(T_0\) be <code>startTime</code>, \(T_D\) be
              <code>duration</code>, \(V\) be the <code>values</code> array,
              and \(N\) be the length of the <code>values</code> array. Then,
              during the time interval: \(T_0 \le t &lt; T_0 + T_D\), let
            </p>
            <pre class="nohighlight">
              $$
                \begin{align*} k &amp;= \left\lfloor \frac{N - 1}{T_D}(t-T_0) \right\rfloor \\
                \end{align*}
              $$
            
</pre>
            <p>
              Then \(v(t)\) is computed by linearly interpolating between
              \(V[k]\) and \(V[k+1]\),
            </p>
            <p>
              After the end of the curve time interval (\(t \ge T_0 + T_D\)),
              the value will remain constant at the final curve value, until
              there is another automation event (if any).
            </p>
            <p>
              An implicit call to <code><a href=
              "#widl-AudioParam-setValueAtTime-AudioParam-float-value-double-startTime">
              setValueAtTime</a></code> is made at time \(T_0 + T_D\) with
              value \(V[N-1]\) so that following automations will start from
              the end of the <code><a href=
              "#widl-AudioParam-setValueCurveAtTime-AudioParam-Float32Array-values-double-startTime-double-duration">
              setValueCurveAtTime</a></code> event.
            </p>
          </dd>
          <dt>
            AudioParam cancelScheduledValues(double startTime)
          </dt>
          <dd>
            <p>
              Cancels all scheduled parameter changes with times greater than
              or equal to <code>startTime</code>. Active
              <code><a>setTargetAtTime</a></code> automations (those with
              <code>startTime</code> less than the supplied time value) will
              also be cancelled.
            </p>
            <dl class="parameters">
              <dt>
                double startTime
              </dt>
              <dd>
                The starting time at and after which any previously scheduled
                parameter changes will be cancelled. It is a time in the same
                time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute.
                <span class="synchronous">A TypeError exception MUST be thrown
                if <code>startTime</code> is negative or is not a finite
                number.</span> If <var>startTime</var> is less than <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a>, it is
                clamped to <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a>.
              </dd>
            </dl>
          </dd>
        </dl>
        <section>
          <h3>
            Computation of Value
          </h3>
          <p>
            There are two different kind of <a>AudioParam</a>s, <a>simple
            parameters</a> and <a>compound parameters</a>. <dfn data-lt=
            "simple parameter">Simple parameters</dfn> (the default) are used
            on their own to compute the final audio output of an
            <a>AudioNode</a>. <dfn data-lt="compound parameter">Compound
            parameters</dfn> are <a>AudioParam</a> that are used with other
            <a>AudioParam</a>s to compute a value that is then used as an input
            to compute the output of an <a>AudioNode</a>.
          </p>
          <p>
            The <dfn>computedValue</dfn> is the final value controlling the
            audio DSP and is computed by the audio rendering thread during each
            rendering time quantum. It must be internally computed as follows:
          </p>
          <ol>
            <li>An <em>intrinsic</em> parameter value will be calculated at
            each time, which is either the value set directly to the
            <code>value</code> attribute, or, if there are any scheduled
            parameter changes (automation events) with times before or at this
            time, the value as calculated from these events. When read, the
            <code>value</code> attribute always returns the <em>intrinsic</em>
            value for the current time. If automation events are removed from a
            given time range, then the <em>intrinsic</em> value will remain
            unchanged and stay at its previous value until either the
            <code>value</code> attribute is directly set, or automation events
            are added for the time range.
            </li>
            <li>The <em>computedValue</em> is the sum of the <em>intrinsic</em>
            value and the value of the <a href="#input-audioparam-buffer">input
            <code>AudioParam</code> buffer</a>.
            </li>
            <li>If this <a>AudioParam</a> is a <a>compound parameter</a>,
            compute its final value with other <a>AudioParam</a>s.
            </li>
          </ol>
          <p>
            The <dfn>nominal range</dfn> for a <a>computedValue</a> are the
            lower and higher values this parameter can effectively have. For
            <a>simple parameters</a>, the <a>computedValue</a> is clamped to
            the <a>nominal range</a> for this parameter. <a>Compound
            parameters</a> have their final value clamped to their <a>nominal
            range</a> after having been computed from the different
            <a>AudioParam</a> value they are composed of.
          </p>
          <p>
            When automation methods are used, clamping is still applied.
            However, the automation is run as if there were no clamping at all.
            Only when the automation values are to be applied to the output is
            the clamping done as specified above.
          </p>
          <p>
            For example, consider a node \(N\) has an AudioParam \(p\) with a
            nominal range of \([0, 1]\), and following automation sequence
          </p>
          <pre class="example">
            N.p.setValueAtTime(0, 0);
            N.p.linearRampToValueAtTime(100, 1);
            N.p.linearRampToValueAtTime(0, 2)
          </pre>
          <p>
            The initial slope of the curve is 100, until it reaches the maximum
            value of 1, at which time, the output is held constant. Finally,
            near time 2, the slope of the curve is -100.
          </p>
        </section>
        <section>
          <h3 id="example1-AudioParam">
            AudioParam Automation Example
          </h3>
          <figure>
            <!-- The image here was created from
  http://googlechrome.github.io/web-audio-samples/samples/audio/timeline.html -->
            <img alt="AudioParam automation" src=
            "images/audioparam-automation1.png">
            <figcaption>
              An example of parameter automation.
            </figcaption>
          </figure>
          <pre class="code example">
var curveLength = 44100;
var curve = new Float32Array(curveLength);
for (var i = 0; i &lt; curveLength; ++i)
    curve[i] = Math.sin(Math.PI * i / curveLength);

var t0 = 0;
var t1 = 0.1;
var t2 = 0.2;
var t3 = 0.3;
var t4 = 0.325;
var t5 = 0.5;
var t6 = 0.6;
var t7 = 0.7;
var t8 = 1.0;
var timeConstant = 0.1;

param.setValueAtTime(0.2, t0);
param.setValueAtTime(0.3, t1);
param.setValueAtTime(0.4, t2);
param.linearRampToValueAtTime(1, t3);
param.linearRampToValueAtTime(0.8, t4);
param.setTargetAtTime(.5, t4, timeConstant);
// Compute where the setTargetAtTime will be at time t5 so we can make
// the following exponential start at the right point so there's no
// jump discontinuity.  From the spec, we have
//   v(t) = 0.5 + (0.8 - 0.5)*exp(-(t-t4)/timeConstant)
// Thus v(t5) = 0.5 + (0.8 - 0.5)*exp(-(t5-t4)/timeConstant)
param.setValueAtTime(0.5 + (0.8 - 0.5)*Math.exp(-(t5 - t4)/timeConstant), t5);
param.exponentialRampToValueAtTime(0.75, t6);
param.exponentialRampToValueAtTime(0.05, t7);
param.setValueCurveAtTime(curve, t7, t8 - t7);
</pre>
        </section>
      </section>
      <section>
        <h2 id="GainNode">
          The GainNode Interface
        </h2>
        <p>
          Changing the gain of an audio signal is a fundamental operation in
          audio applications. The <code>GainNode</code> is one of the building
          blocks for creating <a href="#mixer-gain-structure">mixers</a>. This
          interface is an <a><code>AudioNode</code></a> with a single input and
          single output:
        </p>
        <pre>
  numberOfInputs  : 1
  numberOfOutputs : 1

  channelCountMode = "max";
  channelInterpretation = "speakers";
</pre>
        <p>
          Each sample of each channel of the input data of the
          <a><code>GainNode</code></a> MUST be multiplied by the
          <a>computedValue</a> of the <a href=
          "#widl-GainNode-gain"><code>gain</code></a>
          <a><code>AudioParam</code></a>.
        </p>
        <dl title=
        "[Constructor(BaseAudioContext context, optional GainOptions options)]interface GainNode : AudioNode"
        class="idl">
          <dt>
            readonly attribute AudioParam gain
          </dt>
          <dd>
            <p>
              Represents the amount of gain to apply. Its default
              <code>value</code> is 1 (no gain change). This parameter is
              <a>a-rate</a>. Its <a>nominal range</a> is (-\(\infty\),
              +\(\infty\)).
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            GainOptions
          </h2>
          <p>
            This specifies options to use in constructing a
            <a><code>GainNode</code></a>. All members are optional; if not
            specified, the normal defaults are used in constructing the node.
          </p>
          <dl title="dictionary GainOptions : AudioNodeOptions" class="idl">
            <dt>
              float gain
            </dt>
            <dd>
              The initial gain value for the <a href=
              "#widl-GainNode-gain"><code>gain</code></a> AudioParam.
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2 id="DelayNode">
          The DelayNode Interface
        </h2>
        <p>
          A delay-line is a fundamental building block in audio applications.
          This interface is an <a><code>AudioNode</code></a> with a single
          input and single output:
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <p>
          It delays the incoming audio signal by a certain amount.
          Specifically, at each time <em>t</em>, input signal
          <em>input(t)</em>, delay time <em>delayTime(t)</em> and output signal
          <em>output(t)</em>, the output will be <em>output(t) = input(t -
          delayTime(t))</em>. The default <code>delayTime</code> is 0 seconds
          (no delay).
        </p>
        <p>
          When the number of channels in a <a>DelayNode</a>'s input changes
          (thus changing the output channel count also), there may be delayed
          audio samples which have not yet been output by the node and are part
          of its internal state. If these samples were received earlier with a
          different channel count, they must be upmixed or downmixed before
          being combined with newly received input so that all internal
          delay-line mixing takes place using the single prevailing channel
          layout.
        </p>
        <dl title="interface DelayNode : AudioNode" class="idl">
          <dt>
            readonly attribute AudioParam delayTime
          </dt>
          <dd>
            <p>
              An <a><code>AudioParam</code></a> object representing the amount
              of delay (in seconds) to apply. Its default <code>value</code> is
              0 (no delay). The minimum value is 0 and the maximum value is
              determined by the <code>maxDelayTime</code> argument to the
              <code>AudioContext</code> method <code>createDelay</code>.
            </p>
            <p>
              If <a><code>DelayNode</code></a> is part of a <a>cycle</a>, then
              the value of the <a><code>delayTime</code></a> attribute is
              clamped to a minimum of 128 frames (one block).
            </p>
            <p>
              Its <a>nominal range</a> is [0, maxDelayTime], where
              <code>maxDelayTime</code> is the value passed to the
              <code>createDelay</code> method on the
              <a><code>AudioContext</code></a>.
            </p>
            <p>
              This parameter is <a>a-rate</a>.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2 id="AudioBuffer">
          The AudioBuffer Interface
        </h2>
        <p>
          This interface represents a memory-resident audio asset (for one-shot
          sounds and other short audio clips). Its format is non-interleaved
          IEEE 32-bit linear PCM with a nominal range of -1 -&gt; +1. It can
          contain one or more channels. Typically, it would be expected that
          the length of the PCM data would be fairly short (usually somewhat
          less than a minute). For longer sounds, such as music soundtracks,
          streaming should be used with the <code>audio</code> element and
          <code>MediaElementAudioSourceNode</code>.
        </p>
        <p>
          An <a>AudioBuffer</a> may be used by one or more
          <a><code>AudioContext</code></a>s, and can be shared between an
          <a><code>OfflineAudioContext</code></a> and an
          <a><code>AudioContext</code></a>.
        </p>
        <dl title="interface AudioBuffer" class="idl">
          <dt>
            readonly attribute float sampleRate
          </dt>
          <dd>
            <p>
              The sample-rate for the PCM audio data in samples per second.
            </p>
          </dd>
          <dt>
            readonly attribute unsigned long length
          </dt>
          <dd>
            <p>
              Length of the PCM audio data in sample-frames.
            </p>
          </dd>
          <dt>
            readonly attribute double duration
          </dt>
          <dd>
            <p>
              Duration of the PCM audio data in seconds.
            </p>
          </dd>
          <dt>
            readonly attribute unsigned long numberOfChannels
          </dt>
          <dd>
            <p>
              The number of discrete audio channels.
            </p>
          </dd>
          <dt>
            Float32Array getChannelData()
          </dt>
          <dd>
            <p>
              Returns the <code>Float32Array</code> representing the PCM audio
              data for the specific channel.
            </p>
            <dl class="parameters">
              <dt>
                unsigned long channel
              </dt>
              <dd>
                This parameter is an index representing the particular channel
                to get data for. An index value of 0 represents the first
                channel. <span class="synchronous">This index value MUST be
                less than <code>numberOfChannels</code> or an IndexSizeError
                exception MUST be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void copyFromChannel()
          </dt>
          <dd>
            <p>
              The <code>copyFromChannel</code> method copies the samples from
              the specified channel of the <a>AudioBuffer</a> to the
              <code>destination</code> array.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array destination
              </dt>
              <dd>
                The array the channel data will be copied to.
              </dd>
              <dt>
                unsigned long channelNumber
              </dt>
              <dd>
                The index of the channel to copy the data from. If
                <code>channelNumber</code> is greater or equal than the number
                of channel of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
              <dt>
                optional unsigned long startInChannel = 0
              </dt>
              <dd>
                An optional offset to copy the data from. If
                <code>startInChannel</code> is greater than the
                <code>length</code> of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
            </dl>
          </dd>
          <dt>
            void copyToChannel()
          </dt>
          <dd>
            <p>
              The <code>copyToChannel</code> method copies the samples to the
              specified channel of the <a>AudioBuffer</a>, from the
              <code>source</code> array.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array source
              </dt>
              <dd>
                The array the channel data will be copied from.
              </dd>
              <dt>
                unsigned long channelNumber
              </dt>
              <dd>
                The index of the channel to copy the data to. If
                <code>channelNumber</code> is greater or equal than the number
                of channel of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
              <dt>
                optional unsigned long startInChannel = 0
              </dt>
              <dd>
                An optional offset to copy the data to. If
                <code>startInChannel</code> is greater than the
                <code>length</code> of the <a>AudioBuffer</a>, an
                <code>IndexSizeError</code> MUST be thrown.
              </dd>
            </dl>
          </dd>
        </dl>
        <p class="note">
          The methods <code>copyToChannel</code> and
          <code>copyFromChannel</code> can be used to fill part of an array by
          passing in a <code>Float32Array</code> that's a view onto the larger
          array. When reading data from an <a>AudioBuffer</a>'s channels, and
          the data can be processed in chunks, <code>copyFromChannel</code>
          should be preferred to calling <code>getChannelData</code> and
          accessing the resulting array, because it may avoid unnecessary
          memory allocation and copying.
        </p>
        <p>
          An internal operation <a href="#acquire-the-content">acquire the
          contents of an <code>AudioBuffer</code></a> is invoked when the
          contents of an <a>AudioBuffer</a> are needed by some API
          implementation. This operation returns immutable channel data to the
          invoker.
        </p>
        <p>
          When an <dfn id="acquire-the-content">acquire the content</dfn>
          operation occurs on an <a>AudioBuffer</a>, run the following steps:
        </p>
        <ol>
          <li>If any of the <a>AudioBuffer</a>'s <code>ArrayBuffer</code> have
          been <em>detached</em>, abort these steps, and return a zero-length
          channel data buffers to the invoker.
          </li>
          <li>
            <em>Detach</em>all <code>ArrayBuffer</code>s for arrays previously
            returned by <code>getChannelData</code> on this <a>AudioBuffer</a>.
          </li>
          <li>Retain the underlying data buffers from those
          <code>ArrayBuffer</code>s and return references to them to the
          invoker.
          </li>
          <li>Attach <code>ArrayBuffer</code>s containing copies of the data to
          the <a>AudioBuffer</a>, to be returned by the next call to
          <code>getChannelData</code>.
          </li>
        </ol>The <a href="#acquire-the-content">acquire the contents of an
        AudioBuffer</a> operation is invoked in the following cases:
        <ul>
          <li>When <code>AudioBufferSourceNode.start</code> is called, it
          <a href="#acquire-the-content">acquires the contents</a> of the
          node's <code>buffer</code>. If the operation fails, nothing is
          played.
          </li>
          <li>When a <a>ConvolverNode</a>'s <code>buffer</code> is set to an
          <a>AudioBuffer</a> while the node is connected to an output node, or
          a <a>ConvolverNode</a> is connected to an output node while the
          <a>ConvolverNode</a>'s <code>buffer</code> is set to an
          <a>AudioBuffer</a>, it <a href="#acquire-the-content">acquires the
          content</a> of the <a>AudioBuffer</a>.
          </li>
          <li>When the dispatch of an <a>AudioProcessingEvent</a> completes, it
          <a href="#acquire-the-content">acquires the contents</a> of its
          <code>outputBuffer</code>.
          </li>
        </ul>
        <p class="note">
          This means that <code>copyToChannel</code> cannot be used to change
          the content of an <a>AudioBuffer</a> currently in use by an
          <code>AudioNode</code> that has <a href=
          "#acquire-the-content">acquired the content of an AudioBuffer</a>,
          since the <a>AudioNode</a> will continue to use the data previously
          acquired.
        </p>
      </section>
      <section>
        <h2 id="AudioBufferSourceNode">
          The AudioBufferSourceNode Interface
        </h2>
        <p>
          This interface represents an audio source from an in-memory audio
          asset in an <code>AudioBuffer</code>. It is useful for playing audio
          assets which require a high degree of scheduling flexibility, for
          instance, playing back in rhythmically-perfect ways. If
          sample-accurate playback of network- or disk-backed assets is
          required, an implementer should use <a><code>AudioWorker</code></a>
          to implement playback.
        </p>
        <p>
          The start() method is used to schedule when sound playback will
          happen. The start() method may not be issued multiple times. The
          playback will stop automatically when the buffer's audio data has
          been completely played (if the <code>loop</code> attribute is false),
          or when the stop() method has been called and the specified time has
          been reached. Please see more details in the start() and stop()
          description.
        </p>
        <pre>
  numberOfInputs  : 0
  numberOfOutputs : 1
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the AudioBuffer assigned to the .buffer attribute, or is
          one channel of silence if .buffer is NULL.
        </p>
        <dl title="interface AudioBufferSourceNode : AudioNode" class="idl">
          <dt>
            attribute AudioBuffer? buffer
          </dt>
          <dd>
            <p>
              Represents the audio asset to be played. To set the
              <code>buffer</code> attribute, execute these steps:
            </p>
            <ol>
              <li>Let <code>new buffer</code> be the <a>AudioBuffer</a> to be
              assigned to <code>buffer</code>.
              </li>
              <li>If <code>new buffer</code> is not <code>null</code> and
              <code>buffer set</code> is true, throw an
              <code>InvalidStateError</code> and abort these steps.
              </li>
              <li>If <code>new buffer</code> is not <code>null</code>, set
              <code>buffer set</code> to true.
              </li>
              <li>Assign <code>new buffer</code> to the <code>buffer</code>
              attribute.
              </li>
            </ol>
          </dd>
          <dt>
            readonly attribute AudioParam playbackRate
          </dt>
          <dd>
            <p>
              The speed at which to render the audio stream. Its default
              <code>value</code> is 1. This parameter is <a>k-rate</a>. This is
              a <a>compound parameter</a> with <code>detune</code>. Its nominal
              range is \([-\infty, \infty]\).
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam detune
          </dt>
          <dd>
            <p>
              An additional parameter, in cents, to modulate the speed at which
              is rendered the audio stream. Its default value is 0. This
              parameter is <a>k-rate</a>. This parameter is a <a>compound
              parameter</a> with <code>playbackRate</code>. Its nominal range
              is \((-\infty, \infty)\).
            </p>
          </dd>
          <dt>
            attribute boolean loop
          </dt>
          <dd>
            <p>
              Indicates if the audio data should play in a loop. The default
              value is false. If <code>loop</code> is dynamically modified
              during playback, the new value will take effect on the next
              processing block of audio.
            </p>
          </dd>
          <dt>
            attribute double loopStart
          </dt>
          <dd>
            <p>
              An optional value in seconds where looping should begin if the
              <code>loop</code> attribute is true. Its default
              <code>value</code> is 0, and it may usefully be set to any value
              between 0 and the duration of the buffer. If
              <code>loopStart</code> is less than 0, looping will begin at 0.
              If <code>loopStart</code> is greater than the duration of the
              buffer, looping will begin at the end of the buffer. This
              attribute is converted to an exact sample frame offset within the
              buffer by multiplying by the buffer's sample rate and rounding to
              the nearest integer value. Thus its behavior is independent of
              the value of the <a href=
              "#widl-AudioBufferSourceNode-playbackRate"><code>playbackRate</code></a>
              parameter.
            </p>
          </dd>
          <dt>
            attribute double loopEnd
          </dt>
          <dd>
            <p>
              An optional value in seconds where looping should end if the
              <code>loop</code> attribute is true. Its value is exclusive of
              the content of the loop: the sample frames comprising the loop
              run from the values <code>loopStart</code> to
              <code>loopEnd-(1.0/sampleRate)</code>. Its default
              <code>value</code> is 0, and it may usefully be set to any value
              between 0 and the duration of the buffer. If <code>loopEnd</code>
              is less than 0, looping will end at 0. If <code>loopEnd</code> is
              greater than the duration of the buffer, looping will end at the
              end of the buffer. This attribute is converted to an exact sample
              frame offset within the buffer by multiplying by the buffer's
              sample rate and rounding to the nearest integer value. Thus its
              behavior is independent of the value of the <a href=
              "#widl-AudioBufferSourceNode-playbackRate"><code>playbackRate</code></a>
              parameter.
            </p>
          </dd>
          <dt>
            void start()
          </dt>
          <dd>
            <p>
              Schedules a sound to playback at an exact time.
              <code>start</code> may only be called one time and <span class=
              "synchronous">must be called before <code>stop</code> is called
              or an InvalidStateError exception MUST be thrown.</span>
            </p>
            <dl class="parameters">
              <dt>
                optional double when = 0
              </dt>
              <dd>
                The <a><code>when</code></a> parameter describes at what time
                (in seconds) the sound should start playing. It is in the same
                time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute.
                If 0 is passed in for this value or if the value is less than
                <b>currentTime</b>, then the sound will start playing
                immediately. <span class="synchronous">A TypeError exception
                MUST be thrown if <code>when</code> is negative.</span>
              </dd>
              <dt>
                optional double offset = 0
              </dt>
              <dd>
                The <dfn id="dfn-offset">offset</dfn> parameter describes the
                offset time in the buffer (in seconds) where playback will
                begin. If 0 is passed in for this value, then playback will
                start from the beginning of the buffer. <span class=
                "synchronous">A TypeError exception MUST be thrown if
                <code>offset</code> is negative.</span> If <code>offset</code>
                is greater than <code>loopEnd</code>, playback will begin at
                <code>loopEnd</code> (and immediately loop to
                <code>loopStart</code>). This parameter is converted to an
                exact sample frame offset within the buffer by multiplying by
                the buffer's sample rate and rounding to the nearest integer
                value. Thus its behavior is independent of the value of the
                <a href=
                "#widl-AudioBufferSourceNode-playbackRate"><code>playbackRate</code></a>
                parameter.
              </dd>
              <dt>
                optional double duration
              </dt>
              <dd>
                The <a><code>duration</code></a> parameter describes the
                duration of the portion (in seconds) to be played. If this
                parameter is not passed, the duration will be equal to the
                total duration of the AudioBuffer minus the <code>offset</code>
                parameter. Thus if neither <code>offset</code> nor
                <code>duration</code> are specified then the implied duration
                is the total duration of the AudioBuffer. <span class=
                "synchronous">An TypeError exception MUST be thrown if
                <code>duration</code> is negative.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            void stop()
          </dt>
          <dd>
            <p>
              Schedules a sound to stop playback at an exact time. If
              <code>stop</code> is called again after already having been
              called, the last invocation will be the only one applied; stop
              times set by previous calls will not be applied, unless the
              buffer has already stopped prior to any subsequent calls. If the
              buffer has already stopped, further calls to <code>stop</code>
              will have no effect. If a stop time is reached prior to the
              scheduled start time, the sound will not play.
            </p>
            <dl class="parameters">
              <dt>
                optional double when = 0
              </dt>
              <dd>
                The <a><code>when</code></a> parameter describes at what time
                (in seconds) the sound should stop playing. It is in the same
                time coordinate system as the
                <a><code>AudioContext</code></a>'s <a href=
                "#widl-BaseAudioContext-currentTime">currentTime</a> attribute.
                If 0 is passed in for this value or if the value is less than
                <a href=
                "#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>,
                then the sound will stop playing immediately. <span class=
                "synchronous">A TypeError exception MUST be thrown if
                <code>when</code> is negative</span>.
              </dd>
            </dl>
          </dd>
          <dt>
            attribute EventHandler onended
          </dt>
          <dd>
            <p>
              A property used to set the <code>EventHandler</code> (described
              in <cite><a href=
              "https://html.spec.whatwg.org/multipage/webappapis.html#eventhandler">
              HTML</a></cite>[[!HTML]]) for the ended event that is dispatched
              to <a><code>AudioBufferSourceNode</code></a> node types. When the
              playback of the buffer for an
              <a><code>AudioBufferSourceNode</code></a> is finished, an event
              of type <code>Event</code> (described in <cite><a href=
              "https://html.spec.whatwg.org/multipage/infrastructure.html#event">
              HTML</a></cite> [[!HTML]]) will be dispatched to the event
              handler.
            </p>
          </dd>
        </dl>
        <p>
          Both <code>playbackRate</code> and <code>detune</code> are
          <a>k-rate</a> parameters that form a <a>compound parameter</a> and
          are used together to determine a <em>computedPlaybackRate</em> value:
        </p>
        <pre>
  computedPlaybackRate(t) = playbackRate(t) * pow(2, detune(t) / 1200)
        </pre>
        <p>
          The <code>computedPlaybackRate</code> is the effective speed at which
          the <a><code>AudioBuffer</code></a> of this
          <a><code>AudioBufferSourceNode</code></a> MUST be played. Its nominal
          range is [-100, 100].
        </p>
        <p>
          This MUST be implemented by <em>resampling</em> the input data using
          a resampling ratio of 1 / <code>computedPlaybackRate</code>, hence
          changing both the pitch and speed of the audio.
        </p>
        <section>
          <h3 id="looping-AudioBufferSourceNode">
            Looping
          </h3>
          <p>
            If the <code>loop</code> attribute is true when
            <code>start()</code> is called, then playback will continue
            indefinitely until <code>stop()</code> is called and the stop time
            is reached. We'll call this "loop" mode. Playback always starts at
            the point in the buffer indicated by the <code>offset</code>
            argument of <code>start()</code>, and in <em>loop</em> mode will
            continue playing until it reaches the <em>actualLoopEnd</em>
            position in the buffer (or the end of the buffer), at which point
            it will wrap back around to the <em>actualLoopStart</em> position
            in the buffer, and continue playing according to this pattern.
          </p>
          <p>
            In <em>loop</em> mode then the <em>actual</em> loop points are
            calculated as follows from the <code>loopStart</code> and
            <code>loopEnd</code> attributes:
          </p>
          <blockquote>
            <pre>
if ((loopStart || loopEnd) &amp;& loopStart &gt;= 0 &amp;& loopEnd &gt; 0 &amp;& loopStart &lt; loopEnd) {
    actualLoopStart = loopStart;
    actualLoopEnd = min(loopEnd, buffer.duration);
} else {
    actualLoopStart = 0;
    actualLoopEnd = buffer.duration;
}
</pre>
          </blockquote>
          <p>
            Note that the default <code>value</code>s for
            <code>loopStart</code> and <code>loopEnd</code> are both 0, which
            indicates that looping should occur from the very start to the very
            end of the buffer.
          </p>
          <p>
            Please note that as a low-level implementation detail, the
            AudioBuffer is at a specific sample-rate (usually the same as the
            <a><code>AudioContext</code></a> sample-rate), and that the loop
            times (in seconds) must be converted to the appropriate
            sample-frame positions in the buffer according to this sample-rate.
          </p>
          <p>
            When scheduling the beginning and the end of playback using the
            <code>start()</code> and <code>stop()</code> methods, the resulting
            start or stop time MUST be rounded to the nearest sample-frame in
            the sample rate of the <a><code>AudioContext</code></a>. That is,
            no sub-sample scheduling is possible.
          </p>
        </section>
      </section>
      <section>
        <h2 id="MediaElementAudioSourceNode">
          The MediaElementAudioSourceNode Interface
        </h2>
        <p>
          This interface represents an audio source from an <code>audio</code>
          or <code>video</code> element.
        </p>
        <pre>
  numberOfInputs  : 0
  numberOfOutputs : 1
</pre>
        <p>
          The number of channels of the output corresponds to the number of
          channels of the media referenced by the
          <code>HTMLMediaElement</code>. Thus, changes to the media element's
          .src attribute can change the number of channels output by this node.
          If the .src attribute is not set, then the number of channels output
          will be one silent channel.
        </p>
        <dl title="interface MediaElementAudioSourceNode : AudioNode" class=
        "idl"></dl>
        <p>
          A <a>MediaElementAudioSourceNode</a> is created given an
          <code>HTMLMediaElement</code> using the <a>AudioContext</a>
          <code>createMediaElementSource()</code> method.
        </p>
        <p>
          The number of channels of the single output equals the number of
          channels of the audio referenced by the <code>HTMLMediaElement</code>
          passed in as the argument to <code>createMediaElementSource()</code>,
          or is 1 if the <code>HTMLMediaElement</code> has no audio.
        </p>
        <p>
          The <code>HTMLMediaElement</code> must behave in an identical fashion
          after the <a>MediaElementAudioSourceNode</a> has been created,
          <em>except</em> that the rendered audio will no longer be heard
          directly, but instead will be heard as a consequence of the
          <a>MediaElementAudioSourceNode</a> being connected through the
          routing graph. Thus pausing, seeking, volume, <code>src</code>
          attribute changes, and other aspects of the
          <code>HTMLMediaElement</code> must behave as they normally would if
          <em>not</em> used with a <a>MediaElementAudioSourceNode</a>.
        </p>
        <pre class="example">
  var mediaElement = document.getElementById('mediaElementID');
  var sourceNode = context.createMediaElementSource(mediaElement);
  sourceNode.connect(filterNode);
</pre>
        <section>
          <h2>
            Security with MediaElementAudioSourceNode and cross-origin
            resources
          </h2>
          <p>
            <code>HTMLMediaElement</code> allows the playback of cross-origin
            resources. Because Web Audio allows inspection of the content of
            the resource (e.g. using a <a>MediaElementAudioSourceNode</a>, and
            a <a>ScriptProcessorNode</a> to read the samples), information
            leakage can occur if scripts from one <a href=
            "http://www.w3.org/html/wg/drafts/html/master/browsers.html#origin">
            origin</a> inspect the content of a resource from another <a href=
            "http://www.w3.org/html/wg/drafts/html/master/browsers.html#origin">
            origin</a>.
          </p>
          <p>
            To prevent this, a <a>MediaElementAudioSourceNode</a> MUST output
            <em>silence</em> instead of the normal output of the
            <code>HTMLMediaElement</code> if it has been created using an
            <code>HTMLMediaElement</code> for which the execution of the
            <a href="https://fetch.spec.whatwg.org/#fetching">fetch
            algorithm</a> labeled the resource as <a href=
            "http://www.w3.org/html/wg/drafts/html/master/infrastructure.html#cors-cross-origin">
            CORS-cross-origin</a>.
          </p>
        </section>
      </section>
      <section>
        <h2 id="AudioWorker">
          The <dfn>AudioWorker</dfn> interface
        </h2>
        <p>
          An AudioWorker object is the main-thread representation of a worker
          "thread" that supports processing of audio in Javascript. This
          AudioWorker object is a factory that is used to create multiple audio
          nodes of the same type; this enables easy sharing of code, program
          data and global state across nodes. An AudioWorker can then be used
          to create instances of <a>AudioWorkerNode</a>, which is the
          main-thread representation of an individual node processed by that
          AudioWorker.
        </p>
        <p>
          These <a>control thread</a> objects cause the instantiation of a
          processing context in the <a>rendering thread</a>. All audio
          processing by AudioWorkerNodes run in the audio processing thread.
          This has a few side effects that bear mentioning: blocking the audio
          worker's thread can cause glitches in the audio, and if the audio
          thread is normally elevated in thread priority (to reduce glitching
          possibility), it must be demoted to normal thread priority (in order
          to avoid escalating thread priority of user-supplied script code).
        </p>
        <p>
          From inside an audio worker script, the Audio Worker factory is
          represented by an <a><code>AudioWorkerGlobalScope</code></a> object
          representing the node's contextual information, and individual audio
          nodes created by the factory are represented by
          <a><code>AudioWorkerNodeProcessor</code></a> objects.
        </p>
        <p>
          In addition, all <a>AudioWorkerNode</a>s that are created by the same
          <a>AudioWorker</a> share an <a>AudioWorkerGlobalScope</a>; this can
          allow them to share context and data across nodes (for example,
          loading a single instance of a shared database used by the individual
          nodes, or sharing context in order to implement oscillator
          synchronization).
        </p>
        <dl title="interface AudioWorker : Worker" class="idl">
          <dt>
            void terminate()
          </dt>
          <dd>
            <p>
              The terminate() method, when invoked, must cause the cessation of
              any <a>AudioProcessEvent</a>s being dispatched inside the
              AudioWorker's associated <a>AudioWorkerGlobalScope</a>. It will
              also cause all associated AudioWorkerNodes to cease processing,
              and will cause the destruction of the worker's context. In
              practical terms, this means all nodes created from this
              AudioWorker will disconnect themselves, and will cease performing
              any useful functions.
            </p>
          </dd>
          <dt>
            void postMessage(any message, optional sequence&lt;Transferable&gt;
            transfer)
          </dt>
          <dd>
            <p>
              postMessage may be called to send a message to the
              <a>AudioWorkerGlobalScope</a>, similar to the algorithm defined
              by [[!Workers]].
            </p>
          </dd>
          <dt>
            readonly attribute AudioWorkerParamDescriptor[] parameters
          </dt>
          <dd>
            <p>
              This array contains descriptors for each of the current
              parameters on nodes created by this AudioWorker. This enables
              users of the AudioWorker to easily iterate over the AudioParam
              names and default values.
            </p>
          </dd>
          <dt>
            attribute EventHandler onmessage
          </dt>
          <dd>
            <p>
              The onmessage handler is called whenever the
              <a>AudioWorkerGlobalScope</a> posts a message back to the main
              thread.
            </p>
          </dd>
          <dt>
            attribute EventHandler onloaded
          </dt>
          <dd>
            <p>
              The onloaded handler is called after the script is successfully
              loaded and its global scope code is run to initialize the
              <a>AudioWorkerGlobalScope</a>.
            </p>
          </dd>
          <dt>
            AudioWorkerNode createNode(int numberOfInputs, int numberOfOutputs)
          </dt>
          <dd>
            <p>
              Creates a node instance in the audio worker.
            </p>
          </dd>
          <dt>
            AudioParam addParameter(DOMString name, float defaultValue)
          </dt>
          <dd>
            <p>
              Causes a correspondingly-named read-only <a>AudioParam</a> to be
              present on any <a>AudioWorkerNode</a>s created (previously or
              subsequently) by this <a>AudioWorker</a>, and a
              correspondingly-named read-only <a>Float32Array</a> to be present
              on the <a><code>parameters</code></a> object exposed on the
              <a>AudioProcessEvent</a> on subsequent audio processing events
              for such nodes. The AudioParam may immediately have its
              scheduling methods called, its .<a>value</a> set, or
              <a>AudioNode</a>s connected to it.
            </p>
            <p>
              The <code>name</code> parameter is the name used for the
              read-only AudioParam added to the AudioWorkerNode, and the name
              used for the read-only <code>Float32Array</code> that will be
              present on the <a><code>parameters</code></a> object exposed on
              subsequent <a>AudioProcessEvent</a>s.
            </p>
            <p>
              The <a><code>defaultValue</code></a> parameter is the default
              value for the <a>AudioParam</a>'s <a>value</a> attribute, as well
              as therefore the default value that will appear in the
              Float32Array in the worker script (if no other parameter changes
              or connections affect the value).
            </p>
          </dd>
          <dt>
            void removeParameter(DOMString name)
          </dt>
          <dd>
            <p>
              Removes a previously-added parameter named <code>name</code> from
              all <a>AudioWorkerNode</a>s associated with this
              <a>AudioWorker</a> and its <a>AudioWorkerGlobalScope</a>. This
              will also remove the correspondingly-named read-only
              <a>AudioParam</a> from the <a>AudioWorkerNode</a>, and will
              remove the correspondingly-named read-only <a>Float32Array</a>s
              from the <a>AudioProcessEvent</a>'s
              <a><code>parameters</code></a> member on subsequent audio
              processing events. <span class="synchronous">A NotFoundError
              exception must be thrown if no parameter with that name exists on
              this <a>AudioWorker</a></span>.
            </p>
            <p>
              The <code>name</code> parameter identifies the parameter to be
              removed.
            </p>
          </dd>
        </dl>
        <p>
          Note that <a>AudioWorkerNode</a> objects will also have read-only
          AudioParam objects for each named parameter added via the
          <code>addParameter</code> method. As this is dynamic, it cannot be
          captured in IDL.
        </p>
        <p>
          As the <a>AudioWorker</a> interface inherits from
          <code>Worker</code>, <a>AudioWorker</a>s must implement the
          <code>Worker</code> interface for communication with the audio worker
          script.
        </p>
        <section>
          <h2 id="AudioWorkerNode">
            The <dfn>AudioWorkerNode</dfn> Interface
          </h2>
          <p>
            This interface represents an <a><code>AudioNode</code></a> which
            interacts with a <code>Worker</code> thread to generate, process,
            or analyse audio directly. The user creates a separate audio
            processing worker script, which is hosted inside the
            AudioWorkerGlobalScope and runs inside the audio processing thread,
            rather than the main UI thread. The AudioWorkerNode represents the
            processing node in the main processing thread's node graph; the
            AudioWorkerGlobalScope represents the context in which the user's
            audio processing script is run.
          </p>
          <p>
            Nota bene that if the Web Audio implementation normally runs audio
            process at higher than normal thread priority, utilizing
            AudioWorkerNodes may cause demotion of the priority of the audio
            thread (since user scripts cannot be run with higher than normal
            priority).
          </p>
          <pre>
    numberOfInputs  : variable
    numberOfOutputs : variable

    channelCount = numberOfInputChannels;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
          <p>
            The number of input and output channels specified in the
            createAudioWorkerNode() call determines the initial number of input
            and output channels (and the number of channels present for each
            input and output in the AudioBuffers passed to the AudioProcess
            event handler inside the <a>AudioWorkerGlobalScope</a>). It is
            invalid for both <!--a href=
            "#AudioWorker-numberOfInputChannels"--><a><code>numberOfInputChannels</code></a>
and <!--a href=
            "#AudioWorker-numberOfOutputChannels"--><a><code>numberOfOutputChannels</code></a>
to be zero.
          </p>
          <p>
            Example usage:
          </p>
          <pre>
    var bitcrusherFactory = context.createAudioWorker( "bitcrusher.js" );
    var bitcrusherNode = bitcrusherFactory.createNode();
</pre>
          <dl title="interface AudioWorkerNode : AudioNode" class="idl">
            <dt>
              void postMessage(any message, optional
              sequence&lt;Transferable&gt; transfer)
            </dt>
            <dd>
              <p>
                postMessage may be called to send a message to the
                AudioWorkerNodeProcessor, via the algorithm defined by <a href=
                "https://w3c.github.io/workers/#dom-worker-postmessage">the
                Worker specification</a>. Note that this is different from
                calling postMessage() on the AudioWorker itself, as that would
                affect the AudioWorkerGlobalScope.
              </p>
            </dd>
            <dt>
              attribute EventHandler onmessage
            </dt>
            <dd>
              <p>
                The onmessage handler is called whenever the
                AudioWorkerNodeProcessor posts a node message back to the main
                thread.
              </p>
            </dd>
          </dl>
          <p>
            Note that <a>AudioWorkerNode</a> objects will also have read-only
            AudioParam objects for each named parameter added via the
            <code>addParameter</code> method on the AudioWorker. As this is
            dynamic, it cannot be captured here in IDL.
          </p>
        </section>
        <section>
          <h2 id="AudioWorkerParamDescriptor">
            The <dfn>AudioWorkerParamDescriptor</dfn> Interface
          </h2>
          <p>
            This interface represents the description of an AudioWorkerNode
            AudioParam - in short, its name and default value. This enables
            easy iteration over the AudioParams from an AudioWorkerGlobalScope
            (which does not have an instance of those AudioParams).
          </p>
          <dl title="interface AudioWorkerParamDescriptor" class="idl">
            <dt>
              readonly attribute DOMString name
            </dt>
            <dd>
              <p>
                The name of the AudioParam.
              </p>
            </dd>
            <dt>
              readonly attribute float defaultValue
            </dt>
            <dd>
              <p>
                The default value of the AudioParam.
              </p>
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            The AudioWorkerGlobalScope Interface
          </h2>
          <p>
            This interface is a <code>DedicatedWorkerGlobalScope</code>-derived
            object representing the context in which an audio processing script
            is run; it is designed to enable the generation, processing, and
            analysis of audio data directly using JavaScript in a Worker
            thread, with shared context between multiple instances of audio
            nodes. This facilitates nodes that may have substantial shared
            data, e.g. a convolution node.
          </p>-
          <p>
            The <a><code>AudioWorkerGlobalScope</code></a> handles - <dfn id=
            "audioprocess-worker">audioprocess</dfn> events dispatched -
            synchronously to process audio frame blocks for nodes created by
            this worker. - <a href=
            "#audioprocess-worker"><code>audioprocess</code></a> events are
            only - dispatched for nodes that have at least one input - or one
            output connected. TODO: should this be true?
          </p>
          <dl title=
          "interface AudioWorkerGlobalScope : DedicatedWorkerGlobalScope"
          class="idl">
            <dt>
              readonly attribute float sampleRate
            </dt>
            <dd>
              <p>
                The sample rate of the host <a>AudioContext</a> (since inside
                the <code>Worker</code> scope, the user will not have direct
                access to the <a>AudioContext</a>.
              </p>
            </dd>
            <dt>
              AudioParam addParameter(DOMString name, float defaultValue)
            </dt>
            <dd>
              <p>
                Causes a correspondingly-named read-only <a>AudioParam</a> to
                be present on previously-created and subsequently-created
                <a>AudioWorkerNode</a>s created by this factory, and a
                correspondingly-named read-only <a>Float32Array</a> to be
                present on the <a><code>parameters</code></a> object exposed on
                the <a>AudioProcessEvent</a> on subsequent audio processing
                events for nodes created from this factory.
              </p>
              <p>
                It is purposeful that AudioParams can be added (or removed)
                from an Audio Worker from either the control thread or the
                worker script; this enables immediate creation of worker-based
                nodes and their prototypes, but also enables packaging an
                entire worker including its AudioParam configuration into a
                single script. It is recommended that nodes be used only after
                the AudioWorkerNode's oninitialized has been called, in order
                to allow the worker script to configure the node.
              </p>
              <p>
                The <code>name</code> parameter is the name used for the
                read-only AudioParam added to the AudioWorkerNode, and the name
                used for the read-only <code>Float32Array</code> that will be
                present on the <a><code>parameters</code></a> object exposed on
                subsequent <a>AudioProcessEvent</a>s.
              </p>
              <p>
                The <dfn id="dfn-defaultValue">defaultValue</dfn> parameter is
                the default value for the <a>AudioParam</a>'s <a>value</a>
                attribute, as well as therefore the default value that will
                appear in the Float32Array in the worker script (if no other
                parameter changes or connections affect the value).
              </p>
            </dd>
            <dt>
              void removeParameter(DOMString name)
            </dt>
            <dd>
              <p>
                Removes a previously-added parameter named <code>name</code>
                from nodes processed by this factory. This will also remove the
                correspondingly-named read-only <a>AudioParam</a> from the
                <a>AudioWorkerNode</a>, and will remove the
                correspondingly-named read-only <a>Float32Array</a> from the
                <a>AudioProcessEvent</a>'s <a><code>parameters</code></a>
                member on subsequent audio processing events. <span class=
                "synchronous">A NotFoundError exception MUST be thrown if no
                parameter with that name exists on this node.</span>
              </p>
              <p>
                The <code>name</code> parameter identifies the parameter to be
                removed.
              </p>
            </dd>
            <dt>
              attribute EventHandler onaudioprocess
            </dt>
            <dd>
              <p>
                A property used to set the <code>EventHandler</code> (described
                in [[!HTML]]) for the <a href=
                "#audioprocess-worker"><code>audioprocess</code></a> event that
                is dispatched to <a><code>AudioWorkerGlobalScope</code></a> to
                process audio while the associated nodes are connected (to at
                least one input or output). An event of type
                <a><code>AudioProcessEvent</code></a> will be dispatched to the
                event handler.
              </p>
            </dd>
            <dt>
              attribute EventHandler onnodecreate
            </dt>
            <dd>
              <p>
                A property used to set the <code>EventHandler</code> (described
                in [[!HTML]]) for the <a href=
                "#audioprocess-worker"><code>nodecreate</code></a> event that
                is dispatched to <a><code>AudioWorkerGlobalScope</code></a>
                when a new <a><code>AudioWorkerNode</code></a> has been
                created. This enables the scope to do node-level initialization
                of the <a>AudioNodeProcessor</a> object. An event of type
                <a><code>AudioWorkerNodeCreationEvent</code></a> will be
                dispatched to the event handler.
              </p>
            </dd>
            <dt>
              readonly attribute AudioWorkerParamDescriptor[] parameters
            </dt>
            <dd>
              <p>
                This array contains descriptors for each of the current
                parameters on nodes created in this AudioWorkerGlobalScope.
                This enables audio worker implementations to easily iterate
                over the AudioParam names and default values.
              </p>
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            The AudioWorkerNodeProcessor Interface
          </h2>
          <p>
            An object supporting this interface represents each individual node
            instantiated in an <a><code>AudioWorkerGlobalScope</code></a>; it
            is designed to manage the data for an individual node. Shared
            context between multiple instances of audio nodes is accessible
            from the <a>AudioWorkerGlobalScope</a>; this object represents the
            individual node and can be used for data storage or main-thread
            communication.
          </p>
          <dl title="interface AudioWorkerNodeProcessor : EventTarget" class=
          "idl">
            <dt>
              void postMessage(any message, optional
              sequence&lt;Transferable&gt; transfer)
            </dt>
            <dd>
              <p>
                postMessage may be called to send a message to the
                AudioWorkerNode, via the algorithm defined by <a href=
                "https://w3c.github.io/workers/#dom-worker-postmessage">the
                Worker specification</a>. Note that this is different from
                calling postMessage() on the AudioWorker itself, as that would
                dispatch to the AudioWorkerGlobalScope.
              </p>
            </dd>
            <dt>
              attribute EventHandler onmessage
            </dt>
            <dd>
              <p>
                The onmessage handler is called whenever the AudioWorkerNode
                posts a node message back to the audio thread.
              </p>
            </dd>
          </dl>
        </section>
        <section class="informative">
          <h3>
            Audio Worker Examples
          </h3>
          <section>
            <h4>
              A Bitcrusher Node
            </h4>
            <p>
              Bitcrushing is a mechanism by which the audio quality of an audio
              stream is reduced - both by quantizing the value (simulating
              lower bit-depth in integer-based audio), and by quantizing in
              time (simulating a lower digital sample rate). This example shows
              how to use AudioParams (in this case, treated as a-rate) inside
              an AudioWorker.
            </p>
            <h5>
              Main file javascript
            </h5>
            <pre>
var bitcrusherFactory = null;
audioContext.createAudioWorker("bitcrusher_worker.js").then(function (factory) {
  // cache 'factory' in case you want to create more nodes!
  bitcrusherFactory = factory;
  var bitcrusherNode = factory.createNode();
  bitcrusherNode.bits.setValueAtTime(8, 0);
  bitcrusherNode.connect(output);
  input.connect(bitcrusherNode);
});
</pre>
            <h5>
              bitcrusher_worker.js
            </h5>
            <pre>
// Custom parameter - number of bits to crush down to - default 8
this.addParameter("bits", 8);

// Custom parameter - frequency reduction, 0-1, default 0.5
this.addParameter("frequencyReduction", 0.5);

onnodecreate = function (e) {
  e.node.phaser = 0;
  e.node.lastDataValue = 0;
}

onaudioprocess = function (e) {
  for (var channel = 0; channel &lt; e.inputs[0].length; channel++) {
    var inputBuffer = e.inputs[0][channel];
    var outputBuffer = e.outputs[0][channel];
    var bufferLength = inputBuffer.length;
    var bitsArray = e.parameters.bits;
    var frequencyReductionArray = e.parameters.frequencyReduction;

    for (var i = 0; i &lt; bufferLength; i++) {
      var bits = bitsArray ? bitsArray[i] : 8;
      var frequencyReduction = frequencyReductionArray ?
        frequencyReductionArray[i] : 0.5;

      var step = Math.pow(1 / 2, bits);
      e.node.phaser += frequencyReduction;
      if (e.node.phaser &gt;= 1.0) {
        e.node.phaser -= 1.0;
        e.node.lastDataValue = step * Math.floor(inputBuffer[i] / step + 0.5);
      }
      outputBuffer[i] = e.node.lastDataValue;
    }
  }
};
</pre>
          </section>
          <section>
            <h4>
              TODO: fix up this example. A Volume Meter and Clip Detector
            </h4>
            <p>
              Another common need is a clip-detecting volume meter. This
              example shows how to communicate basic parameters (that do not
              need AudioParam scheduling) across to a Worker, as well as
              communicating data back to the control thread. This node does not
              use any output.
            </p>
            <h5>
              Main file javascript
            </h5>
            <pre>
function setupNodeMessaging(node) {
  // This handles communication back from the volume meter
  node.onmessage = function (event) {
    if (event.data instanceof Object) {
      if (event.data.hasOwnProperty("clip"))
        this.clip = event.data.clip;
      if (event.data.hasOwnProperty("volume"))
        this.volume = event.data.volume;
    }
  }

  // Set up some default configuration parameters
  node.postMessage({
    "smoothing": 0.9, // Smoothing parameter
    "clipLevel": 0.9, // Level to consider "clipping"
    "clipLag": 750, // How long to keep "clipping" lit up after clip (ms)
    "updating": 100 // How frequently to update volume and clip param (ms)
  });

  // Set up volume and clip attributes.  These will be updated by our onmessage.
  node.volume = 0;
  node.clip = false;
}

var vuNode = null;

audioContext.createAudioWorker("vu_meter_worker.js").then(function (factory) {
  // cache 'factory' in case you want to create more nodes!
  vuFactory = factory;
  // we don't need an output, and let's force to mono
  vuNode = factory.createNode([1], []);
  setupNodeMessaging(vuNode);
});

window.requestAnimationFrame(function (timestamp) {
  if (vuNode) {
    // Draw a bar based on vuNode.volume and vuNode.clip
  }
});

</pre>
            <h5>
              vu_meter_worker.js
            </h5>
            <pre>
// Custom parameter - number of bits to crush down to - default 8
this.addParameter("bits", 8);

// Custom parameter - frequency reduction, 0-1, default 0.5
this.addParameter("frequencyReduction", 0.5);

onnodecreate = function (e) {
  e.node.timeToNextUpdate = 0.1 * sampleRate;
  e.node.smoothing = 0.5;
  e.node.clipLevel = 0.95;
  e.node.clipLag = 1;
  e.node.updatingInterval = 150;
  // This just handles setting attribute values
  e.node.onmessage = function (event) {
    if (event.data instanceof Object) {
      if (event.data.hasOwnProperty("smoothing"))
        this.smoothing = event.data.smoothing;
      if (event.data.hasOwnProperty("clipLevel"))
        this.clipLevel = event.data.clipLevel;
      if (event.data.hasOwnProperty("clipLag"))
        this.clipLag = event.data.clipLag / 1000; // convert to seconds
      if (event.data.hasOwnProperty("updating")) // convert to samples
        this.updatingInterval = event.data.updating * sampleRate / 1000;
    }
  };
}

onaudioprocess = function (event) {
  var buf = event.inputs[0][0]; // Node forces mono
  var bufLength = buf.length;
  var sum = 0;
  var x;

  // Do a root-mean-square on the samples: sum up the squares...
  for (var i = 0; i &lt; bufLength; i++) {
    x = buf[i];
    if (Math.abs(x) &gt;= event.node.clipLevel) {
      event.node.clipping = true;
      event.node.unsentClip = true; // Make sure, for every clip, we send a message.
      event.node.lastClip = event.playbackTime + (i / sampleRate);
    }
    sum += x * x;
  }

  // ... then take the square root of the sum.
  var rms = Math.sqrt(sum / bufLength);

  // Now smooth this out with the smoothing factor applied
  // to the previous sample - take the max here because we
  // want "fast attack, slow release."
  event.node.volume = Math.max(rms, event.node.volume * event.node.smoothing);
  if (event.node.clipping &amp;& (!event.node.unsentClip) &amp;& (event.playbackTime
      &gt; (this.lastClip + clipLag)))
    event.node.clipping = false;

  // How long has it been since our last update?
  event.node.timeToNextUpdate -= event.node.last;
  if (event.node.timeToNextUpdate & lt; 0) {
    event.node.timeToNextUpdate = event.node.updatingInterval;
    event.node.postMessage({
      "volume": event.node.volume,
      "clip": event.node.clipping
    });
    event.node.unsentClip = false;
  }
};
</pre>
          </section>
          <section>
            <h4>
              Reimplementing ChannelMerger
            </h4>
            <p>
              This worker shows how to merge inputs into a single output
              channel.
            </p>
            <h5>
              Main file javascript
            </h5>
            <pre>
var mergerNode = audioContext.createAudioWorker("merger_worker.js", [1, 1, 1, 1, 1, 1], [6]);
</pre>
            <pre>
var mergerFactory = null;

audioContext.createAudioWorker("merger_worker.js").then(function (factory) {
  // cache 'factory' in case you want to create more nodes!
  mergerFactory = factory;
  var merger6channelNode = factory.createNode([1, 1, 1, 1, 1, 1], [6]);
  // connect inputs and outputs here
});
</pre>
            <h5>
              merger_worker.js
            </h5>
            <pre>
onaudioprocess= function (e) {
  for (var input=0; input&lt;e,node.inputs.length; input++)
    e.node.outputs[0][input].set(e.node.inputs[input][0]);
};
</pre>
          </section>
        </section>
      </section>
      <section class="informative">
        <h2>
          The ScriptProcessorNode Interface - DEPRECATED
        </h2>
        <p>
          This interface is an <a><code>AudioNode</code></a> which can
          generate, process, or analyse audio directly using JavaScript. This
          node type is deprecated, to be replaced by the
          <a>AudioWorkerNode</a>; this text is only here for informative
          purposes until implementations remove this node type.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = numberOfInputChannels;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
        <p>
          The <code>channelCountMode</code> cannot be changed from "explicit"
          and the <code>channelCount</code> cannot be changed. <span class=
          "synchronous">An attempt to change either of these MUST throw an
          InvalidStateError exception</span>.
        </p>
        <p>
          The <a><code>ScriptProcessorNode</code></a> is constructed with a
          <dfn>bufferSize</dfn> which must be one of the following values: 256,
          512, 1024, 2048, 4096, 8192, 16384. This value controls how
          frequently the 
          <!--a href="#audioprocess-spnode"--><a>audioprocess</a> event is
          dispatched and how many sample-frames need to be processed each call.
          <!--a href="#audioprocess-spnode"--><a><code>audioprocess</code></a>
          events are only dispatched if the
          <a><code>ScriptProcessorNode</code></a> has at least one input or one
          output connected. Lower numbers for <a href=
          "#widl-ScriptProcessorNode-bufferSize">bufferSize</a> will result in
          a lower (better) <a href="#latency">latency</a>. Higher numbers will
          be necessary to avoid audio breakup and <a href=
          "#audio-glitching">glitches</a>. This value will be picked by the
          implementation if the bufferSize argument to
          <code>createScriptProcessor</code> is not passed in, or is set to 0.
        </p>
        <p>
          <dfn>numberOfInputChannels</dfn> and
          <dfn>numberOfOutputChannels</dfn> determine the number of input and
          output channels. It is invalid for both
          <code>numberOfInputChannels</code> and
          <code>numberOfOutputChannels</code> to be zero.
        </p>
        <pre>
var node = context.createScriptProcessor(bufferSize, numberOfInputChannels,
  numberOfOutputChannels);
</pre>
        <dl title="interface ScriptProcessorNode : AudioNode" class="idl">
          <dt>
            attribute EventHandler onaudioprocess
          </dt>
          <dd>
            <p>
              A property used to set the <code>EventHandler</code> (described
              in <cite><a href=
              "https://html.spec.whatwg.org/multipage/webappapis.html#eventhandler">
              HTML</a></cite>[[!HTML]]) for the <!--a href=
            "#audioprocess-spnode"--><a><code>audioprocess</code></a> event
that is dispatched to <a><code>ScriptProcessorNode</code></a> node types. An
event of type <a><code>AudioProcessingEvent</code></a> will be dispatched to
the event handler.
            </p>
          </dd>
          <dt>
            readonly attribute long bufferSize
          </dt>
          <dd>
            <p>
              The size of the buffer (in sample-frames) which needs to be
              processed each time <code>onaudioprocess</code> is called. Legal
              values are (256, 512, 1024, 2048, 4096, 8192, 16384).
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The AudioWorkerNodeCreationEvent Interface
        </h2>
        <p>
          This is an <code>Event</code> object which is dispatched to
          <a><code>AudioWorkerGlobalScope</code></a> objects when a new node
          instance is created. This allows AudioWorkers to initialize any
          node-local data (e.g. allocating a delay or initializing local
          variables).
        </p>
        <dl title="interface AudioWorkerNodeCreationEvent : Event" class="idl">
          <dt>
            readonly attribute AudioWorkerNodeProcessor node
          </dt>
          <dd>
            <p>
              The new node being created. Any node-local data storage (e.g.,
              the buffer for a delay node) should be created on this object.
            </p>
          </dd>
          <dt>
            readonly attribute Array inputs
          </dt>
          <dd>
            <p>
              An array of channelCounts for the inputs.
            </p>
          </dd>
          <dt>
            readonly attribute Array outputs
          </dt>
          <dd>
            <p>
              An array of channelCounts for the outputs.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The AudioProcessEvent Interface
        </h2>
        <p>
          This is an <code>Event</code> object which is dispatched to
          <a><code>AudioWorkerGlobalScope</code></a> objects to perform
          processing.
        </p>
        <p>
          The event handler processes audio from the input (if any) by
          accessing the audio data from the <code>inputBuffers</code>
          attribute. The audio data which is the result of the processing (or
          the synthesized data if there are no inputs) is then placed into the
          <code>outputBuffers</code>.
        </p>
        <dl title="interface AudioProcessEvent : Event" class="idl">
          <dt>
            readonly attribute double playbackTime
          </dt>
          <dd>
            <p>
              The starting time of the block of audio being processed in
              response to this event. By definition this will be equal to the
              value of <a><code>BaseAudioContext</code></a>'s <a href=
              "#widl-BaseAudioContext-currentTime">currentTime</a> attribute
              that was most recently observable in the control thread.
            </p>
          </dd>
          <dt>
            readonly attribute AudioWorkerNodeProcessor node
          </dt>
          <dd>
            <p>
              The node to which this processing event is being dispatched. Any
              node-local data storage (e.g., the buffer for a delay node)
              should be maintained on this object.
            </p>
          </dd>
          <dt>
            readonly attribute Float32Array[][] inputs
          </dt>
          <dd>
            <p>
              A readonly Array of Arrays of Float32Arrays. The top-level Array
              is organized by input; each input may contain multiple channels;
              each channel contains a Float32Array of sample data. The initial
              size of the channel array will be determined by the number of
              channels specified for that input in the createAudioWorkerNode()
              method. However, an onprocess handler may alter this number of
              channels in the input dynamically, either by adding a
              Float32Array of blocksize length (128) or by reducing the Array
              (by reducing the Array.length or by using Array.pop() or
              Array.slice(). The event object, the Array and the Float32Arrays
              will be reused by the processing system, in order to minimize
              memory churn.
            </p>
            <p>
              Any reordering performed on the Array for an input will not
              reorganize the connections to the channels for subsequent events.
            </p>
          </dd>
          <dt>
            readonly attribute Float32Array[][] outputs
          </dt>
          <dd>
            <p>
              A readonly Array of Arrays of Float32Arrays. The top-level Array
              is organized by output; each output may contain multiple
              channels; each channel contains a Float32Array of sample data.
              The initial size of the channel array will be determined by the
              number of channels specified for that output in the
              createAudioWorkerNode() method. However, an onprocess handler may
              alter this number of channels in the output dynamically, either
              by adding a Float32Array of blocksize length (128) or by reducing
              the Array (by reducing the Array.length or by using Array.pop()
              or Array.slice(). The event object, the Array and the
              Float32Arrays will be reused by the processing system, in order
              to minimize memory churn.
            </p>
            <p>
              Any reordering performed on the Array for an output will not
              reorganize the connections to the channels for subsequent events.
            </p>
          </dd>
          <dt>
            readonly attribute object parameters
          </dt>
          <dd>
            <p>
              This object attribute exposes a correspondingly-named read-only
              <a>Float32Array</a> for each parameter that has been added via
              <a>addParameter</a>. As this is dynamic, this cannot be captured
              in IDL. The length of this Float32Array will correspond to the
              length of the inputBuffer. The contents of this Float32Array will
              be the values to be used for the AudioParam at the corresponding
              points in time. It is expected that this Float32Array will be
              reused by the audio engine.
            </p>
          </dd>
        </dl>
      </section>
      <section class="informative">
        <h2>
          The AudioProcessingEvent Interface - DEPRECATED
        </h2>
        <p>
          This is an <code>Event</code> object which is dispatched to
          <a><code>ScriptProcessorNode</code></a> nodes. It will be removed
          when the ScriptProcessorNode is removed, as the replacement
          <a>AudioWorker</a> uses the <a>AudioProcessEvent</a>.
        </p>
        <p>
          The event handler processes audio from the input (if any) by
          accessing the audio data from the <code>inputBuffer</code> attribute.
          The audio data which is the result of the processing (or the
          synthesized data if there are no inputs) is then placed into the
          <code>outputBuffer</code>.
        </p>
        <dl title="interface AudioProcessingEvent : Event" class="idl">
          <dt>
            readonly attribute double playbackTime
          </dt>
          <dd>
            <p>
              The time when the audio will be played in the same time
              coordinate system as the <a><code>AudioContext</code></a>'s
              <a href="#widl-BaseAudioContext-currentTime">currentTime</a>.
            </p>
          </dd>
          <dt>
            readonly attribute AudioBuffer inputBuffer
          </dt>
          <dd>
            <p>
              An AudioBuffer containing the input audio data. It will have a
              number of channels equal to the
              <code>numberOfInputChannels</code> parameter of the
              createScriptProcessor() method. This AudioBuffer is only valid
              while in the scope of the <code>onaudioprocess</code> function.
              Its values will be meaningless outside of this scope.
            </p>
          </dd>
          <dt>
            readonly attribute AudioBuffer outputBuffer
          </dt>
          <dd>
            <p>
              An AudioBuffer where the output audio data should be written. It
              will have a number of channels equal to the
              <code>numberOfOutputChannels</code> parameter of the
              createScriptProcessor() method. Script code within the scope of
              the <code>onaudioprocess</code> function is expected to modify
              the <code>Float32Array</code> arrays representing channel data in
              this AudioBuffer. Any script modifications to this AudioBuffer
              outside of this scope will not produce any audible effects.
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The PannerNode Interface
        </h2>
        <p>
          This interface represents a processing node which <a href=
          "#Spatialization">positions / spatializes</a> an incoming audio
          stream in three-dimensional space. The spatialization is in relation
          to the <a>AudioContext</a>'s <a><code>AudioListener</code></a>
          (<code>listener</code> attribute).
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The input of this node is either mono (1 channel) or stereo (2
          channels) and cannot be increased. Connections from nodes with fewer
          or more channels will be <a href=
          "#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a>, but a NotSupportedError MUST be thrown if an
          attempt is made to set channelCount to a value greater than 2 or if
          channelCountMode is set to "max".
        </p>
        <p>
          The output of this node is hard-coded to stereo (2 channels) and
          cannot be configured.
        </p>
        <p>
          The <a><code>PanningModelType</code></a> enum determines which
          spatialization algorithm will be used to position the audio in 3D
          space. The default is <code>"equalpower"</code>.
        </p>
        <dl title="enum PanningModelType" class="idl">
          <dt>
            equalpower
          </dt>
          <dd>
            A simple and efficient spatialization algorithm using equal-power
            panning.
          </dd>
          <dt>
            HRTF
          </dt>
          <dd>
            A higher quality spatialization algorithm using a convolution with
            measured impulse responses from human subjects. This panning method
            renders stereo output.
          </dd>
        </dl>
        <p>
          The <a><code>DistanceModelType</code></a> enum determines which
          algorithm will be used to reduce the volume of an audio source as it
          moves away from the listener. The default is "inverse".
        </p>
        <p>
          In the description of each distance model below, let \(d\) be the
          distance between the listener and the panner; \(d_{ref}\) be the
          value of the <code>refDistance</code> attribute; \(d_{max}\) be the
          value of the <code>maxDistance</code> attribute; and \(f\) be the
          value of the <code>rolloffFactor</code> attribute.
        </p>
        <dl title="enum DistanceModelType" class="idl">
          <dt>
            linear
          </dt>
          <dd>
            <p>
              A linear distance model which calculates <em>distanceGain</em>
              according to:
            </p>
            <pre class="nohighlight">
            $$
              1 - f\frac{\max(\min(d, d_{max}), d_{ref}) - d_{ref}}{d_{max} - d_{ref}}
            $$
            
</pre>
            <p>
              That is, \(d\) is clamped to the interval \([d_{ref},\,
              d_{max}]\).
            </p>
          </dd>
          <dt>
            inverse
          </dt>
          <dd>
            <p>
              An inverse distance model which calculates <em>distanceGain</em>
              according to:
            </p>
            <pre class="nohighlight">
              $$
                \frac{d_{ref}}{d_{ref} + f (\max(d, d_{ref}) - d_{ref})}
              $$
            
</pre>
            <p>
              That is, \(d\) is clamped to the interval \([d_{ref},\,
              \infty)\).
            </p>
          </dd>
          <dt>
            exponential
          </dt>
          <dd>
            <p>
              An exponential distance model which calculates
              <em>distanceGain</em> according to:
            </p>
            <pre class="nohighlight">
              $$
                \left(\frac{\max(d, d_{ref})}{d_{ref}}\right)^{-f}
              $$
            
</pre>
            <p>
              That is, \(d\) is clamped to the interval \([d_{ref},\,
              \infty)\).
            </p>
          </dd>
        </dl>
        <dl title=
        "[Constructor(BaseAudioContext context, optional PannerOptions options)]interface PannerNode : AudioNode"
        class="idl">
          <dt>
            attribute PanningModelType panningModel
          </dt>
          <dd>
            <p>
              Specifies the panning model used by this
              <a><code>PannerNode</code></a>. Defaults to
              <a><code>"equalpower"</code></a>.
            </p>
          </dd><!--<dt> // Uses a 3D cartesian coordinate system </dt>-->
          <dt>
            readonly attribute AudioParam positionX
          </dt>
          <dd>
            <p>
              Sets the x coordinate position of the audio source in a 3D
              Cartesian system. The default value is 0. This parameter is
              a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam positionY
          </dt>
          <dd>
            <p>
              Sets the y coordinate position of the audio source in a 3D
              Cartesian system. The default value is 0. This parameter is
              a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam positionZ
          </dt>
          <dd>
            <p>
              Sets the z coordinate position of the audio source in a 3D
              Cartesian system. The default value is 0. This parameter is
              a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam orientationX
          </dt>
          <dd>
            <p>
              Describes the x component of the vector of the direction the
              audio source is pointing in 3D Cartesian coordinate space.
              Depending on how directional the sound is (controlled by the
              <b>cone</b> attributes), a sound pointing away from the listener
              can be very quiet or completely silent. The default value is 1.
              This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam orientationY
          </dt>
          <dd>
            <p>
              Describes the y component of the vector of the direction the
              audio source is pointing in 3D cartesian coordinate space. The
              default value is 0. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam orientationZ
          </dt>
          <dd>
            <p>
              Describes the Z component of the vector of the direction the
              audio source is pointing in 3D cartesian coordinate space. The
              default value is 0. This parameter is a-rate.
            </p>
          </dd><!--<dt> // Distance model and attributes </dt>-->
          <dt>
            attribute DistanceModelType distanceModel
          </dt>
          <dd>
            <p>
              Specifies the distance model used by this
              <a><code>PannerNode</code></a>. Defaults to
              <a><code>"inverse"</code></a>.
            </p>
          </dd>
          <dt>
            attribute double refDistance
          </dt>
          <dd>
            <p>
              A reference distance for reducing volume as source move further
              from the listener. The default value is 1.
            </p>
          </dd>
          <dt>
            attribute double maxDistance
          </dt>
          <dd>
            <p>
              The maximum distance between source and listener, after which the
              volume will not be reduced any further. The default value is
              10000.
            </p>
          </dd>
          <dt>
            attribute double rolloffFactor
          </dt>
          <dd>
            <p>
              Describes how quickly the volume is reduced as source moves away
              from listener. The default value is 1.
            </p>
          </dd><!--<dt> // Directional sound cone </dt>-->
          <dt>
            attribute double coneInnerAngle
          </dt>
          <dd>
            <p>
              A parameter for directional audio sources, this is an angle, in
              degrees, inside of which there will be no volume reduction. The
              default value is 360. The behavior is undefined if the angle is
              outside the interval [0, 360].
            </p>
          </dd>
          <dt>
            attribute double coneOuterAngle
          </dt>
          <dd>
            <p>
              A parameter for directional audio sources, this is an angle, in
              degrees, outside of which the volume will be reduced to a
              constant value of <a><code>coneOuterGain</code></a>. The default
              value is 360. The behavior is undefined if the angle is outside
              the interval [0, 360].
            </p>
          </dd>
          <dt>
            attribute double coneOuterGain
          </dt>
          <dd>
            <p>
              A parameter for directional audio sources, this is the gain
              outside of the <a><code>coneOuterAngle</code></a>. The default
              value is 0. It is a linear value (not dB) in the range [0, 1]. An
              InvalidStateError MUST be thrown if the parameter is outside this
              range.
            </p>
          </dd>
          <dt>
            void setPosition(float x, float y, float z)
          </dt>
          <dd>
            <p>
              This method is DEPRECATED. It is equivalent to setting
              <code>positionX</code>, <code>positionY</code>, and
              <code>positionZ</code> AudioParams directly.
            </p>
            <p>
              Sets the position of the audio source relative to the
              <a><code>listener</code></a> attribute. A 3D cartesian coordinate
              system is used.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent the coordinates in
              3D space.
            </p>
            <p>
              The default value is (0,0,0)
            </p>
          </dd>
          <dt>
            void setOrientation(float x, float y, float z)
          </dt>
          <dd>
            <p>
              This method is DEPRECATED. It is equivalent to setting
              <code>orientationX</code>, <code>orientationY</code>, and
              <code>orientationZ</code> AudioParams directly.
            </p>
            <p>
              Describes which direction the audio source is pointing in the 3D
              cartesian coordinate space. Depending on how directional the
              sound is (controlled by the <b>cone</b> attributes), a sound
              pointing away from the listener can be very quiet or completely
              silent.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent a direction vector
              in 3D space.
            </p>
            <p>
              The default value is (1,0,0)
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            PannerOptions
          </h2>
          <p>
            This specifies options for constructing a
            <a><code>PannerNode</code></a>. All members are optional; if not
            specified, the normal default is used in constructing the node.
          </p>
          <dl title="dictionary PannerOptions : AudioNodeOptions" class="idl">
            <dt>
              PanningModelType panningModel
            </dt>
            <dd>
              The panning model to use for the node.
            </dd>
            <dt>
              DistanceModelType distanceModel
            </dt>
            <dd>
              The distance model to use for the node.
            </dd>
            <dt>
              float positionX
            </dt>
            <dd>
              The initial X value for the <a href=
              "#widl-PannerNode-positionX"><code>positionX</code></a>
              AudioParam.
            </dd>
            <dt>
              float positionY
            </dt>
            <dd>
              The initial Y value for the <a href=
              "#widl-PannerNode-positionY"><code>positionY</code></a>
              AudioParam.
            </dd>
            <dt>
              float positionZ
            </dt>
            <dd>
              The initial Z value for the <a href=
              "#widl-PannerNode-positionZ"><code>positionZ</code></a>
              AudioParam.
            </dd>
            <dt>
              float orientationX
            </dt>
            <dd>
              The initial X value for the <a href=
              "#widl-PannerNode-orientationX"><code>orientationX</code></a>
              AudioParam.
            </dd>
            <dt>
              float orientationY
            </dt>
            <dd>
              The initial Y value for the <a href=
              "#widl-PannerNode-orientationY"><code>orientationY</code></a>
              AudioParam.
            </dd>
            <dt>
              float orientationZ
            </dt>
            <dd>
              The initial Z value for the <a href=
              "#widl-PannerNode-orientationZ"><code>orientationZ</code></a>
              AudioParam.
            </dd>
            <dt>
              double refDistance
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-PannerNode-refDistance"><code>refDistance</code></a>
              attribute of the node.
            </dd>
            <dt>
              double maxDistance
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-PannerNode-maxDistance"><code>maxDistance</code></a>
              attribute of the node.
            </dd>
            <dt>
              double rolloffFactor
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-PannerNode-rolloffFactor"><code>rolloffFactor</code></a>
              attribute of the node.
            </dd>
            <dt>
              double coneInnerAngle
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-PannerNode-coneInnerAngle"><code>coneInnerAngle</code></a>
              attribute of the node.
            </dd>
            <dt>
              double coneOuterAngle
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-PannerNode-coneOuterAngle"><code>coneOuterAngle</code></a>
              attribute of the node.
            </dd>
            <dt>
              double coneOuterGain
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-PannerNode-coneOuterGain"><code>coneOuterGain</code></a>
              attribute of the node.
            </dd>
          </dl>
        </section>
        <section class="informative">
          <h3 id="panner-channel-limitations">
            Channel Limitations
          </h3>
          <p>
            The set of <a href="#panner-channel-limitations">channel
            limitations</a> for <a><code>StereoPannerNode</code></a> also apply
            to <a><code>PannerNode</code></a>.
          </p>
        </section>
      </section>
      <section>
        <h2 id="AudioListener">
          The AudioListener Interface
        </h2>
        <p>
          This interface represents the position and orientation of the person
          listening to the audio scene. All <a><code>PannerNode</code></a>
          objects spatialize in relation to the
          <a><code>BaseAudioContext</code></a>'s <a href=
          "#widl-BaseAudioContext-listener">listener</a>. See
          <a>Spatialization/Panning</a> for more details about spatialization.
        </p>
        <p>
          The <code>positionX, positionY, positionZ</code> parameters represent
          the location of the listener in 3D Cartesian coordinate space.
          <a><code>PannerNode</code></a> objects use this position relative to
          individual audio sources for spatialization.
        </p>
        <p>
          The <code>forwardX, forwardY, forwardZ</code> parameters represent a
          direction vector in 3D space. Both a <code>forward</code> vector and
          an <code>up</code> vector are used to determine the orientation of
          the listener. In simple human terms, the <code>forward</code> vector
          represents which direction the person's nose is pointing. The
          <code>up</code> vector represents the direction the top of a person's
          head is pointing. These values are expected to be linearly
          independent (at right angles to each other), and unpredictable
          behavior may result if they are not. For normative requirements of
          how these values are to be interpreted, see the
          <a>Spatialization/Panning</a> section.
        </p>
        <dl title="interface AudioListener" class="idl">
          <dt>
            readonly attribute AudioParam positionX
          </dt>
          <dd>
            <p>
              Sets the x coordinate position of the audio listener in a 3D
              Cartesian coordinate space. The default value is 0. This
              parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam positionY
          </dt>
          <dd>
            <p>
              Sets the y coordinate position of the audio listener in a 3D
              Cartesian coordinate space. The default value is 0. This
              parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam positionZ
          </dt>
          <dd>
            <p>
              Sets the z coordinate position of the audio listener in a 3D
              Cartesian coordinate space. The default value is 0. This
              parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam forwardX
          </dt>
          <dd>
            <p>
              Sets the x coordinate component of the forward direction the
              listener is pointing in 3D Cartesian coordinate space. The
              default value is 0. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam forwardY
          </dt>
          <dd>
            <p>
              Sets the y coordinate component of the forward direction the
              listener is pointing in 3D Cartesian coordinate space. The
              default value is 0. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam forwardZ
          </dt>
          <dd>
            <p>
              Sets the z coordinate component of the forward direction the
              listener is pointing in 3D Cartesian coordinate space. The
              default value is -1. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam upX
          </dt>
          <dd>
            <p>
              Sets the x coordinate component of the up direction the listener
              is pointing in 3D Cartesian coordinate space. The default value
              is 0. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam upY
          </dt>
          <dd>
            <p>
              Sets the y coordinate component of the up direction the listener
              is pointing in 3D Cartesian coordinate space. The default value
              is 1. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam upZ
          </dt>
          <dd>
            <p>
              Sets the z coordinate component of the up direction the listener
              is pointing in 3D Cartesian coordinate space. The default value
              is 0. This parameter is a-rate.
            </p>
          </dd>
          <dt>
            void setPosition(float x, float y, float z)
          </dt>
          <dd>
            <p>
              This method is DEPRECATED. It is equivalent to setting
              <code>positionX.value</code>, <code>positionY.value</code>, and
              <code>positionZ.value</code> directly with the given
              <code>x</code>, <code>y</code>, and <code>z</code> values,
              respectively.
            </p>
            <p>
              Sets the position of the listener in a 3D cartesian coordinate
              space. <a><code>PannerNode</code></a> objects use this position
              relative to individual audio sources for spatialization.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent the coordinates in
              3D space.
            </p>
            <p>
              The default value is (0,0,0)
            </p>
          </dd>
          <dt>
            void setOrientation(float x, float y, float z, float xUp, float
            yUp, float zUp)
          </dt>
          <dd>
            <p>
              This method is DEPRECATED. It is equivalent to setting
              <code>orientationX.value</code>, <code>orientationY.value</code>,
              <code>orientationZ.value</code>, <code>upX.value</code>,
              <code>upY.value</code>, and <code>upZ.value</code> directly with
              the given <code>x</code>, <code>y</code>, <code>z</code>,
              <code>xUp</code>, <code>yUp</code>, and <code>zUp</code> values,
              respectively.
            </p>
            <p>
              Describes which direction the listener is pointing in the 3D
              cartesian coordinate space. Both a <b>front</b> vector and an
              <b>up</b> vector are provided. In simple human terms, the
              <b>front</b> vector represents which direction the person's nose
              is pointing. The <b>up</b> vector represents the direction the
              top of a person's head is pointing. These values are expected to
              be linearly independent (at right angles to each other). For
              normative requirements of how these values are to be interpreted,
              see the <a href="#Spatialization">spatialization section</a>.
            </p>
            <p>
              The <code>x, y, z</code> parameters represent a <b>front</b>
              direction vector in 3D space, with the default value being
              (0,0,-1).
            </p>
            <p>
              The <code>xUp, yUp, zUp</code> parameters represent an <b>up</b>
              direction vector in 3D space, with the default value being
              (0,1,0).
            </p>
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          The StereoPannerNode Interface
        </h2>
        <p>
          This interface represents a processing node which positions an
          incoming audio stream in a stereo image using a low-cost <a href=
          "#Spatialzation-equal-power-panning">equal-power panning
          algorithm</a>. This panning effect is common in positioning audio
          components in a stereo stream.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The input of this node is stereo (2 channels) and cannot be
          increased. Connections from nodes with fewer or more channels will be
          <a href="#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a> , but a NotSupportedError will be thrown if an
          attempt is made to set <code>channelCount</code> to a value great
          than 2 or if <code>channelCountMode</code> is set to
          <code>"max"</code>.
        </p>
        <p>
          The output of this node is hard-coded to stereo (2 channels) and
          cannot be configured.
        </p>
        <dl title=
        "[Constructor(BaseAudioContext context, optional StereoPannerOptions options)]interface StereoPannerNode : AudioNode"
        class="idl">
          <dt>
            readonly attribute AudioParam pan
          </dt>
          <dd>
            <p>
              The position of the input in the output's stereo image. -1
              represents full left, +1 represents full right. Its default value
              is 0, and its <a>nominal range</a> is [-1, 1]. This parameter is
              <a>a-rate</a>.
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            StereoPannerOptions
          </h2>
          <p>
            This specifies the options to use in constructing a
            <a><code>StereoPannerNode</code></a>. All members are optional; if
            not specified, the normal default is used in constructing the node.
          </p>
          <dl title="dictionary StereoPannerOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              float pan
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-StereoPannerNode-pan"><code>pan</code></a> AudioParam.
            </dd>
          </dl>
        </section>
        <section class="informative">
          <h3>
            Channel Limitations
          </h3>
          <p>
            Because its processing is constrained by the above definitions,
            <a><code>StereoPannerNode</code></a> is limited to mixing no more
            than 2 channels of audio, and producing exactly 2 channels. It is
            possible to use a <a><code>ChannelSplitterNode</code></a>,
            intermediate processing by a subgraph of
            <a><code>GainNode</code></a>s and/or other nodes, and recombination
            via a <a><code>ChannelMergerNode</code></a> to realize arbitrary
            approaches to panning and mixing.
          </p>
        </section>
      </section>
      <section>
        <h2 id="ConvolverNode">
          The ConvolverNode Interface
        </h2>
        <p>
          This interface represents a processing node which applies a linear
          convolution effect given an impulse response.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The input of this node is either mono (1 channel) or stereo (2
          channels) and cannot be increased. Connections from nodes with fewer
          or more channels will be <a href=
          "#channel-up-mixing-and-down-mixing">up-mixed or down-mixed
          appropriately</a>, but a NotSupportedError MUST be thrown if an
          attempt is made to set channelCount to a value great than 2 or if
          channelCountMode is set to "max".
        </p>
        <dl title="interface ConvolverNode : AudioNode" class="idl">
          <dt>
            attribute AudioBuffer? buffer
          </dt>
          <dd>
            <p>
              A mono, stereo, or 4-channel <a><code>AudioBuffer</code></a>
              containing the (possibly multi-channel) impulse response used by
              the <a><code>ConvolverNode</code></a>. <span class=
              "synchronous">The <code>AudioBuffer</code> must have 1, 2, or 4
              channels or a NotSupportedError exception MUST be thrown</span>.
              <span class="synchronous">This <a><code>AudioBuffer</code></a>
              must be of the same sample-rate as the
              <a><code>AudioContext</code></a> or a NotSupportedError exception
              MUST be thrown</span>. At the time when this attribute is set,
              the <em>buffer</em> and the state of the <em>normalize</em>
              attribute will be used to configure the
              <a><code>ConvolverNode</code></a> with this impulse response
              having the given normalization. The initial value of this
              attribute is null.
            </p>
          </dd>
          <dt>
            attribute boolean normalize
          </dt>
          <dd>
            <p>
              Controls whether the impulse response from the buffer will be
              scaled by an equal-power normalization when the
              <code>buffer</code> atttribute is set. Its default value is
              <code>true</code> in order to achieve a more uniform output level
              from the convolver when loaded with diverse impulse responses. If
              <code>normalize</code> is set to <code>false</code>, then the
              convolution will be rendered with no pre-processing/scaling of
              the impulse response. Changes to this value do not take effect
              until the next time the <em>buffer</em> attribute is set.
            </p>
            <p>
              If the <em>normalize</em> attribute is false when the
              <em>buffer</em> attribute is set then the
              <a><code>ConvolverNode</code></a> will perform a linear
              convolution given the exact impulse response contained within the
              <em>buffer</em>.
            </p>
            <p>
              Otherwise, if the <em>normalize</em> attribute is true when the
              <em>buffer</em> attribute is set then the
              <a><code>ConvolverNode</code></a> will first perform a scaled
              RMS-power analysis of the audio data contained within
              <em>buffer</em> to calculate a <em>normalizationScale</em> given
              this algorithm:
            </p>
            <pre>

function calculateNormalizationScale(buffer)
{
    var GainCalibration = 0.00125;
    var GainCalibrationSampleRate = 44100;
    var MinPower = 0.000125;

    // Normalize by RMS power.
    var numberOfChannels = buffer.numberOfChannels;
    var length = buffer.length;

    var power = 0;

    for (var i = 0; i &lt; numberOfChannels; i++) {
        var channelPower = 0;
        var channelData = buffer.getChannelData(i);

        for (var j = 0; j &lt; length; j++) {
            var sample = channelData[j];
            channelPower += sample * sample;
        }

        power += channelPower;
    }

    power = Math.sqrt(power / (numberOfChannels * length));

    // Protect against accidental overload.
    if (!isFinite(power) || isNaN(power) || power &lt; MinPower)
        power = MinPower;

    var scale = 1 / power;

    // Calibrate to make perceived volume same as unprocessed.
    scale *= GainCalibration;

    // Scale depends on sample-rate.
    if (buffer.sampleRate)
        scale *= GainCalibrationSampleRate / buffer.sampleRate;

    // True-stereo compensation.
    if (numberOfChannels == 4)
        scale *= 0.5;

    return scale;
}
      
</pre>
            <p>
              During processing, the ConvolverNode will then take this
              calculated <em>normalizationScale</em> value and multiply it by
              the result of the linear convolution resulting from processing
              the input with the impulse response (represented by the
              <em>buffer</em>) to produce the final output. Or any
              mathematically equivalent operation may be used, such as
              pre-multiplying the input by <em>normalizationScale</em>, or
              pre-multiplying a version of the impulse-response by
              <em>normalizationScale</em>.
            </p>
          </dd>
        </dl>
        <section>
          <h3 id="Convolution-channel-configurations">
            Channel Configurations for Input, Impulse Response and Output
          </h3>
          <p>
            Implementations MUST support the following allowable configurations
            of impulse response channels in a <a><code>ConvolverNode</code></a>
            to achieve various reverb effects with 1 or 2 channels of input.
          </p>
          <p>
            The first image in the diagram illustrates the general case, where
            the source has N input channels, the impulse response has K
            channels, and the playback system has M output channels. Because
            <a><code>ConvolverNode</code></a> is limited to 1 or 2 channels of
            input, not every case can be handled.
          </p>
          <p>
            Single channel convolution operates on a mono audio input, using a
            mono impulse response, and generating a mono output. The remaining
            images in the diagram illustrate the supported cases for mono and
            stereo playback where N and M are 1 or 2 and K is 1, 2, or 4.
            Developers desiring more complex and arbitrary matrixing can use a
            <a><code>ChannelSplitterNode</code></a>, multiple single-channel
            <a><code>ConvolverNode</code></a>s and a
            <a><code>ChannelMergerNode</code></a>.
          </p>
          <figure>
            <img alt="reverb matrixing" src="images/reverb-matrixing.png">
            <figcaption>
              A graphical representation of supported input and output channel
              count possibilities when using a
              <a><code>ConvolverNode</code></a>.
            </figcaption>
          </figure>
        </section>
      </section>
      <section>
        <h2>
          The AnalyserNode Interface
        </h2>
        <p>
          This interface represents a node which is able to provide real-time
          frequency and time-domain analysis information. The audio stream will
          be passed un-processed from input to output.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1    <em>Note that this output may be left unconnected.</em>

    channelCount = 1;
    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <dl title=
        "[Constructor(BaseAudioContext context, optional AnalyserOptions options)]interface AnalyserNode : AudioNode"
        class="idl">
          <!--<dt> // Real-time frequency-domain data </dt>-->
          <dt>
            void getFloatFrequencyData()
          </dt>
          <dd>
            <p>
              Copies the <a>current frequency data</a> into the passed
              floating-point array. If the array has fewer elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be dropped. If the array has more elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be ignored.
            </p>
            <p>
              The frequency data are in dB units.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array array
              </dt>
              <dd>
                This parameter is where the frequency-domain analysis data will
                be copied.
              </dd>
            </dl>
          </dd>
          <dt>
            void getByteFrequencyData()
          </dt>
          <dd>
            <p>
              Copies the <a>current frequency data</a> into the passed unsigned
              byte array. If the array has fewer elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be dropped. If the array has more elements than the
              <a><code>frequencyBinCount</code></a>, the excess elements will
              be ignored.
            </p>
            <p>
              The values stored in the unsigned byte array are computed in the
              following way. Let \(Y[k]\) be the <a>current frequency data</a>
              as described in <a href=
              "#fft-windowing-and-smoothing-over-time">FFT windowing and
              smoothing</a>. Then the byte value, \(b[k]\), is
            </p>
            <pre class="nohighlight">
                  $$
                    b[k] = \left\lfloor
                        \frac{255}{\mbox{dB}_{max} - \mbox{dB}_{min}}
                        \left(Y[k] - \mbox{dB}_{min}\right)
                      \right\rfloor
                  $$
            </pre>
            <p>
              where \(\mbox{dB}_{min}\) is <code><a>minDecibels</a></code> and
              \(\mbox{dB}_{max}\) is <code><a>maxDecibels</a></code>. If
              \(b[k]\) lies outside the range of 0 to 255, \(b[k]\) is clipped
              to lie in that range.
            </p>
            <dl class="parameters">
              <dt>
                Uint8Array array
              </dt>
              <dd>
                This parameter is where the frequency-domain analysis data will
                be copied.
              </dd>
            </dl>
          </dd><!--<dt>// Real-time waveform data </dt>-->
          <dt>
            void getFloatTimeDomainData()
          </dt>
          <dd>
            <p>
              Copies the current down-mixed time-domain (waveform) data into
              the passed floating-point array. If the array has fewer elements
              than the value of <a><code>fftSize</code></a>, the excess
              elements will be dropped. If the array has more elements than
              <a><code>fftSize</code></a>, the excess elements will be ignored.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array array
              </dt>
              <dd>
                This parameter is where the time-domain sample data will be
                copied.
              </dd>
            </dl>
          </dd>
          <dt>
            void getByteTimeDomainData()
          </dt>
          <dd>
            <p>
              Copies the current down-mixed time-domain (waveform) data into
              the passed unsigned byte array. If the array has fewer elements
              than the value of <a><code>fftSize</code></a>, the excess
              elements will be dropped. If the array has more elements than
              <a><code>fftSize</code></a>, the excess elements will be ignored.
            </p>
            <p>
              The values stored in the unsigned byte array are computed in the
              following way. Let \(x[k]\) be the time-domain data. Then the
              byte value, \(b[k]\), is
            </p>
            <pre class="nohighlight">
              $$
                b[k] = \left\lfloor 128(1 + x[k]) \right\rfloor.
              $$
            </pre>
            <p>
              If \(b[k]\) lies outside the range 0 to 255, \(b[k]\) is clipped
              to lie in that range.
            </p>
            <dl class="parameters">
              <dt>
                Uint8Array array
              </dt>
              <dd>
                This parameter is where the time-domain sample data will be
                copied.
              </dd>
            </dl>
          </dd>
          <dt>
            attribute unsigned long fftSize
          </dt>
          <dd>
            <p>
              The size of the FFT used for frequency-domain analysis.
              <span class="synchronous">This must be a power of two in the
              range 32 to 32768, otherwise an IndexSizeError exception MUST be
              thrown</span>. The default value is 2048. Note that large FFT
              sizes can be costly to compute.
            </p>
          </dd>
          <dt>
            readonly attribute unsigned long frequencyBinCount
          </dt>
          <dd>
            <p>
              Half the FFT size.
            </p>
          </dd>
          <dt>
            attribute float minDecibels
          </dt>
          <dd>
            <p>
              <dfn id="minDecibels">minDecibels</dfn> is the minimum power
              value in the scaling range for the FFT analysis data for
              conversion to unsigned byte values. The default value is -100.
              <span class="synchronous">If the value of this attribute is set
              to a value more than or equal to <code><a>maxDecibels</a></code>,
              an IndexSizeError exception MUST be thrown.</span>
            </p>
          </dd>
          <dt>
            attribute float maxDecibels
          </dt>
          <dd>
            <p>
              <dfn id="maxDecibels">maxDecibels</dfn> is the maximum power
              value in the scaling range for the FFT analysis data for
              conversion to unsigned byte values. The default value is -30.
              <span class="synchronous">If the value of this attribute is set
              to a value less than or equal to <code><a>minDecibels</a></code>,
              an IndexSizeError exception MUST be thrown.</span>
            </p>
          </dd>
          <dt>
            attribute float smoothingTimeConstant
          </dt>
          <dd>
            <p>
              A value from 0 -&gt; 1 where 0 represents no time averaging with
              the last analysis frame. The default value is 0.8. <span class=
              "synchronous">If the value of this attribute is set to a value
              less than 0 or more than 1, an IndexSizeError exception MUST be
              thrown.</span>
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            AnalyserOptions
          </h2>
          <p>
            This specifies the options to be used when constructing an
            <a><code>AnalyserNode</code></a>. All members are optional; if not
            specified, the normal default values are used to construct the
            node.
          </p>
          <dl title="dictionary AnalyserOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              unsigned long fftSize = 2048
            </dt>
            <dd>
              The desired initial size of the FFT for frequency-domain
              analysis.
            </dd>
            <dt>
              float maxDecibels = -30
            </dt>
            <dd>
              The desired initial maximum power in dB for FFT analysis.
            </dd>
            <dt>
              float minDecibels = -100
            </dt>
            <dd>
              The desired initial minimum power in dB for FFT analysis.
            </dd>
            <dt>
              float smoothingTimeConstant = 0.8
            </dt>
            <dd>
              The desired initial smoothing constant for the FFT analysis.
            </dd>
          </dl>
        </section>
        <section>
          <h3>
            FFT Windowing and smoothing over time
          </h3>When the <dfn id="current-frequency-data">current frequency
          data</dfn> are computed, the following operations are to be
          performed:
          <ol>
            <li>
              <a href="#channel-up-mixing-and-down-mixing">Down-mix</a> all
              channels of the time domain input data to mono assuming a
              <a>channelCount</a> of 1, <a>channelCountMode</a> of "max" and
              <a>channelInterpetation</a> of "speakers". This is independent of
              the settings for the <a>AnalyserNode</a> itself.
            </li>
            <li>
              <a href="#blackman-window">Apply a Blackman window</a> to the
              time domain input data
            </li>
            <li>
              <a href="#fourier-transform">Apply a Fourier tranform</a> to the
              windowed time domain input data to get imaginary and real
              frequency data
            </li>
            <li>
              <a href="#smoothing-over-time">Smooth over time</a> the frequency
              domain data
            </li>
            <li>
              <a href="#conversion-to-db">Conversion to dB</a>.
            </li>
          </ol>
          <p>
            In the following, let \(N\) be the value of the
            <code>.fftSize</code> attribute of this <code>AnalyserNode</code>.
          </p>
          <p>
            <dfn id="blackman-window">Applying a Blackman window</dfn> consists
            in the following operation on the input time domain data. Let
            \(x[n]\) for \(n = 0, \ldots, N - 1\) be the time domain data. The
            Blackman window is defined by
          </p>
          <pre class="nohighlight">
          $$
          \begin{align*}
            \alpha &amp;= \mbox{0.16} \\ a_0 &amp;= \frac{1-\alpha}{2} \\
             a_1   &amp;= \frac{1}{2} \\
             a_2   &amp;= \frac{\alpha}{2} \\
             w[n] &amp;= a_0 - a_1 \cos\frac{2\pi n}{N} + a_2 \cos\frac{4\pi n}{N}, \mbox{ for } n = 0, \ldots, N - 1
           \end{align*}
           $$
          
</pre>
          <p>
            The windowed signal \(\hat{x}[n]\) is
          </p>
          <pre class="nohighlight">
            $$
              \hat{x}[n] = x[n] w[n], \mbox{ for } n = 0, \ldots, N - 1
            $$
          
</pre>
          <p>
            <dfn id="fourier-transform">Applying a Fourier tranform</dfn>
            consists of computing the Fourier transform in the following way.
            Let \(X[k]\) be the complex frequency domain data and
            \(\hat{x}[n]\) be the windowed time domain data computed above.
            Then
          </p>
          <pre class="nohighlight">
            $$
              X[k] = \frac{1}{N} \sum_{n = 0}^{N - 1} \hat{x}[n]\, e^{\frac{-2\pi i k n}{N}}
            $$
          </pre>
          <p>
            for \(k = 0, \dots, N/2-1\).
          </p>
          <p>
            <dfn id="smoothing-over-time">Smoothing over time</dfn> frequency
            data consists in the following operation:
          </p>
          <ul>
            <li>Let \(\hat{X}_{-1}[k]\) be the result of this operation on the
            <a>previous block</a>. The <dfn>previous block</dfn> is defined as
            being the buffer computed by the previous <a href=
            "#smoothing-over-time">smoothing over time</a> operation, or an
            array of \(N\) zeros if this is the first time we are <a href=
            "#smoothing-over-time">smoothing over time</a>.
            </li>
            <li>Let \(\tau\) be the value of the <a href=
            "#widl-AnalyserNode-smoothingTimeConstant"><code>smoothingTimeConstant</code></a>
            attribute for this <a><code>AnalyserNode</code></a>.
            </li>
            <li>Let \(X[k]\) be the result of <a href=
            "#fourier-transform">applying a Fourier transform</a> of the
            current block.
            </li>
          </ul>
          <p>
            Then the smoothed value, \(\hat{X}[k]\), is computed by
          </p>
          <pre class="nohighlight">
            $$
              \hat{X}[k] = \tau\, \hat{X}_{-1}[k] + (1 - \tau)\, |X[k]|
            $$
          
</pre>
          <p>
            for \(k = 0, \ldots, N - 1\).
          </p>
          <p>
            <dfn id="conversion-to-db">Conversion to dB</dfn> consists of the
            following operation, where \(\hat{X}[k]\) is computed in <a href=
            "#smoothing-over-time">smoothing over time</a>:
          </p>
          <pre class="nohighlight">
          $$
            Y[k] = 20\log_{10}\hat{X}[k]
          $$
          
</pre>
          <p>
            for \(k = 0, \ldots, N-1\).
          </p>
          <p>
            This array, \(Y[k]\), is copied to the output array for
            <code>getFloatFrequencyData</code>. For
            <code>getByteFrequencyData</code>, the \(Y[k]\) is clipped to lie
            between <code><a>minDecibels</a></code> and
            <code><a>maxDecibels</a></code> and then scaled to fit in an
            unsigned byte such that <code><a>minDecibels</a></code> is
            represented by the value 0 and <code><a>maxDecibels</a></code> is
            represented by the value 255.
          </p>
        </section>
      </section>
      <section>
        <h2>
          The ChannelSplitterNode Interface
        </h2>
        <p>
          The <code>ChannelSplitterNode</code> is for use in more advanced
          applications and would often be used in conjunction with
          <a><code>ChannelMergerNode</code></a>.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : Variable N (defaults to 6) // number of "active" (non-silent) outputs is determined by number of channels in the input

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          This interface represents an <a><code>AudioNode</code></a> for
          accessing the individual channels of an audio stream in the routing
          graph. It has a single input, and a number of "active" outputs which
          equals the number of channels in the input audio stream. For example,
          if a stereo input is connected to an
          <a><code>ChannelSplitterNode</code></a> then the number of active
          outputs will be two (one from the left channel and one from the
          right). There are always a total number of N outputs (determined by
          the <code>numberOfOutputs</code> parameter to the
          <a><code>AudioContext</code></a> method <a href=
          "#widl-BaseAudioContext-createChannelSplitter-ChannelSplitterNode-unsigned-long-numberOfOutputs">
          <code>createChannelSplitter()</code></a>), The default number is 6 if
          this value is not provided. Any outputs which are not "active" will
          output silence and would typically not be connected to anything.
        </p>
        <h3>
          Example:
        </h3>
        <figure>
          <img alt="channel splitter" src="images/channel-splitter.png">
          <figcaption>
            A diagram of a ChannelSplitter
          </figcaption>
        </figure>
        <p>
          Please note that in this example, the splitter does <b>not</b>
          interpret the channel identities (such as left, right, etc.), but
          simply splits out channels in the order that they are input.
        </p>
        <p>
          One application for <code>ChannelSplitterNode</code> is for doing
          "matrix mixing" where individual gain control of each channel is
          desired.
        </p>
        <dl title=
        "[Constructor(BaseAudioContext context, optional ChannelSplitterOptions options)] interface ChannelSplitterNode : AudioNode"
        class="idl"></dl>
        <section>
          <h2>
            ChannelSplitterOptions
          </h2>
          <dl title="dictionary ChannelSplitterOptions : AudioNodeOptions"
          class="idl">
            <dt>
              unsigned long numberOfOutputs = 6
            </dt>
            <dd>
              The number outputs for the
              <a><code>ChannelSplitterNode</code></a>.
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The ChannelMergerNode Interface
        </h2>
        <p>
          The <a><code>ChannelMergerNode</code></a> is for use in more advanced
          applications and would often be used in conjunction with
          <a><code>ChannelSplitterNode</code></a>.
        </p>
        <pre>
  numberOfInputs  : Variable N (default to 6)
  numberOfOutputs : 1

  channelCount = 1;
  channelCountMode = "explicit";
  channelInterpretation = "speakers";
</pre>
        <p>
          This interface represents an <a><code>AudioNode</code></a> for
          combining channels from multiple audio streams into a single audio
          stream. It has a variable number of inputs (defaulting to 6), but not
          all of them need be connected. There is a single output whose audio
          stream has a number of channels equal to the number of inputs.
        </p>
        <p>
          To merge multiple inputs into one stream, each input gets downmixed
          into one channel (mono) based on the specified mixing rule. An
          unconnected input still counts as <b>one silent channel</b> in the
          output. Changing input streams does <b>not</b> affect the order of
          output channels.
        </p>
        <p>
          For <a><code>ChannelMergerNode</code></a>, <code>channelCount</code>
          and <code>channelCountMode</code> properties cannot be changed.
          <code>InvalidState</code> error MUST be thrown when they changed.
        </p>
        <h3 id="example-2">
          Example:
        </h3>
        <p>
          For example, if a default <a><code>ChannelMergerNode</code></a> has
          two connected stereo inputs, the first and second input will be
          downmixed to mono respectively before merging. The output will be a
          6-channel stream whose first two channels are be filled with the
          first two (downmixed) inputs and the rest of channels will be silent.
        </p>
        <p>
          Also the <a><code>ChannelMergerNode</code></a> can be used to arrange
          multiple audio streams in a certain order for the multi-channel
          speaker array such as 5.1 surround set up. The merger does not
          interpret the channel identities (such as left, right, etc.), but
          simply combines channels in the order that they are input.
        </p>
        <figure>
          <img alt="channel merger" src="images/channel-merger.png">
          <figcaption>
            A diagram of ChannelMerger
          </figcaption>
        </figure>
        <dl title=
        "[Constructor(BaseAudioContext context, optional ChannelSplitterOptions options)] interface ChannelMergerNode : AudioNode"
        class="idl"></dl>
        <section>
          <h2>
            ChannelMergerOptions
          </h2>
          <dl title="dictionary ChannelMergerOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              unsigned long numberOfInputs = 6
            </dt>
            <dd>
              The number inputs for the
              <a><code>ChannelSplitterNode</code></a>.
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The DynamicsCompressorNode Interface
        </h2>
        <p>
          <a><code>DynamicsCompressorNode</code></a> is an
          <a><code>AudioNode</code></a> processor implementing a dynamics
          compression effect.
        </p>
        <p>
          Dynamics compression is very commonly used in musical production and
          game audio. It lowers the volume of the loudest parts of the signal
          and raises the volume of the softest parts. Overall, a louder,
          richer, and fuller sound can be achieved. It is especially important
          in games and musical applications where large numbers of individual
          sounds are played simultaneous to control the overall signal level
          and help avoid clipping (distorting) the audio output to the
          speakers.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
        <dl title=
        "[Constructor(BaseAudioContext context, optional DynamicsCompressorOptions options)]interface DynamicsCompressorNode : AudioNode"
        class="idl">
          <dt>
            readonly attribute AudioParam threshold
          </dt>
          <dd>
            <p>
              The decibel value above which the compression will start taking
              effect. Its default <code>value</code> is -24. Its <a>nominal
              range</a> is [-100, 0].
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam knee
          </dt>
          <dd>
            <p>
              A decibel value representing the range above the threshold where
              the curve smoothly transitions to the "ratio" portion. Its
              default <code>value</code> is 30. Its <a>nominal range</a> is [0,
              40].
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam ratio
          </dt>
          <dd>
            <p>
              The amount of dB change in input for a 1 dB change in output. Its
              default <code>value</code> is 12. Its <a>nominal range</a> is [1,
              20].
            </p>
          </dd>
          <dt>
            readonly attribute float reduction
          </dt>
          <dd>
            <p>
              A read-only decibel value for metering purposes, representing the
              current amount of gain reduction that the compressor is applying
              to the signal. If fed no signal the value will be 0 (no gain
              reduction).
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam attack
          </dt>
          <dd>
            <p>
              The amount of time (in seconds) to reduce the gain by 10dB. Its
              default <code>value</code> is 0.003. Its <a>nominal range</a> is
              [0, 1].
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam release
          </dt>
          <dd>
            <p>
              The amount of time (in seconds) to increase the gain by 10dB. Its
              default <code>value</code> is 0.250. Its <a>nominal range</a> is
              [0, 1].
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            DynamicsCompressorOptions
          </h2>
          <p>
            This specifies the options to use in constructing a
            <a><code>DynamicsCompressorNode</code></a>. All members are
            optional; if not specified the normal defaults are used in
            constructing the node.
          </p>
          <dl title="dictionary DynamicsCompressorOptions : AudioNodeOptions"
          class="idl">
            <dt>
              float attack
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-DynamicsCompressorNode-attack"><code>attack</code></a>
              AudioParam.
            </dd>
            <dt>
              float knee
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-DynamicsCompressorNode-knee"><code>knee</code></a>
              AudioParam.
            </dd>
            <dt>
              float ratio
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-DynamicsCompressorNode-ratio"><code>ratio</code></a>
              AudioParam.
            </dd>
            <dt>
              float release
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-DynamicsCompressorNode-release"><code>release</code></a>
              AudioParam.
            </dd>
            <dt>
              float threshold
            </dt>
            <dd>
              The initial value for the <a href=
              "#widl-DynamicsCompressorNode-threshold"><code>threshold</code></a>
              AudioParam.
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The BiquadFilterNode Interface
        </h2>
        <p>
          <a><code>BiquadFilterNode</code></a> is an
          <a><code>AudioNode</code></a> processor implementing very common
          low-order filters.
        </p>
        <p>
          Low-order filters are the building blocks of basic tone controls
          (bass, mid, treble), graphic equalizers, and more advanced filters.
          Multiple <a><code>BiquadFilterNode</code></a> filters can be combined
          to form more complex filters. The filter parameters such as <a href=
          "#widl-BiquadFilterNode-frequency"><code>frequency</code></a> can be
          changed over time for filter sweeps, etc. Each
          <a><code>BiquadFilterNode</code></a> can be configured as one of a
          number of common filter types as shown in the IDL below. The default
          filter type is <code>"lowpass"</code>.
        </p>
        <p>
          Both <a href=
          "#widl-BiquadFilterNode-frequency"><code>frequency</code></a> and
          <a href="#widl-BiquadFilterNode-detune"><code>detune</code></a> form
          a <a>compound parameter</a> and are both <a>a-rate</a>. They are used
          together to determine a <dfn id=
          "computedFreq-biquad">computedFrequency</dfn> value:
        </p>
        <pre class="highlight">
  computedFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)
</pre>
        <p>
          The nominal range for this <a>compound parameter</a> is [0,
          <a>Nyquist frequency</a>].
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <dl title="enum BiquadFilterType" class="idl">
          <dt>
            lowpass
          </dt>
          <dd>
            <p>
              A <a href="https://en.wikipedia.org/wiki/Low-pass_filter">lowpass
              filter</a> allows frequencies below the cutoff frequency to pass
              through and attenuates frequencies above the cutoff. It
              implements a standard second-order resonant lowpass filter with
              12dB/octave rolloff.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The cutoff frequency
                </dd>
                <dt>
                  Q
                </dt>
                <dd>
                  Controls how peaked the response will be at the cutoff
                  frequency. A large value makes the response more peaked.
                  Please note that for this filter type, this value is not a
                  traditional Q, but is a resonance value in decibels.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            highpass
          </dt>
          <dd>
            <p>
              A <a href=
              "https://en.wikipedia.org/wiki/High-pass_filter">highpass
              filter</a> is the opposite of a lowpass filter. Frequencies above
              the cutoff frequency are passed through, but frequencies below
              the cutoff are attenuated. It implements a standard second-order
              resonant highpass filter with 12dB/octave rolloff.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The cutoff frequency below which the frequencies are
                  attenuated
                </dd>
                <dt>
                  Q
                </dt>
                <dd>
                  Controls how peaked the response will be at the cutoff
                  frequency. A large value makes the response more peaked.
                  Please note that for this filter type, this value is not a
                  traditional Q, but is a resonance value in decibels.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            bandpass
          </dt>
          <dd>
            <p>
              A <a href=
              "https://en.wikipedia.org/wiki/Band-pass_filter">bandpass
              filter</a> allows a range of frequencies to pass through and
              attenuates the frequencies below and above this frequency range.
              It implements a second-order bandpass filter.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The center of the frequency band
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls the width of the band. The width becomes narrower as
                  the Q value increases.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            lowshelf
          </dt>
          <dd>
            <p>
              The lowshelf filter allows all frequencies through, but adds a
              boost (or attenuation) to the lower frequencies. It implements a
              second-order lowshelf filter.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The upper limit of the frequences where the boost (or
                  attenuation) is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  The boost, in dB, to be applied. If the value is negative,
                  the frequencies are attenuated.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            highshelf
          </dt>
          <dd>
            <p>
              The highshelf filter is the opposite of the lowshelf filter and
              allows all frequencies through, but adds a boost to the higher
              frequencies. It implements a second-order highshelf filter
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The lower limit of the frequences where the boost (or
                  attenuation) is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  The boost, in dB, to be applied. If the value is negative,
                  the frequencies are attenuated.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            peaking
          </dt>
          <dd>
            <p>
              The peaking filter allows all frequencies through, but adds a
              boost (or attenuation) to a range of frequencies.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The center frequency of where the boost is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls the width of the band of frequencies that are
                  boosted. A large value implies a narrow width.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  The boost, in dB, to be applied. If the value is negative,
                  the frequencies are attenuated.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            notch
          </dt>
          <dd>
            <p>
              The notch filter (also known as a <a href=
              "https://en.wikipedia.org/wiki/Band-stop_filter">band-stop or
              band-rejection filter</a>) is the opposite of a bandpass filter.
              It allows all frequencies through, except for a set of
              frequencies.
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The center frequency of where the notch is applied.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls the width of the band of frequencies that are
                  attenuated. A large value implies a narrow width.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
              </dl>
            </blockquote>
          </dd>
          <dt>
            allpass
          </dt>
          <dd>
            <p>
              An <a href=
              "https://en.wikipedia.org/wiki/All-pass_filter#Digital_Implementation">
              allpass filter</a> allows all frequencies through, but changes
              the phase relationship between the various frequencies. It
              implements a second-order allpass filter
            </p>
            <blockquote>
              <dl>
                <dt>
                  frequency
                </dt>
                <dd>
                  The frequency where the center of the phase transition
                  occurs. Viewed another way, this is the frequency with
                  maximal <a href=
                  "https://en.wikipedia.org/wiki/Group_delay">group delay</a>.
                </dd>
                <dt>
                  <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a>
                </dt>
                <dd>
                  Controls how sharp the phase transition is at the center
                  frequency. A larger value implies a sharper transition and a
                  larger group delay.
                </dd>
                <dt>
                  gain
                </dt>
                <dd>
                  Not used in this filter type.
                </dd>
              </dl>
            </blockquote>
          </dd>
        </dl>
        <p>
          All attributes of the <a><code>BiquadFilterNode</code></a> are
          <a>a-rate</a> <a><code>AudioParam</code></a>.
        </p>
        <dl title=
        "[Constructor(BaseAudioContext context, optional BiquadFilterOptions options)]interface BiquadFilterNode : AudioNode"
        class="idl">
          <dt>
            attribute BiquadFilterType type
          </dt>
          <dd>
            <p>
              The type of this <a><code>BiquadFilterNode</code></a>. The exact
              meaning of the other parameters depend on the value of the
              <a><code>type</code></a> attribute.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam frequency
          </dt>
          <dd>
            <p>
              The frequency at which the <a><code>BiquadFilterNode</code></a>
              will operate, in Hz. Its default value is 350Hz. It forms a
              <a>compound parameter</a> with <code>detune</code>. Its nominal
              range is [0, <a>Nyquist frequency</a>].
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam detune
          </dt>
          <dd>
            <p>
              A detune value, in cents, for the frequency. Its default value is
              0. It forms a <a>compound parameter</a> with
              <code>frequency</code>. Its nominal range is \((-\infty,
              \infty)\).
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam Q
          </dt>
          <dd>
            <p>
              The <a href="https://en.wikipedia.org/wiki/Q_factor">Q</a> factor
              has a default value of 1. Its <a>nominal range</a> is \((-\infty,
              \infty)\). This is not used for <a href=
              "#idl-def-BiquadFilterType.lowshelf">lowshelf</a> or <a href=
              "#idl-def-BiquadFilterType.highshelf">highshelf</a> filters.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam gain
          </dt>
          <dd>
            <p>
              The gain has a default value of 0. Its <a>nominal range</a> is
              \((-\infty, \infty)\). Its value is in dB units. The gain is only
              used for <a href=
              "#idl-def-BiquadFilterType.lowshelf">lowshelf</a>, <a href=
              "#idl-def-BiquadFilterType.highshelf">highshelf</a>, and <a href=
              "#idl-def-BiquadFilterType.peaking">peaking</a> filters.
            </p>
          </dd>
          <dt>
            void getFrequencyResponse()
          </dt>
          <dd>
            <p>
              <span class="sychronous">Given the current filter parameter
              settings, synchronously calculates the frequency response for the
              specified frequencies.</span> The three parameters MUST be
              <code>Float32Array</code>s of the same length, or an
              <code>InvalidAccessError</code> MUST be thrown.
            </p>
            <p>
              The frequency response returned MUST be computed with the
              <a><code>AudioParam</code></a> sampled for the current processing
              block.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array frequencyHz
              </dt>
              <dd>
                <p>
                  This parameter specifies an array of frequencies at which the
                  response values will be calculated.
                </p>
              </dd>
              <dt>
                Float32Array magResponse
              </dt>
              <dd>
                <p>
                  This parameter specifies an output array receiving the linear
                  magnitude response values.
                </p>
                <p>
                  If a value in the <code>frequencyHz</code> parameter is not
                  within [0; sampleRate/2], where <code>sampleRate</code> is
                  the value of the <a href=
                  "#widl-BaseAudioContext-sampleRate"><code>sampleRate</code></a>
                  property of the <a>AudioContext</a>, the corresponding value
                  at the same index of the <code>magResponse</code> array MUST
                  be <code>NaN</code>.
                </p>
              </dd>
              <dt>
                Float32Array phaseResponse
              </dt>
              <dd>
                <p>
                  This parameter specifies an output array receiving the phase
                  response values in radians.
                </p>
                <p>
                  If a value in the <code>frequencyHz</code> parameter is not
                  within [0; sampleRate/2], where <code>sampleRate</code> is
                  the value of the <a href=
                  "#widl-BaseAudioContext-sampleRate"><code>sampleRate</code></a>
                  property of the <a>AudioContext</a>, the corresponding value
                  at the same index of the <code>phaseResponse</code> array
                  MUST be <code>NaN</code>.
                </p>
              </dd>
            </dl>
          </dd>
        </dl>
        <section>
          <h2>
            BiquadFilterOptions
          </h2>
          <p>
            This specifies the options to be used when constructing a
            <a><code>BiquadFilterNode</code></a>. All members are optional; if
            not specified, the normal default values are used to construct the
            node.
          </p>
          <dl title="dictionary BiquadFilterOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              BiquadFilterType type
            </dt>
            <dd>
              The desired initial type of the filter.
            </dd>
            <dt>
              float Q
            </dt>
            <dd>
              The desired initial value for <a><code>Q</code></a>.
            </dd>
            <dt>
              float detune
            </dt>
            <dd>
              The desired initial value for <a><code>detune</code></a>.
            </dd>
            <dt>
              float frequency
            </dt>
            <dd>
              The desired initial value for <a><code>frequency</code></a>.
            </dd>
            <dt>
              float gain
            </dt>
            <dd>
              The desired initial value for <a><code>gain</code></a>.
            </dd>
          </dl>
        </section>
        <section>
          <h3>
            Filters characteristics
          </h3>
          <p>
            There are multiple ways of implementing the type of filters
            available through the <a><code>BiquadFilterNode</code></a> each
            having very different characteristics. The formulas in this section
            describe the filters that a <a>conforming implementation</a> MUST
            implement, as they determine the characteristics of the different
            filter types. They are inspired by formulas found in the <a href=
            "http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt">Audio EQ
            Cookbook</a>.
          </p>
          <p>
            The transfer function for the filters implemented by the
            <a><code>BiquadFilterNode</code></a> is:
          </p>
          <pre class="nohighlight">
  $$
  H(z) = \frac{\frac{b_0}{a_0} + \frac{b_1}{a_0}z^{-1} + \frac{b_2}{a_0}z^{-2}}
              {1+\frac{a_1}{a_0}z^{-1}+\frac{a_2}{a_0}z^{-2}}
  $$
            
</pre>
          <p>
            The initial filter state is 0.
          </p>The coefficients in the transfer function above are different for
          each node type. The following intermediate variable are necessary for
          their computation, based on the <a>computedValue</a> of the
          <a><code>AudioParam</code></a>s of the
          <a><code>BiquadFilterNode</code></a>.
          <ul>
            <li>Let \(F_s\) be the value of the <a href=
            "#widl-BaseAudioContext-sampleRate"><code>sampleRate</code></a>
            attribute for this <a>AudioContext</a>.
            </li>
            <li>Let \(f_0\) be the value of the
            <a><code>computedFrequency</code></a>.
            </li>
            <li>Let \(G\) be the value of the <a href=
            "#widl-BiquadFilterNode-gain"><code>gain</code></a>
            <a><code>AudioParam</code></a>.
            </li>
            <li>Let \(Q\) be the value of the <a href=
            "#widl-BiquadFilterNode-Q"><code>Q</code></a>
            <a><code>AudioParam</code></a>.
            </li>
            <li>Finally let 
            <!-- Should \alpha_S be simplified since S is always 1?-->
              <pre class="nohighlight">
$$
\begin{align*}
  A        &amp;= 10^{\frac{G}{40}} \\
  \omega_0 &amp;= 2\pi\frac{f_0}{F_s} \\
  \alpha_Q &amp;= \frac{\sin\omega_0}{2Q} \\
  \alpha_{Q_{dB}} &amp;= \frac{\sin\omega_0}{2 \cdot 10^{Q/20}} \\
  S        &amp;= 1 \\
  \alpha_S &amp;= \frac{\sin\omega_0}{2}\sqrt{\left(A+\frac{1}{A}\right)\left(\frac{1}{S}-1\right)+2}
\end{align*}
$$
            
</pre>
            </li>
          </ul>The six coefficients (\(b_0, b_1, b_2, a_0, a_1, a_2\)) for each
          filter type, are:
          <dl>
            <dt>
              <code>lowpass</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  \begin{align*}
                    b_0 &amp;= \frac{1 - \cos\omega_0}{2} \\
                    b_1 &amp;= 1 - \cos\omega_0 \\
                    b_2 &amp;= \frac{1 - \cos\omega_0}{2} \\
                    a_0 &amp;= 1 + \alpha_{Q_{dB}} \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \alpha_{Q_{dB}}
                  \end{align*}
                $$
              
</pre>
            </dd>
            <dt>
              <code>highpass</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                  $$
                    \begin{align*}
                      b_0 &amp;= \frac{1 + \cos\omega_0}{2} \\
                      b_1 &amp;= -(1 + \cos\omega_0) \\
                      b_2 &amp;= \frac{1 + \cos\omega_0}{2} \\
                      a_0 &amp;= 1 + \alpha_{Q_{dB}} \\
                      a_1 &amp;= -2 \cos\omega_0 \\
                      a_2 &amp;= 1 - \alpha_{Q_{dB}}
                    \end{align*}
                  $$
              
</pre>
            </dd>
            <dt>
              <code>bandpass</code>
            </dt>
            <dd>
              <pre class="nohighlight">
              $$
                \begin{align*}
                  b_0 &amp;= \alpha_Q \\
                  b_1 &amp;= 0 \\
                  b_2 &amp;= -\alpha_Q \\
                  a_0 &amp;= 1 + \alpha_Q \\
                  a_1 &amp;= -2 \cos\omega_0 \\
                  a_2 &amp;= 1 - \alpha_Q
                \end{align*}
              $$
            
</pre>
            </dd>
            <dt>
              <code>notch</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  \begin{align*}
                    b_0 &amp;= 1 \\
                    b_1 &amp;= -2\cos\omega_0 \\
                    b_2 &amp;= 1 \\
                    a_0 &amp;= 1 + \alpha_Q \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \alpha_Q
                  \end{align*}
                $$
              
</pre>
            </dd>
            <dt>
              <code>allpass</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  \begin{align*}
                    b_0 &amp;= 1 - \alpha_Q \\
                    b_1 &amp;= -2\cos\omega_0 \\
                    b_2 &amp;= 1 + \alpha_Q \\
                    a_0 &amp;= 1 + \alpha_Q \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \alpha_Q
                  \end{align*}
                $$
              
</pre>
            </dd>
            <dt>
              <code>peaking</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  \begin{align*}
                    b_0 &amp;= 1 + \alpha_Q\, A \\
                    b_1 &amp;= -2\cos\omega_0 \\
                    b_2 &amp;= 1 - \alpha_Q\,A \\
                    a_0 &amp;= 1 + \frac{\alpha_Q}{A} \\
                    a_1 &amp;= -2 \cos\omega_0 \\
                    a_2 &amp;= 1 - \frac{\alpha_Q}{A}
                  \end{align*}
                $$
              
</pre>
            </dd>
            <dt>
              <code>lowshelf</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  \begin{align*}
                    b_0 &amp;= A \left[ (A+1) - (A-1) \cos\omega_0 + 2 \alpha_S \sqrt{A})\right] \\
                    b_1 &amp;= 2 A \left[ (A-1) - (A+1) \cos\omega_0 )\right] \\
                    b_2 &amp;= A \left[ (A+1) - (A-1) \cos\omega_0 - 2 \alpha_S \sqrt{A}) \right] \\
                    a_0 &amp;= (A+1) + (A-1) \cos\omega_0 + 2 \alpha_S \sqrt{A} \\
                    a_1 &amp;= -2 \left[ (A-1) + (A+1) \cos\omega_0\right] \\
                    a_2 &amp;= (A+1) + (A-1) \cos\omega_0 - 2 \alpha_S \sqrt{A})
                  \end{align*}
                $$
              
</pre>
            </dd>
            <dt>
              <code>highshelf</code>
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  \begin{align*}
                    b_0 &amp;= A\left[ (A+1) + (A-1)\cos\omega_0 + 2\alpha_S\sqrt{A} )\right] \\
                    b_1 &amp;= -2A\left[ (A-1) + (A+1)\cos\omega_0 )\right] \\
                    b_2 &amp;= A\left[ (A+1) + (A-1)\cos\omega_0 - 2\alpha_S\sqrt{A} )\right] \\
                    a_0 &amp;= (A+1) - (A-1)\cos\omega_0 + 2\alpha_S\sqrt{A} \\
                    a_1 &amp;= 2\left[ (A-1) - (A+1)\cos\omega_0\right] \\
                    a_2 &amp;= (A+1) - (A-1)\cos\omega_0 - 2\alpha_S\sqrt{A}
                  \end{align*}
                $$
              
</pre>
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The IIRFilterNode Interface
        </h2>
        <p>
          <a><code>IIRFilterNode</code></a> is an <a><code>AudioNode</code></a>
          processor implementing a general IIR Filter. In general, it is best
          to use <a><code>BiquadFilterNode</code></a>'s to implement
          higher-order filters for the following reasons:
        </p>
        <ul>
          <li>Generally less sensitive to numeric issues
          </li>
          <li>Filter parameters can be automated
          </li>
          <li>Can be used to create all even-ordered IIR filters
          </li>
        </ul>
        <p>
          However, odd-ordered filters cannot be created, so if such filters
          are needed or automation is not needed, then IIR filters may be
          appropriate.
        </p>
        <p>
          Once created, the coefficients of the IIR filter cannot be changed.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <dl title=
        "[Constructor(BaseAudioContext context, IIRFilterOptions options)] interface IIRFilterNode : AudioNode"
        class="idl">
          <dt>
            void getFrequencyResponse()
          </dt>
          <dd>
            <p>
              Given the current filter parameter settings, calculates the
              frequency response for the specified frequencies.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array frequencyHz
              </dt>
              <dd>
                This parameter specifies an array of frequencies at which the
                response values will be calculated.
              </dd>
              <dt>
                Float32Array magResponse
              </dt>
              <dd>
                This parameter specifies an output array receiving the linear
                magnitude response values. If this array is shorter than
                <code>frequencyHz</code> a NotSupportedError MUST be signaled.
              </dd>
              <dt>
                Float32Array phaseResponse
              </dt>
              <dd>
                This parameter specifies an output array receiving the phase
                response values in radians. If this array is shorter than
                <code>frequencyHz</code> a NotSupportedError MUST be signaled.
              </dd>
            </dl>
          </dd>
        </dl>
        <section>
          <h2>
            IIRFilterOptions
          </h2>
          <p>
            The <code>IIRFilterOptions</code> dictionary is used to specify the
            filter coefficients of the <a><code>IIRFilterNode</code></a>.
          </p>
          <dl title="dictionary IIRFilterOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              sequence&lt;double&gt; feedforward
            </dt>
            <dd>
              The feedforward coefficients for the
              <a><code>IIRFilterNode</code></a>. This member is required. If
              not specifed, a NotFoundError MUST be thrown.
            </dd>
            <dt>
              sequence&lt;double&gt; feedback
            </dt>
            <dd>
              The feedback coefficients for the
              <a><code>IIRFilterNode</code></a>. This member is required. If
              not specifed, a NotFoundError MUST be thrown.
            </dd>
          </dl>
        </section>
        <section>
          <h3>
            Filter Definition
          </h3>
          <p>
            Let \(b_m\) be the <code>feedforward</code> coefficients and
            \(a_n\) be the <code>feedback</code> coefficients specified by
            <a href=
            "#widl-BaseAudioContext-createIIRFilter-IIRFilterNode-sequence-double--feedforward-sequence-double--feedback">
            createIIRFilter</a>. Then the transfer function of the general IIR
            filter is given by
          </p>
          <pre class="nohighlight">
            $$
              H(z) = \frac{\sum_{m=0}^{M} b_m z^{-m}}{\sum_{n=0}^{N} a_n z^{-n}}
            $$
          
</pre>
          <p>
            where \(M + 1\) is the length of the \(b\) array and \(N + 1\) is
            the length of the \(a\) array. The coefficient \(a_0\) cannot be 0.
            At least one of \(b_m\) must be non-zero.
          </p>
          <p>
            Equivalently, the time-domain equation is:
          </p>
          <pre class="nohighlight">
            $$
              \sum_{k=0}^{N} a_k y(n-k) = \sum_{k=0}^{M} b_k x(n-k)
            $$
          
</pre>
          <p>
            The initial filter state is the all-zeroes state.
          </p>
        </section>
      </section>
      <section>
        <h2 id="WaveShaperNode">
          The WaveShaperNode Interface
        </h2>
        <p>
          <a><code>WaveShaperNode</code></a> is an
          <a><code>AudioNode</code></a> processor implementing non-linear
          distortion effects.
        </p>
        <p>
          Non-linear waveshaping distortion is commonly used for both subtle
          non-linear warming, or more obvious distortion effects. Arbitrary
          non-linear shaping curves may be specified.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the output always equals the number of
          channels of the input.
        </p>
        <dl title="enum OverSampleType" class="idl">
          <dt>
            none
          </dt>
          <dd>
            Don't oversample
          </dd>
          <dt>
            2x
          </dt>
          <dd>
            Oversample two times
          </dd>
          <dt>
            4x
          </dt>
          <dd>
            Oversample four times
          </dd>
        </dl>
        <dl title=
        "[Constructor(BaseAudioContext context, optional WaveShaperOptions options)]interface WaveShaperNode : AudioNode"
        class="idl" data-merge="OverSampleType">
          <dt>
            attribute Float32Array? curve
          </dt>
          <dd>
            <p>
              The shaping curve used for the waveshaping effect. The input
              signal is nominally within the range [-1; 1]. Each input sample
              within this range will index into the shaping curve, with a
              signal level of zero corresponding to the center value of the
              curve array if there are an odd number of entries, or
              interpolated between the two centermost values if there are an
              even number of entries in the array. Any sample value less than
              -1 will correspond to the first value in the curve array. Any
              sample value greater than +1 will correspond to the last value in
              the curve array.
            </p>
            <p>
              The implementation must perform linear interpolation between
              adjacent points in the curve. Initially the curve attribute is
              null, which means that the WaveShaperNode will pass its input to
              its output without modification.
            </p>
            <p>
              Values of the curve are spread with equal spacing in the [-1; 1]
              range. This means that a <a><code>curve</code></a> with a even
              number of value will not have a value for a signal at zero, and a
              <a><code>curve</code></a> with an odd number of value will have a
              value for a signal at zero.
            </p>
            <p>
              A <code>InvalidStateError</code> MUST be thrown if this attribute
              is set with a <code>Float32Array</code> that has a
              <code>length</code> less than 2.
            </p>
            <p>
              When this attribute is set, an internal copy of the curve is
              created by the <a><code>WaveShaperNode</code></a>. Subsequent
              modifications of the contents of the array used to set the
              attribute therefore have no effect: the attribute must be set
              again in order to change the curve.
            </p>
          </dd>
          <dt>
            attribute OverSampleType oversample
          </dt>
          <dd>
            <p>
              Specifies what type of oversampling (if any) should be used when
              applying the shaping curve. The default value is "none", meaning
              the curve will be applied directly to the input samples. A value
              of "2x" or "4x" can improve the quality of the processing by
              avoiding some aliasing, with the "4x" value yielding the highest
              quality. For some applications, it's better to use no
              oversampling in order to get a very precise shaping curve.
            </p>
            <p>
              A value of "2x" or "4x" means that the following steps must be
              performed:
            </p>
            <ol>
              <li>Up-sample the input samples to 2x or 4x the sample-rate of
              the <a><code>AudioContext</code></a>. Thus for each processing
              block of 128 samples, generate 256 (for 2x) or 512 (for 4x)
              samples.
              </li>
              <li>Apply the shaping curve.
              </li>
              <li>Down-sample the result back to the sample-rate of the
              <a><code>AudioContext</code></a>. Thus taking the 256 (or 512)
              processed samples, generating 128 as the final result.
              </li>
            </ol>
            <p>
              The exact up-sampling and down-sampling filters are not
              specified, and can be tuned for sound quality (low aliasing,
              etc.), low latency, and performance.
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            WaveShaperOptions
          </h2>
          <p>
            This specifies the options for constructing a
            <a><code>WaveShaperNode</code></a>. All members are optional; if
            not specified, the normal default is used in constructing the node.
          </p>
          <dl title="dictionary WaveShaperOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              sequence&lt;float&gt; curve
            </dt>
            <dd>
              The shaping curve for the waveshaping effect.
            </dd>
            <dt>
              OverSampleType oversample
            </dt>
            <dd>
              The type of oversampling to use for the shaping curve.
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The OscillatorNode Interface
        </h2>
        <p>
          <a><code>OscillatorNode</code></a> represents an audio source
          generating a periodic waveform. It can be set to a few commonly used
          waveforms. Additionally, it can be set to an arbitrary periodic
          waveform through the use of a <a><code>PeriodicWave</code></a>
          object.
        </p>
        <p>
          Oscillators are common foundational building blocks in audio
          synthesis. An OscillatorNode will start emitting sound at the time
          specified by the <code>start()</code> method.
        </p>
        <p>
          Mathematically speaking, a <em>continuous-time</em> periodic waveform
          can have very high (or infinitely high) frequency information when
          considered in the frequency domain. When this waveform is sampled as
          a discrete-time digital audio signal at a particular sample-rate,
          then care must be taken to discard (filter out) the high-frequency
          information higher than the <a>Nyquist frequency</a> before
          converting the waveform to a digital form. If this is not done, then
          <em>aliasing</em> of higher frequencies (than the <a>Nyquist
          frequency</a>) will fold back as mirror images into frequencies lower
          than the <a>Nyquist frequency</a>. In many cases this will cause
          audibly objectionable artifacts. This is a basic and well understood
          principle of audio DSP.
        </p>
        <p>
          There are several practical approaches that an implementation may
          take to avoid this aliasing. Regardless of approach, the
          <em>idealized</em> discrete-time digital audio signal is well defined
          mathematically. The trade-off for the implementation is a matter of
          implementation cost (in terms of CPU usage) versus fidelity to
          achieving this ideal.
        </p>
        <p>
          It is expected that an implementation will take some care in
          achieving this ideal, but it is reasonable to consider lower-quality,
          less-costly approaches on lower-end hardware.
        </p>
        <p>
          Both <code>frequency</code> and <code>detune</code> are <a>a-rate</a>
          parameters, and form a <a>compound parameter</a>. They are used
          together to determine a <em>computedFrequency</em> value:
        </p>
        <pre>
  computedFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)
        </pre>
        <p>
          The OscillatorNode's instantaneous phase at each time is the time
          integral of <em>computedFrequency</em>. Its <a>nominal range</a> is
          [-<a>Nyquist frequency</a>, <a>Nyquist frequency</a>].
        </p>
        <pre>
  numberOfInputs  : 0
  numberOfOutputs : 1 (mono output)
</pre>
        <dl title="enum OscillatorType" class="idl">
          <dt>
            sine
          </dt>
          <dd>
            A sine wave
          </dd>
          <dt>
            square
          </dt>
          <dd>
            A square wave of duty period 0.5
          </dd>
          <dt>
            sawtooth
          </dt>
          <dd>
            A sawtooth wave
          </dd>
          <dt>
            triangle
          </dt>
          <dd>
            A triangle wave
          </dd>
          <dt>
            custom
          </dt>
          <dd>
            A custom periodic wave
          </dd>
        </dl>
        <dl title=
        "[Constructor(BaseAudioContext context, optional OscillatorOptions options)] interface OscillatorNode : AudioNode"
        class="idl">
          <dt>
            attribute OscillatorType type
          </dt>
          <dd>
            <p>
              The shape of the periodic waveform. It may directly be set to any
              of the type constant values except for "custom". <span class=
              "synchronous">Doing so MUST throw an InvalidStateError
              exception.</span> The <a href=
              "#widl-OscillatorNode-setPeriodicWave-void-PeriodicWave-periodicWave">
              <code>setPeriodicWave()</code></a> method can be used to set a
              custom waveform, which results in this attribute being set to
              "custom". The default value is "sine". When this attribute is
              set, the phase of the oscillator MUST be conserved.
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam frequency
          </dt>
          <dd>
            <p>
              The frequency (in Hertz) of the periodic waveform. Its default
              <code>value</code> is 440. This parameter is <a>a-rate</a>. It
              forms a <a>compound parameter</a> with <code>detune</code>. Its
              nominal range is [-<a>Nyquist frequency</a>, <a>Nyquist
              frequency</a>].
            </p>
          </dd>
          <dt>
            readonly attribute AudioParam detune
          </dt>
          <dd>
            <p>
              A detuning value (in cents) which will offset the
              <a><code>frequency</code></a> by the given amount. Its default
              <code>value</code> is 0. This parameter is <a>a-rate</a>. It
              forms a <a>compound parameter</a> with <code>frequency</code>.
              Its nominal range is \((-\infty, \infty)\).
            </p>
          </dd>
          <dt>
            void start()
          </dt>
          <dd>
            <p>
              Schedules a sound to playback at an exact time. This behaves
              exactly the same as the <a href=
              "#widl-AudioBufferSourceNode-start-void-double-when-double-offset-double-duration">
              <code>start()</code></a> method for an
              <a>AudioBufferSourceNode</a> except the optional
              <code>offset</code> and <code>duration</code> parameters are not
              allowed.
            </p>
            <dl class="parameters">
              <dt>
                optional double when = 0
              </dt>
              <dd>
                Defined the same as the <code>when</code> parameter of the
                <a href=
                "#widl-AudioBufferSourceNode-start-void-double-when-double-offset-double-duration">
                <code>start()</code></a> method of the
                <a>AudioBufferSourceNode</a>.
              </dd>
            </dl>
          </dd>
          <dt>
            void stop()
          </dt>
          <dd>
            <p>
              Schedules a sound to stop playback at an exact time. This behaves
              exactly the same as the <a href=
              "#widl-AudioBufferSourceNode-stop-void-double-when"><code>stop()</code></a>
              method for an <a>AudioBufferSourceNode</a>
            </p>
            <dl class="parameters">
              <dt>
                optional double when = 0
              </dt>
              <dd>
                Defined the same as the <code>when</code> parameter of the
                <a href=
                "#widl-AudioBufferSourceNode-stop-void-double-when"><code>stop()</code></a>
                method of the <a>AudioBufferSourceNode</a>.
              </dd>
            </dl>
          </dd>
          <dt>
            void setPeriodicWave(PeriodicWave periodicWave)
          </dt>
          <dd>
            <p>
              Sets an arbitrary custom periodic waveform given a
              <a><code>PeriodicWave</code></a>.
            </p>
          </dd>
          <dt>
            attribute EventHandler onended
          </dt>
          <dd>
            <p>
              A property used to set the <code>EventHandler</code> (described
              in <cite><a href=
              "https://html.spec.whatwg.org/multipage/webappapis.html#eventhandler">
              HTML</a></cite>[[HTML]]) for the ended event that is dispatched
              to <a>OscillatorNode</a> node types. When the
              <a><code>OscillatorNode</code></a> has finished playing (i.e. its
              stop time has been reached), an event of type <code>Event</code>
              (described in <cite><a href=
              "https://html.spec.whatwg.org/multipage/infrastructure.html#event">
              HTML</a></cite>[[HTML]]) will be dispatched to the event handler.
            </p>
          </dd>
        </dl>
        <section>
          <h2>
            OscillatorOptions
          </h2>
          <p>
            This specifies the options to be used when constructing an
            <a><code>OscillatorNode</code></a>. All of the members are
            optional; if not specified, the normal default values are used for
            constructing the oscillator.
          </p>
          <dl title="dictionary OscillatorOptions : AudioNodeOptions" class=
          "idl">
            <dt>
              OscillatorType type
            </dt>
            <dd>
              The type of oscillator to be constructed. If this is set to
              "custom" without also specifying a
              <a><code>periodicWave</code></a>, then an InvalidStateError
              exception MUST be thrown.
            </dd>
            <dt>
              float frequency
            </dt>
            <dd>
              The initial frequency for the <a><code>OscillatorNode</code></a>.
            </dd>
            <dt>
              float detune
            </dt>
            <dd>
              The initial detune value for the
              <a><code>OscillatorNode</code></a>.
            </dd>
            <dt>
              PeriodicWave periodicWave
            </dt>
            <dd>
              The <a><code>PeriodicWave</code></a> for the
              <a><code>OscillatorNode</code></a>. If this is specified, then
              the <a><code>type</code></a> member must either be unspecified or
              set to "custom". If this is not true, an InvalidStateError
              exception MUST be thrown.
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            Basic Waveform Phase
          </h2>
          <p>
            The idealized mathematical waveforms for the various oscillator
            types are defined here. In summary, all waveforms are defined
            mathematically to be an odd function with a positive slope at time
            0. The actual waveforms produced by the oscillator may differ to
            prevent aliasing affects.
          </p>
          <p>
            The oscillator must produce the same result as if a PeriodicWave
            with the appropriate <a href="#oscillator-coefficients">Fourier
            series</a> and with normalization enabled were used to create these
            basic waveforms.
          </p>
          <dl>
            <dt>
              "sine"
            </dt>
            <dd>
              The waveform for sine oscillator is:
              <pre class="nohighlight">
                $$
                  x(t) = \sin t
                $$.
              
</pre>
            </dd>
            <dt>
              "square"
            </dt>
            <dd>
              The waveform for the square wave oscillator is:
              <pre class="nohighlight">
                $$
                  x(t) = \begin{cases}
                         1 & \mbox{for } 0â‰¤ t &lt; \pi \\
                         -1 & \mbox{for } -\pi &lt; t &lt; 0.
                         \end{cases}
                $$
              
</pre>
            </dd>
            <dt>
              "sawtooth"
            </dt>
            <dd>
              The waveform for the sawtooth oscillator is the ramp:
              <pre class="nohighlight">
                $$
                  x(t) = \frac{t}{\pi} \mbox{ for } -\pi &lt; t â‰¤ \pi;
                $$
              
</pre>
            </dd>
            <dt>
              "triangle"
            </dt>
            <dd>
              The waveform for the triangle oscillator is:
              <pre class="nohighlight">
                $$
                  x(t) = \begin{cases}
                           \frac{2}{\pi} t & \mbox{for } 0 â‰¤ t â‰¤ \frac{\pi}{2} \\
                           1-\frac{2}{\pi} (t-\frac{\pi}{2}) & \mbox{for }
                           \frac{\pi}{2} &lt; t â‰¤ \pi.
                         \end{cases}
                $$
              
</pre>This is extended to all \(t\) by using the fact that the waveform is an
odd function with period \(2\pi\).
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>
          The PeriodicWave Interface
        </h2>
        <p>
          PeriodicWave represents an arbitrary periodic waveform to be used
          with an <a><code>OscillatorNode</code></a>. Please see <a href=
          "#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
          createPeriodicWave()</a> and <a href=
          "#widl-OscillatorNode-setPeriodicWave-void-PeriodicWave-periodicWave">
          setPeriodicWave()</a> and for more details.
        </p>
        <dl title=
        "[Constructor(BaseAudioContext context, PeriodicWaveOptions options)]interface PeriodicWave"
        class="idl"></dl>
        <section>
          <h2>
            PeriodicWaveConstraints
          </h2>The <code>PeriodicWaveConstraints</code> dictionary is used to
          specify how the waveform is normalized.
          <dl title="dictionary PeriodicWaveConstraints" class="idl">
            <dt>
              boolean disableNormalization = false
            </dt>
            <dd>
              Controls whether the periodic wave is normalized or not. If
              <code>true</code>, the waveform is not normalized; otherwise, the
              waveform is normalized.
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            PeriodicWaveOptions
          </h2>
          <p>
            The <code>PeriodicWaveOptions</code> dictionary is used to specify
            how the waveform is constructed. At least one of <code>real</code>
            or <code>imag</code> must be specified. If not given, it is the
            array of all zeroes of the same length as the other.
          </p>
          <dl title="dictionary PeriodicWaveOptions : PeriodicWaveConstraints"
          class="idl">
            <dt>
              sequence&lt;float&gt; real
            </dt>
            <dd>
              The array of <code>cosine</code> terms equivalent to the
              <a><code>real</code></a> parameter to <a href=
              "#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
              <code>createPeriodicWave</code></a>. This defaults to a sequence
              of all zeroes if <a><code>imag</code></a> is given.
            </dd>
            <dt>
              sequence&lt;float&gt; imag
            </dt>
            <dd>
              The array of <code>sine</code> terms terms equivalent to the
              <a><code>imag</code></a> parameter to <a href=
              "#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
              <code>createPeriodicWave</code></a>. This defaults to a sequence
              of all zeroes if <a><code>real</code></a> is given.
            </dd>
          </dl>
        </section>
        <section>
          <h2>
            Waveform Generation
          </h2>
          <p>
            The <a href=
            "#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
            createPeriodicWave()</a> method takes two arrays to specify the
            Fourier coefficients of the PeriodicWave. Let \(a\) and \(b\)
            represent the real and imaginary arrays of length \(L\). Then the
            basic time-domain waveform, \(x(t)\), can be computed using:
          </p>
          <pre class="nohighlight">
            $$
              x(t) = \sum_{k=1}^{L-1} \left(a[k]\cos2\pi k t + b[k]\sin2\pi k t\right)
            $$
          
</pre>
          <p>
            This is the basic (unnormalized) waveform.
          </p>
        </section>
        <section>
          <h2>
            Waveform Normalization
          </h2>
          <p>
            By default, the waveform defined in the previous section is
            normalized so that the maximum value is 1. The normalization is
            done as follows.
          </p>
          <p>
            Let
          </p>
          <pre class="nohighlight">
          $$
            \tilde{x}(n) = \sum_{k=1}^{L-1} \left(a[k]\cos\frac{2\pi k n}{N} + b[k]\sin\frac{2\pi k n}{N}\right)
          $$
          
</pre>
          <p>
            where \(N\) is a power of two. (Note: \(\tilde{x}(n)\) can
            conveniently be computed using an inverse FFT.) The fixed
            normalization factor \(f\) is computed as follows.
          </p>
          <pre class="nohighlight">
            $$
              f = \max_{n = 0, \ldots, N - 1} |\tilde{x}(n)|
            $$
          
</pre>
          <p>
            Thus, the actual normalized waveform \(\hat{x}(n)\) is
          </p>
          <pre class="nohighlight">
            $$
              \hat{x}(n) = \frac{\tilde{x}(n)}{f}
            $$
          
</pre>
          <p>
            This fixed normalization factor must be applied to all generated
            waveforms.
          </p>
        </section>
        <section>
          <h2>
            Oscillator Coefficients
          </h2>
          <p>
            The builtin oscillator types are created using <a>PeriodicWave</a>
            objects. For completeness the coefficients for the PeriodicWave for
            each of the builtin oscillator types is given here. This is useful
            if a builtin type is desired but without the default normalization.
          </p>
          <p>
            In the following descriptions, let \(a\) be the array of real
            coefficients and \(b\) be the array of imaginary coefficients for
            <a href=
            "#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
            createPeriodicWave()</a>. In all cases \(a[n] = 0\) for all \(n\)
            because the waveforms are odd functions. Also, \(b[0] = 0\) in all
            cases. Hence, only \(b[n]\) for \(n \ge 1\) is specified below.
          </p>
          <dl>
            <dt>
              "sine"
            </dt>
            <dd>
              <pre class="nohighlight">
                  $$
                    b[n] = \begin{cases}
                             1 & \mbox{for } n = 1 \\
                             0 & \mbox{otherwise}
                           \end{cases}
                  $$
              
</pre>
            </dd>
            <dt>
              "square"
            </dt>
            <dd>
              <pre class="nohighlight">
                  $$
                    b[n] = \frac{2}{n\pi}\left[1 - (-1)^n\right]
                  $$
              
</pre>
            </dd>
            <dt>
              "sawtooth"
            </dt>
            <dd>
              <pre class="nohighlight">
                $$
                  b[n] = (-1)^{n+1} \dfrac{2}{n\pi}
                $$
            
</pre>
            </dd>
            <dt>
              "triangle"
            </dt>
            <dd>
              <pre class="nohighlight">
                  $$
                    b[n] = \frac{8\sin\dfrac{n\pi}{2}}{(\pi n)^2}
                  $$
              
</pre>
            </dd>
          </dl>
        </section>
      </section>
      <section>
        <h2 id="MediaStreamAudioSourceNode">
          The MediaStreamAudioSourceNode Interface
        </h2>
        <p>
          This interface represents an audio source from a
          <code>MediaStream</code>. The first
          <code>AudioMediaStreamTrack</code> from the <code>MediaStream</code>
          will be used as a source of audio. Those interfaces are described in
          [[!mediacapture-streams]].
        </p>
        <pre>
    numberOfInputs  : 0
    numberOfOutputs : 1
</pre>
        <p>
          The number of channels of the output corresponds to the number of
          channels of the <code>AudioMediaStreamTrack</code>. If there is no
          valid audio track, then the number of channels output will be one
          silent channel.
        </p>
        <dl title="interface MediaStreamAudioSourceNode : AudioNode" class=
        "idl"></dl>
      </section>
      <section>
        <h2>
          The MediaStreamAudioDestinationNode Interface
        </h2>
        <p>
          This interface is an audio destination representing a
          <code>MediaStream</code> with a single
          <code>AudioMediaStreamTrack</code>. This MediaStream is created when
          the node is created and is accessible via the <dfn>stream</dfn>
          attribute. This stream can be used in a similar way as a
          <code>MediaStream</code> obtained via <code>getUserMedia()</code>,
          and can, for example, be sent to a remote peer using the
          <code>RTCPeerConnection</code> (described in [[!webrtc]])
          <code>addStream()</code> method.
        </p>
        <pre>
    numberOfInputs  : 1
    numberOfOutputs : 0

    channelCount = 2;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>
        <p>
          The number of channels of the input is by default 2 (stereo). Any
          connections to the input are up-mixed/down-mixed to the number of
          channels of the input.
        </p>
        <dl title="interface MediaStreamAudioDestinationNode : AudioNode"
        class="idl">
          <dt>
            readonly attribute MediaStream stream
          </dt>
          <dd>
            <p>
              A MediaStream containing a single AudioMediaStreamTrack with the
              same number of channels as the node itself.
            </p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h2>
        Processing model
      </h2>
      <section class="informative">
        <h3>
          Background
        </h3>
        <p>
          Real-time audio systems that require low latency are often
          implemented using <em>callback functions</em>, where the operating
          system calls the program back when more audio has to be computed in
          order for the playback to stay uninterrupted. Such callback is called
          on a high priority thread (often the highest priority on the system).
          This means that a program that deals with audio only executes code
          from this callback, as any buffering between a rendering thread and
          the callback would naturally add latency or make the system less
          resilient to glitches.
        </p>
        <p>
          For this reason, the traditional way of executing asynchronous
          operations on the Web Platform, the event loop, does not work here,
          as the thread is not <em>continuously executing</em>. Additionally, a
          lot of unnecessary and potentially blocking operations are available
          from traditional execution contexts (Windows and Workers), which is
          not something that is desirable to reach an acceptable level of
          performance.
        </p>
        <p>
          Additionally, the Worker model makes creating a dedicated thread
          necessary for a script execution context, while all <a>AudioNode</a>s
          usually share the same execution context.
        </p>
        <p class="note">
          This section specifies how the end result should look like, not how
          it should be implemented. In particular, instead of using message
          queue, implementors can use memory that is shared between threads, as
          long as the memory operations are not reordered.
        </p>
      </section>
      <section>
        <h3>
          Control thread and rendering thread
        </h3>
        <p>
          The Web Audio API MUST be implemented using a <a>control thread</a>,
          and a <a>rendering thread</a>.
        </p>
        <p>
          The <dfn>control thread</dfn> is the thread from which the
          <a>AudioContext</a> is instantiated, and from which authors
          manipulate the audio graph, that is, from where the operation on a
          <a>BaseAudioContext</a> are invoked. The <dfn>rendering thread</dfn>
          is the thread on which the actual audio output is computed, in
          reaction to the calls from the <a>control thread</a>. It can be a
          real-time, callback-based audio thread, if computing audio for an
          <a>AudioContext</a>, or a normal thread if rendering and audio graph
          offline using an <a>OfflineAudioContext</a>.
        </p>
        <p>
          The <a>control thread</a> uses a traditional event loop, as described
          in [[HTML]].
        </p>
        <p>
          The <a>rendering thread</a> uses a specialized rendering loop,
          described in the section <a href="#rendering-audio">Rendering an
          audio graph</a>
        </p>
        <p>
          Communication from the <a>control thread</a> to the <a>rendering
          thread</a> is done using <a>control message</a> passing.
          Communication in the other direction is done using regular event loop
          tasks.
        </p>
        <p>
          Each <a>AudioContext</a> has a single <dfn>control message
          queue</dfn>, that is a list of <dfn data-lt="control message">control
          messages</dfn> that are operations running on the <a>control
          thread</a>.
        </p>
        <p>
          <dfn id="queuing">Queuing a control message</dfn> means adding the
          message to the end of the <a>control message queue</a> of an
          <a>AudioContext</a>.
        </p>
        <p>
          <a>Control messages</a> in a <a>control message queue</a> are ordered
          by time of insertion. The <dfn>oldest message</dfn> is therefore the
          one at the front of the <a>control message queue</a>.
        </p>
        <p>
          <dfn data-lt="swap">Swapping</dfn> a <a>control message queue</a>
          <var>Q<sub>A</sub></var> with another <a>control message queue</a>
          <var>Q<sub>B</sub></var> means executing the following steps:
        </p>
        <ol>
          <li>Let <var>Q<sub>C</sub></var> be a new, empty <a>control message
          queue</a>.
          </li>
          <li>Move all the <a>control messages</a> <var>Q<sub>A</sub></var> to
          <var>Q<sub>C</sub></var>.
          </li>
          <li>Move all the <a>control messages</a> <var>Q<sub>B</sub></var> to
          <var>Q<sub>A</sub></var>.
          </li>
          <li>Move all the <a>control messages</a> <var>Q<sub>C</sub></var> to
          <var>Q<sub>B</sub></var>.
          </li>
        </ol>
        <p class="note">
          For example, successfuly calling <code>start()</code> on an
          <a>AudioBufferSourceNode</a> <code>source</code> adds a <a>control
          message</a> to the <a href="#control-message-queue">control message
          queue</a> of the <a>AudioContext</a> <code>source.context</code>.
        </p>
      </section>
      <section>
        <h3>
          Asynchronous operations
        </h3>
        <p>
          Calling methods on <a>AudioNode</a>s is effectively asynchronous, and
          MUST to be done in two phases, a synchronous part and an asynchronous
          part. For each method, some part of the execution happens on the
          <a>control thread</a> (for example, throwing an exception in case of
          invalid parameters), and some part happens on the <a>rendering
          thread</a> (for example, changing the value of an <a>AudioParam</a>).
        </p>
        <p>
          In the description of each operation on <a>AudioNode</a>s and
          <a>AudioContext</a>s, the synchronous section is marked with a âŒ›. All
          the other operations are executed <a href=
          "https://html.spec.whatwg.org/multipage/infrastructure.html#in-parallel">
          in parallel</a>, as described in [[HTML]].
        </p>
        <p>
          The synchronous section is executed on the <a>control thread</a>, and
          happens immediately. If it fails, the method execution is aborted,
          possibly throwing an exception. If it succeeds, a <a>control
          message</a>, encoding the operation to be executed on the
          <a>rendering thread</a> is enqueued on the <a>control message
          queue</a> of this <a>rendering thread</a>.
        </p>
        <p>
          The synchronous and asynchronous sections order with respect to other
          events MUST be the same: given two operation <var>A</var> and
          <var>B</var> with respective synchronous and asynchronous section
          <var>A<sub>Sync</sub></var> and <var>A<sub>Async</sub></var>, and
          <var>B<sub>Sync</sub></var> and <var>B<sub>Async</sub></var>, if
          <var>A</var> happens before <var>B</var>, then
          <var>A<sub>Sync</sub></var> happens before
          <var>B<sub>Sync</sub></var>, and <var>A<sub>Async</sub></var> happens
          before <var>B<sub>Async</sub></var>. In other words, synchronous and
          asynchronous sections can't be reordered.
        </p>
      </section>
      <section id="rendering-loop">
        <h3>
          Rendering an audio graph
        </h3>
        <p>
          Rendering an audio graph is done in blocks of 128 samples-frames. A
          block of 128 samples-frames is called a <dfn>render quantum</dfn>.
        </p>
        <p>
          Operations that happen <dfn data-lt="atomic">atomically</dfn> on a
          given thread can only be executed when no other <a>atomic</a>
          operation is running on another thread.
        </p>
        <p>
          The algorithm for rendering a block of audio from an
          <a>AudioContext</a> <em>G</em> with a <a href=
          "#control-message-queue">control message queue</a> <em>Q</em> is as
          follows.
        </p>
        <ol>
          <li>Let <em>Q<sub>rendering</sub></em> be an empty <a>control message
          queue</a>. <a>Atomically</a> <a>swap</a>
          <em>Q<sub>rendering</sub></em> and <em>Q</em>
          </li>
          <li>While there are messages in <em>Q<sub>rendering</sub></em>,
          execute the following steps:
            <ol>
              <li>Execute the asynchronous section of the <a>oldest message</a>
              of <em>Q<sub>rendering</sub></em>.
              </li>
              <li>Remove the <a>oldest message</a> of
              <em>Q<sub>rendering</sub></em>.
              </li>
            </ol>
          </li>
          <li>Order the <a>AudioNode</a>s of the <a>AudioContext</a> to be
          processed.
            <ol>
              <li>Let <em>ordered</em> be an empty list of <a>AudioNode</a>s.
              It will contain an ordered list of <a>AudioNode</a>s when this
              ordering algorithm terminates.
              </li>
              <li>Let <em>nodes</em> be the set of all nodes created by this
              <a>AudioContext</a>, and still alive.
              </li>
              <li>Let <em>cycle breakers</em> be an empty set of
              <a>DelayNode</a>s. It will contain all the <a>DelayNode</a>s that
              are part of a cycle.
              </li>
              <li>For each <a>AudioNode</a> <em>node</em> in <em>nodes</em>
                <ol>
                  <li>If <em>node</em> is a <a>DelayNode</a> that is part of a
                  cycle, add it to <em>cycle breakers</em> and remove it from
                  <em>nodes</em>.
                  </li>
                </ol>
              </li>
              <li>If <em>nodes</em> contains cycles, <a href="#mute">mute</a>
              all the <a>AudioNode</a>s that are part of this cycle, and remove
              them from <em>nodes</em>.
              </li>
              <li>While there are unmarked <a>AudioNode</a>s in <em>nodes</em>:
                <ol>
                  <li>Choose an <a>AudioNode</a> <em>node</em> in
                  <em>nodes</em>
                  </li>
                  <li>
                    <a>Visit</a> <em>node</em>.
                  </li>
                </ol><dfn data-lt="Visit">Visiting a node</dfn> <em>node</em>
                mean performing the following steps:
                <ol>
                  <li>If <em>node</em> is marked, abort these steps.
                  </li>
                  <li>Mark <em>node</em>.
                  </li>
                  <li>For each <a>AudioNode</a> <em>input</em> connected to
                  <em>node</em>:
                    <ol>
                      <li>
                        <a>Visit</a> <em>input</em>.
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
              <li>Add <em>node</em> to the beginning of <em>ordered</em>.
              </li>
            </ol>
          </li>
          <li>Reverse the order of <em>nodes</em>.
          </li>
          <li>For each <a>DelayNode</a> in a cycle, <a href="#available">make
          available for reading</a> a block of audio from the <a>DelayNode</a>
          buffer, available for reading.
          </li>
          <li>For each <a>AudioNode</a>, in the order determined previously:
            <ol>
              <li>For each <a>AudioParam</a> of this <a>AudioNode</a>, execute
              these steps:
                <ol>
                  <li>If this <a>AudioParam</a> has any <a>AudioNode</a>
                  connected to it, <a href="#SummingJunction">sum</a> the
                  buffers <a href="#available">made available for reading</a>
                  by all <a>AudioNode</a> connected to this <a>AudioParam</a>,
                  <a href="#down-mix">down mix</a> the resulting buffer down to
                  Mono, and call this buffer the <dfn id=
                  "input-audioparam-buffer">input <a>AudioParam</a>
                  buffer</dfn>.
                  </li>
                  <li>
                    <a href="#computation-of-value">Compute the value(s)</a> of
                    this <a>AudioParam</a> for this block.
                  </li>
                </ol>
              </li>
              <li>If this <a>AudioNode</a> has any <a>AudioNode</a>s connected
              to its input, <a href="#SummingJunction">sum</a> the buffers
              <a href="#available">made available for reading</a> by all
              <a>AudioNode</a>s connected to this <a>AudioNode</a>. The
              resulting buffer is called the <dfn>input buffer</dfn>.
                <a href="#channel-up-mixing-and-down-mixing">Up or down-mix</a>
                it to match if number of input channels of this
                <a>AudioNode</a>.
              </li>
              <li>If this <a>AudioNode</a> is a <a>source node</a>, <a href=
              "computing">compute a block of audio</a>, and <a href=
              "#available">make it available for reading</a>.
              </li>
              <li>Else, if this <a>AudioNode</a> is a <a>destination node</a>,
              <a href="record">record the input</a> of this <a>AudioNode</a>.
              </li>
              <li>Else, <a>process</a> the <a>input buffer</a>, and <a href=
              "#available">make available for reading</a> the resulting buffer.
              </li>
            </ol>
          </li>
          <li>
            <a>Atomically</a> increment <a href=
            "#widl-BaseAudioContext-currentTime">currentTime</a> by 128 /
            <a href="#widl-BaseAudioContext-currentTime">sampleRate</a>.
          </li>
        </ol>
        <ul>
          <li style="list-style: none; display: inline">
            <p>
              <dfn id="mute">Muting</dfn> an <a>AudioNode</a> means that its
              output MUST be silence for the rendering of this audio block.
            </p>
            <p>
              <dfn id="available">Making a buffer available for reading</dfn>
              from an <a>AudioNode</a> means putting it in a state where other
              <a>AudioNode</a>s connected to this <a>AudioNode</a> can safely
              read from it.
            </p>
            <p class="note">
              For example, implementations can choose to allocate a new buffer,
              or have a more elaborate mechanism, reusing an existing buffer
              that is now unused.
            </p>
            <p>
              <dfn id="record">Recording the input</dfn> of an <a>AudioNode</a>
              means copying the input data of this <a>AudioNode</a> for future
              usage.
            </p>
            <p>
              <dfn id="computing">Computing a block of audio</dfn> means
              running the algorithm for this <a>AudioNode</a> to produce 128
              sample-frames.
            </p>
            <p>
              <dfn data-lt="process">Processing an input buffer</dfn> means
              running the algorithm for an <a>AudioNode</a>, using an <a>input
              buffer</a> and the value(s) of the <a>AudioParam</a>(s) of this
              <a>AudioNode</a> as the input for this algorithm.
            </p>
          </li>
        </ul>
      </section>
    </section>
    <section class="informative">
      <h2>
        Mixer Gain Structure
      </h2>
      <h3 id="background">
        Background
      </h3>
      <p>
        One of the most important considerations when dealing with audio
        processing graphs is how to adjust the gain (volume) at various points.
        For example, in a standard mixing board model, each input bus has
        pre-gain, post-gain, and send-gains. Submix and master out busses also
        have gain control. The gain control described here can be used to
        implement standard mixing boards as well as other architectures.
      </p>
      <section>
        <h3 id="SummingJunction">
          Summing Inputs
        </h3>
        <p>
          The inputs to <a><code>AudioNode</code></a>s have the ability to
          accept connections from multiple outputs. The input then acts as a
          unity gain summing junction with each output signal being added with
          the others:
        </p>
        <figure>
          <img alt="unity gain summing junction" src=
          "images/unity-gain-summing-junction.png">
          <figcaption>
            A graph showing Source 1 and Source 2 output summed at the input of
            Destination
          </figcaption>
        </figure>
        <p>
          In cases where the channel layouts of the outputs do not match, a mix
          (usually up-mix) will occur according to the <a href=
          "#channel-up-mixing-and-down-mixing">mixing rules</a>.
        </p>
        <p>
          No clipping is applied at the inputs or outputs of the
          <a><code>AudioNode</code></a> to allow a maximum of dynamic range
          within the audio graph.
        </p>
      </section>
      <section>
        <h3>
          Gain Control
        </h3>
        <p>
          In many scenarios, it's important to be able to control the gain for
          each of the output signals. The <a><code>GainNode</code></a> gives
          this control:
        </p>
        <figure>
          <img alt="mixer architecture new" src=
          "images/mixer-architecture-new.png">
          <figcaption>
            A graph featuring volume control for each voice
          </figcaption>
        </figure>
        <p>
          Using these two concepts of unity gain summing junctions and
          GainNodes, it's possible to construct simple or complex mixing
          scenarios.
        </p>
      </section>
      <section>
        <h3 id="Example-mixer-with-send-busses">
          Example: Mixer with Send Busses
        </h3>
        <p>
          In a routing scenario involving multiple sends and submixes, explicit
          control is needed over the volume or "gain" of each connection to a
          mixer. Such routing topologies are very common and exist in even the
          simplest of electronic gear sitting around in a basic recording
          studio.
        </p>
        <p>
          Here's an example with two send mixers and a main mixer. Although
          possible, for simplicity's sake, pre-gain control and insert effects
          are not illustrated:
        </p>
        <figure>
          <img alt="mixer gain structure" src=
          "images/mixer-gain-structure.png">
          <figcaption>
            A graph showing a full mixer with send busses.
          </figcaption>
        </figure>
        <p>
          This diagram is using a shorthand notation where "send 1", "send 2",
          and "main bus" are actually inputs to <a><code>AudioNode</code></a>s,
          but here are represented as summing busses, where the intersections
          g2_1, g3_1, etc. represent the "gain" or volume for the given source
          on the given mixer. In order to expose this gain, an
          <a><code>GainNode</code></a> is used:
        </p>
        <p>
          Here's how the above diagram could be constructed in JavaScript:
        </p>
        <pre class="example">

  var context = 0;
  var compressor = 0;
  var reverb = 0;
  var delay = 0;
  var s1 = 0;
  var s2 = 0;

  var source1 = 0;
  var source2 = 0;
  var g1_1 = 0;
  var g2_1 = 0;
  var g3_1 = 0;
  var g1_2 = 0;
  var g2_2 = 0;
  var g3_2 = 0;

  // Setup routing graph
  function setupRoutingGraph() {
      context = new AudioContext();

      compressor = context.createDynamicsCompressor();

      // Send1 effect
      reverb = context.createConvolver();
      // Convolver impulse response may be set here or later

      // Send2 effect
      delay = context.createDelay();

      // Connect final compressor to final destination
      compressor.connect(context.destination);

      // Connect sends 1 & 2 through effects to main mixer
      s1 = context.createGain();
      reverb.connect(s1);
      s1.connect(compressor);

      s2 = context.createGain();
      delay.connect(s2);
      s2.connect(compressor);

      // Create a couple of sources
      source1 = context.createBufferSource();
      source2 = context.createBufferSource();
      source1.buffer = manTalkingBuffer;
      source2.buffer = footstepsBuffer;

      // Connect source1
      g1_1 = context.createGain();
      g2_1 = context.createGain();
      g3_1 = context.createGain();
      source1.connect(g1_1);
      source1.connect(g2_1);
      source1.connect(g3_1);
      g1_1.connect(compressor);
      g2_1.connect(reverb);
      g3_1.connect(delay);

      // Connect source2
      g1_2 = context.createGain();
      g2_2 = context.createGain();
      g3_2 = context.createGain();
      source2.connect(g1_2);
      source2.connect(g2_2);
      source2.connect(g3_2);
      g1_2.connect(compressor);
      g2_2.connect(reverb);
      g3_2.connect(delay);

      // We now have explicit control over all the volumes g1_1, g2_1, ..., s1, s2
      g2_1.gain.value = 0.2;  // For example, set source1 reverb gain

      // Because g2_1.gain is an "AudioParam",
      // an automation curve could also be attached to it.
      // A "mixing board" UI could be created in canvas or WebGL controlling these gains.
  }

   
</pre>
      </section>
    </section>
    <section>
      <h2 id="DynamicLifetime">
        Dynamic Lifetime
      </h2>
      <section>
        <h3>
          Background
        </h3>
        <p class="norm">
          <em>This section is non-normative. Please see <a href=
          "#lifetime-AudioContext">AudioContext lifetime</a> and <a href=
          "#lifetime-AudioNode">AudioNode lifetime</a> for normative
          requirements.</em>
        </p>
        <p>
          In addition to allowing the creation of static routing
          configurations, it should also be possible to do custom effect
          routing on dynamically allocated voices which have a limited
          lifetime. For the purposes of this discussion, let's call these
          short-lived voices "notes". Many audio applications incorporate the
          ideas of notes, examples being drum machines, sequencers, and 3D
          games with many one-shot sounds being triggered according to game
          play.
        </p>
        <p>
          In a traditional software synthesizer, notes are dynamically
          allocated and released from a pool of available resources. The note
          is allocated when a MIDI note-on message is received. It is released
          when the note has finished playing either due to it having reached
          the end of its sample-data (if non-looping), it having reached a
          sustain phase of its envelope which is zero, or due to a MIDI
          note-off message putting it into the release phase of its envelope.
          In the MIDI note-off case, the note is not released immediately, but
          only when the release envelope phase has finished. At any given time,
          there can be a large number of notes playing but the set of notes is
          constantly changing as new notes are added into the routing graph,
          and old ones are released.
        </p>
        <p>
          The audio system automatically deals with tearing-down the part of
          the routing graph for individual "note" events. A "note" is
          represented by an <a><code>AudioBufferSourceNode</code></a>, which
          can be directly connected to other processing nodes. When the note
          has finished playing, the context will automatically release the
          reference to the <a><code>AudioBufferSourceNode</code></a>, which in
          turn will release references to any nodes it is connected to, and so
          on. The nodes will automatically get disconnected from the graph and
          will be deleted when they have no more references. Nodes in the graph
          which are long-lived and shared between dynamic voices can be managed
          explicitly. Although it sounds complicated, this all happens
          automatically with no extra JavaScript handling required.
        </p>
      </section>
      <section>
        <h3>
          Example
        </h3>
        <figure>
          <img alt="dynamic allocation" src="images/dynamic-allocation.png">
          <figcaption>
            A graph featuring a subgraph that will be releases early.
          </figcaption>
        </figure>
        <p>
          The low-pass filter, panner, and second gain nodes are directly
          connected from the one-shot sound. So when it has finished playing
          the context will automatically release them (everything within the
          dotted line). If there are no longer any JavaScript references to the
          one-shot sound and connected nodes, then they will be immediately
          removed from the graph and deleted. The streaming source, has a
          global reference and will remain connected until it is explicitly
          disconnected. Here's how it might look in JavaScript:
        </p>
        <pre class="example">

var context = 0;
var compressor = 0;
var gainNode1 = 0;
var streamingAudioSource = 0;

// Initial setup of the "long-lived" part of the routing graph
function setupAudioContext() {
    context = new AudioContext();

    compressor = context.createDynamicsCompressor();
    gainNode1 = context.createGain();

    // Create a streaming audio source.
    var audioElement = document.getElementById('audioTagID');
    streamingAudioSource = context.createMediaElementSource(audioElement);
    streamingAudioSource.connect(gainNode1);

    gainNode1.connect(compressor);
    compressor.connect(context.destination);
}

// Later in response to some user action (typically mouse or key event)
// a one-shot sound can be played.
function playSound() {
    var oneShotSound = context.createBufferSource();
    oneShotSound.buffer = dogBarkingBuffer;

    // Create a filter, panner, and gain node.
    var lowpass = context.createBiquadFilter();
    var panner = context.createPanner();
    var gainNode2 = context.createGain();

    // Make connections
    oneShotSound.connect(lowpass);
    lowpass.connect(panner);
    panner.connect(gainNode2);
    gainNode2.connect(compressor);

    // Play 0.75 seconds from now (to play immediately pass in 0)
    oneShotSound.start(context.currentTime + 0.75);
}
</pre>
      </section>
    </section>
    <section>
      <h2>
        Channel up-mixing and down-mixing
      </h2>
      <p class="norm">
        This section is normative.
      </p>
      <p>
        <a href="#mixer-gain-structure"></a> describes how an input to an
        <a><code>AudioNode</code></a> can be connected from one or more outputs
        of an <a><code>AudioNode</code></a>. Each of these connections from an
        output represents a stream with a specific non-zero number of channels.
        An input has <em>mixing rules</em> for combining the channels from all
        of the connections to it. As a simple example, if an input is connected
        from a mono output and a stereo output, then the mono connection will
        usually be up-mixed to stereo and summed with the stereo connection.
        But, of course, it's important to define the exact <em>mixing
        rules</em> for every input to every <a><code>AudioNode</code></a>. The
        default mixing rules for all of the inputs have been chosen so that
        things "just work" without worrying too much about the details,
        especially in the very common case of mono and stereo streams. Of
        course, the rules can be changed for advanced use cases, especially
        multi-channel.
      </p>
      <p>
        To define some terms, <em>up-mixing</em> refers to the process of
        taking a stream with a smaller number of channels and converting it to
        a stream with a larger number of channels. <em>down-mixing</em> refers
        to the process of taking a stream with a larger number of channels and
        converting it to a stream with a smaller number of channels.
      </p>
      <p>
        An <a><code>AudioNode</code></a> input use three basic pieces of
        information to determine how to mix all the outputs connected to it. As
        part of this process it computes an internal value
        <dfn><code>computedNumberOfChannels</code></dfn> representing the
        actual number of channels of the input at any given time:
      </p>
      <p>
        The <a><code>AudioNode</code></a> attributes involved in channel
        up-mixing and down-mixing rules are defined <a href=
        "#the-audionode-interface">above</a>. The following is a more precise
        specification on what each of them mean.
      </p>
      <ul>
        <li>
          <a href="#widl-AudioNode-channelCount"><code>channelCount</code></a>
          is used to help compute <a><code>computedNumberOfChannels</code></a>.
        </li>
        <li>
          <a href=
          "#widl-AudioNode-channelCountMode"><code>channelCountMode</code></a>
          determines how <a><code>computedNumberOfChannels</code></a> will be
          computed. Once this number is computed, all of the connections will
          be up or down-mixed to that many channels. For most nodes, the
          default value is <a href=
          "#idl-def-ChannelCountMode.max"><code>"max"</code></a>.
          <ul>
            <li>
              <a href="#idl-def-ChannelCountMode.max"><code>"max"</code></a>:
              <a><code>computedNumberOfChannels</code></a> is computed as the
              maximum of the number of channels of all connections. In this
              mode <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a> is
              ignored.
            </li>
            <li>
              <a href=
              "#idl-def-ChannelCountMode.clamped-max"><code>"clamped-max"</code></a>:
              same as â€œmaxâ€ up to a limit of the <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a>
            </li>
            <li>
              <a href=
              "#idl-def-ChannelCountMode.explicit"><code>"explicit"</code></a>:
              <a><code>computedNumberOfChannels</code></a> is the exact value
              as specified in <a href=
              "#widl-AudioNode-channelCount"><code>channelCount</code></a>
            </li>
          </ul>
        </li>
        <li>
          <a href=
          "#widl-AudioNode-channelInterpretation"><code>channelInterpretation</code></a>
          determines how the individual channels will be treated. For example,
          will they be treated as speakers having a specific layout, or will
          they be treated as simple discrete channels? This value influences
          exactly how the up and down mixing is performed. The default value is
          "speakers".
          <ul>
            <li>
              <a href=
              "#idl-def-ChannelInterpretation.speakers"><code>â€œspeakersâ€</code></a>:
              use <a href="#ChannelLayouts">up-down-mix equations for
              mono/stereo/quad/5.1</a>. In cases where the number of channels
              do not match any of these basic speaker layouts, revert to
              "discrete".
            </li>
            <li>
              <a href=
              "#idl-def-ChannelInterpretation.discrete"><code>â€œdiscreteâ€</code></a>:
              up-mix by filling channels until they run out then zero out
              remaining channels. down-mix by filling as many channels as
              possible, then dropping remaining channels
            </li>
          </ul>
        </li>
      </ul>
      <p>
        For each input of an <a><code>AudioNode</code></a>, an implementation
        must:
      </p>
      <ol>
        <li>Compute <a><code>computedNumberOfChannels</code></a>.
        </li>
        <li>For each connection to the input:
          <ul>
            <li>up-mix or down-mix the connection to
            <a><code>computedNumberOfChannels</code></a> according to
              <a href="#widl-AudioNode-channelInterpretation"><code>channelInterpretation</code></a>.
            </li>
            <li>Mix it together with all of the other mixed streams (from other
            connections). This is a straight-forward mixing together of each of
            the corresponding channels from each connection.
            </li>
          </ul>
        </li>
      </ol>
      <section>
        <h3 id="ChannelLayouts">
          Speaker Channel Layouts
        </h3>
        <p>
          When <a href=
          "#widl-AudioNode-channelInterpretation"><code>channelInterpretation</code></a>
          is <a href="#idl-def-ChannelInterpretation.speakers">"speakers"</a>
          then the up-mixing and down-mixing is defined for specific channel
          layouts.
        </p>
        <p>
          Mono (one channel), stereo (two channels), quad (four channels), and
          5.1 (six channels) MUST be supported. Other channel layout may be
          supported in future version of this specification.
        </p>
      </section>
      <section>
        <h4 id="ChannelOrdering">
          Channel ordering
        </h4>
        <pre>
    Mono
      0: M: mono

    Stereo
      0: L: left
      1: R: right
    
</pre>
        <pre>
  Quad
      0: L:  left
      1: R:  right
      2: SL: surround left
      3: SR: surround right

    5.1
      0: L:   left
      1: R:   right
      2: C:   center
      3: LFE: subwoofer
      4: SL:  surround left
      5: SR:  surround right
  
</pre>
      </section>
      <section>
        <h4 id="UpMix-sub">
          Up Mixing speaker layouts
        </h4>
        <pre>
Mono up-mix:

    1 -&gt; 2 : up-mix from mono to stereo
        output.L = input;
        output.R = input;

    1 -&gt; 4 : up-mix from mono to quad
        output.L = input;
        output.R = input;
        output.SL = 0;
        output.SR = 0;

    1 -&gt; 5.1 : up-mix from mono to 5.1
        output.L = 0;
        output.R = 0;
        output.C = input; // put in center channel
        output.LFE = 0;
        output.SL = 0;
        output.SR = 0;

Stereo up-mix:

    2 -&gt; 4 : up-mix from stereo to quad
        output.L = input.L;
        output.R = input.R;
        output.SL = 0;
        output.SR = 0;

    2 -&gt; 5.1 : up-mix from stereo to 5.1
        output.L = input.L;
        output.R = input.R;
        output.C = 0;
        output.LFE = 0;
        output.SL = 0;
        output.SR = 0;

Quad up-mix:

    4 -&gt; 5.1 : up-mix from quad to 5.1
        output.L = input.L;
        output.R = input.R;
        output.C = 0;
        output.LFE = 0;
        output.SL = input.SL;
        output.SR = input.SR;
</pre>
      </section>
      <section>
        <h4 id="down-mix">
          Down Mixing speaker layouts
        </h4>
        <p>
          A down-mix will be necessary, for example, if processing 5.1 source
          material, but playing back stereo.
        </p>
        <pre>
  Mono down-mix:

      2 -&gt; 1 : stereo to mono
          output = 0.5 * (input.L + input.R);

      4 -&gt; 1 : quad to mono
          output = 0.25 * (input.L + input.R + input.SL + input.SR);

      5.1 -&gt; 1 : 5.1 to mono
          output = sqrt(0.5) * (input.L + input.R) + input.C + 0.5 * (input.SL + input.SR)


  Stereo down-mix:

      4 -&gt; 2 : quad to stereo
          output.L = 0.5 * (input.L + input.SL);
          output.R = 0.5 * (input.R + input.SR);

      5.1 -&gt; 2 : 5.1 to stereo
          output.L = L + sqrt(0.5) * (input.C + input.SL)
          output.R = R + sqrt(0.5) * (input.C + input.SR)

  Quad down-mix:

      5.1 -&gt; 4 : 5.1 to quad
          output.L = L + sqrt(0.5) * input.C
          output.R = R + sqrt(0.5) * input.C
          output.SL = input.SL
          output.SR = input.SR

  
</pre>
      </section>
      <section class="informative">
        <h3 id="ChannelRules-section">
          Channel Rules Examples
        </h3>
        <pre class="example">
  // Set gain node to explicit 2-channels (stereo).
  gain.channelCount = 2;
  gain.channelCountMode = "explicit";
  gain.channelInterpretation = "speakers";

  // Set "hardware output" to 4-channels for DJ-app with two stereo output busses.
  context.destination.channelCount = 4;
  context.destination.channelCountMode = "explicit";
  context.destination.channelInterpretation = "discrete";

  // Set "hardware output" to 8-channels for custom multi-channel speaker array
  // with custom matrix mixing.
  context.destination.channelCount = 8;
  context.destination.channelCountMode = "explicit";
  context.destination.channelInterpretation = "discrete";

  // Set "hardware output" to 5.1 to play an HTMLAudioElement.
  context.destination.channelCount = 6;
  context.destination.channelCountMode = "explicit";
  context.destination.channelInterpretation = "speakers";

  // Explicitly down-mix to mono.
  gain.channelCount = 1;
  gain.channelCountMode = "explicit";
  gain.channelInterpretation = "speakers";
  
</pre>
      </section>
    </section>
    <section>
      <h2 id="audio-sample-values">
        Audio Signal Values
      </h2>
      <p>
        The nominal range of all audio signals at a destination node of any
        audio graph is [-1, 1]. The audio rendition of signal values outside
        this range, or of the values <code>NaN</code>, positive infinity or
        negative infinity, is undefined by this specification.
      </p>
    </section>
    <section>
      <h2 id="Spatialization">
        <dfn>Spatialization/Panning</dfn>
      </h2>
      <section>
        <h3 id="Spatialization-background">
          Background
        </h3>
        <p>
          A common feature requirement for modern 3D games is the ability to
          dynamically spatialize and move multiple audio sources in 3D space.
          Game audio engines such as OpenAL, FMOD, Creative's EAX, Microsoft's
          XACT Audio, etc. have this ability.
        </p>
        <p>
          Using an <a><code>PannerNode</code></a>, an audio stream can be
          spatialized or positioned in space relative to an
          <a><code>AudioListener</code></a>. An
          <a><code>AudioContext</code></a> will contain a single
          <a><code>AudioListener</code></a>. Both panners and listeners have a
          position in 3D space using a right-handed cartesian coordinate
          system. The units used in the coordinate system are not defined, and
          do not need to be because the effects calculated with these
          coordinates are independent/invariant of any particular units such as
          meters or feet. <a><code>PannerNode</code></a> objects (representing
          the source stream) have an <em>orientation</em> vector representing
          in which direction the sound is projecting. Additionally, they have a
          <em>sound cone</em> representing how directional the sound is. For
          example, the sound could be omnidirectional, in which case it would
          be heard anywhere regardless of its orientation, or it can be more
          directional and heard only if it is facing the listener.
          <a><code>AudioListener</code></a> objects (representing a person's
          ears) have an <em>orientation</em> and <em>up</em> vector
          representing in which direction the person is facing. Because both
          the source stream and the listener can be moving, they both have a
          <em>velocity</em> vector representing both the speed and direction of
          movement. Taken together, these two velocities can be used to
          generate a doppler shift effect which changes the pitch.
        </p>
        <p>
          During rendering, the <a><code>PannerNode</code></a> calculates an
          <em>azimuth</em> and <em>elevation</em>. These values are used
          internally by the implementation in order to render the
          spatialization effect. See the <a href=
          "#Spatialization-panning-algorithm">Panning Algorithm</a> section for
          details of how these values are used.
        </p>
      </section>
      <section id="azimuth-elevation">
        <h3>
          Azimuth and Elevation
        </h3>
        <p>
          The following algorithm must be used to calculate the
          <em>azimuth</em> and <em>elevation</em>: for the
          <a><code>PannerNode</code></a>
        </p>
        <pre>
  // Calculate the source-listener vector.
  vec3 sourceListener = source.position - listener.position;

  if (sourceListener.isZero()) {
      // Handle degenerate case if source and listener are at the same point.
      azimuth = 0;
      elevation = 0;
      return;
  }

  sourceListener.normalize();

  // Align axes.
  vec3 listenerFront = listener.orientation;
  vec3 listenerUp = listener.up;
  vec3 listenerRight = listenerFront.cross(listenerUp);
  listenerRight.normalize();

  vec3 listenerFrontNorm = listenerFront;
  listenerFrontNorm.normalize();

  vec3 up = listenerRight.cross(listenerFrontNorm);

  float upProjection = sourceListener.dot(up);

  vec3 projectedSource = sourceListener - upProjection * up;
  projectedSource.normalize();

  azimuth = 180 * acos(projectedSource.dot(listenerRight)) / PI;

  // Source in front or behind the listener.
  float frontBack = projectedSource.dot(listenerFrontNorm);
  if (frontBack &lt; 0)
      azimuth = 360 - azimuth;

  // Make azimuth relative to "front" and not "right" listener vector.
  if ((azimuth &gt;= 0) &amp;& (azimuth &lt;= 270))
      azimuth = 90 - azimuth;
  else
      azimuth = 450 - azimuth;

  elevation = 90 - 180 * acos(sourceListener.dot(up)) / PI;

  if (elevation &gt; 90)
      elevation = 180 - elevation;
  else if (elevation &lt; -90)
      elevation = -180 - elevation;
  
</pre>
      </section>
      <section>
        <h3 id="Spatialization-panning-algorithm">
          Panning Algorithm
        </h3>
        <p>
          <em>Mono-to-stereo</em> and <em>stereo-to-stereo</em> panning must be
          supported. <em>Mono-to-stereo</em> processing is used when all
          connections to the input are mono. Otherwise
          <em>stereo-to-stereo</em> processing is used.
        </p>
        <section>
          <h4 id="Spatialzation-equal-power-panning">
            Equal-power panning
          </h4>
          <p>
            This is a simple and relatively inexpensive algorithm which
            provides basic, but reasonable results. It is used for the
            <a><code>StereoPannerNode</code></a>, and for the
            <a><code>PannerNode</code></a> when the <a href=
            "#widl-PannerNode-panningModel"><code>panningModel</code></a>
            attribute is set to <code>"equalpower"</code>, in which case the
            the <em>elevation</em> value is ignored.
          </p>
          <p>
            For a <a><code>PannerNode</code></a>, the following algorithm MUST
            be implemented.
          </p>
          <ol>
            <li>
              <p>
                Let <em>azimuth</em> be the value computed in the <a href=
                "#azimuth-elevation">azimuth and elevation</a> section.
              </p>
            </li>
            <li>
              <p>
                The <em>azimuth</em> value is first contained to be within the
                range [-90, 90] according to:
              </p>
              <pre>
  // First, clamp azimuth to allowed range of [-180, 180].
  azimuth = max(-180, azimuth);
  azimuth = min(180, azimuth);

  // Then wrap to range [-90, 90].
  if (azimuth &lt; -90)
    azimuth = -180 - azimuth;
  else if (azimuth &gt; 90)
    azimuth = 180 - azimuth;
        
</pre>
            </li>
            <li>
              <p>
                A normalized value <em>x</em> is calculated from
                <em>azimuth</em> for a mono input as:
              </p>
              <pre>
  x = (azimuth + 90) / 180;
        
</pre>
              <p>
                Or for a stereo input as:
              </p>
              <pre>
  if (azimuth &lt;= 0) { // -90 ~ 0
    // Transform the azimuth value from [-90, 0] degrees into the range [-90, 90].
    x = (azimuth + 90) / 90;
  } else { // 0 ~ 90
    // Transform the azimuth value from [0, 90] degrees into the range [-90, 90].
    x = azimuth / 90;
  }
        
</pre>
            </li>
          </ol>
          <p>
            For a <a><code>StereoPannerNode</code></a>, the following algorithm
            MUST be implemented.
          </p>
          <ol>
            <li>
              <p>
                Let <em>pan</em> be the <a>computedValue</a> of the
                <code>pan</code> <a>AudioParam</a> of this
                <a><code>StereoPannerNode</code></a>.
              </p>
            </li>
            <li>
              <p>
                Clamp <em>pan</em> to [-1, 1].
              </p>
              <pre>
pan = max(-1, pan);
pan = min(1, pan);
        
</pre>
            </li>
            <li>
              <p>
                Calculate <em>x</em> by normalizing <em>pan</em> value to [0,
                1]. For mono input:
              </p>
              <pre>
x = (pan + 1) / 2;
        
</pre>
              <p>
                For stereo input:
              </p>
              <pre>
if (pan &lt;= 0)
  x = pan + 1;
else
  x = pan;
        
</pre>
            </li>
          </ol>
          <p>
            Then following steps are used to achieve equal-power panning:
          </p>
          <ol>
            <li>
              <p>
                Left and right gain values are calculated as:
              </p>
              <pre>
gainL = cos(x * Math.PI / 2);
gainR = sin(x * Math.PI / 2);
        
</pre>
            </li>
            <li>
              <p>
                For mono input, the stereo output is calculated as:
              </p>
              <pre>
outputL = input * gainL;
outputR = input * gainR;
        
</pre>
              <p>
                Else for stereo input, the output is calculated as:
              </p>
              <pre>
if (pan &lt;= 0) {
  // Pass through inputL to outputL and equal-power pan inputR as in mono case.
  outputL = inputL + inputR * gainL;
  outputR = inputR * gainR;
} else {
  // Pass through inputR to outputR and equal-power pan inputL as in mono case.
  outputL = inputL * gainL;
  outputR = inputR + inputL * gainR;
}
        
</pre>
            </li>
          </ol>
        </section>
        <section>
          <h4>
            HRTF panning (stereo only)
          </h4>
          <p>
            This requires a set of <a href=
            "https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a>
            (Head-related Transfer Function) impulse responses recorded at a
            variety of azimuths and elevations. The implementation requires a
            highly optimized convolution function. It is somewhat more costly
            than "equalpower", but provides more perceptually spatialized
            sound.
          </p>
          <figure>
            <img alt="HRTF panner" src="images/HRTF_panner.png">
            <figcaption>
              A diagram showing the process of panning a source using HRTF.
            </figcaption>
          </figure>
        </section>
      </section>
      <section>
        <h3 id="Spatialization-distance-effects">
          Distance Effects
        </h3>
        <p>
          Sounds which are closer are louder, while sounds further away are
          quieter. Exactly <em>how</em> a sound's volume changes according to
          distance from the listener depends on the <em>distanceModel</em>
          attribute.
        </p>
        <p>
          During audio rendering, a <em>distance</em> value will be calculated
          based on the panner and listener positions according to:
        </p>
        <pre>
  function dotProduct(v1, v2) {
    var d = 0;
    for (var i = 0; i &lt; Math.min(v1.length, v2.length); i++)
      d += v1[i] * v2[i];
    return d;
  }
  var v = panner.position - listener.position;
  var distance = Math.sqrt(dotProduct(v, v));
  
</pre>
        <p>
          <em>distance</em> will then be used to calculate
          <em>distanceGain</em> which depends on the <em>distanceModel</em>
          attribute. See the <a href=
          "#idl-def-DistanceModelType">distanceModel</a> section for details of
          how this is calculated for each distance model. The value computed by
          the <a href="#idl-def-DistanceModelType">distanceModel</a> equations
          are to be clamped to [0, 1].
        </p>
        <p>
          As part of its processing, the <a><code>PannerNode</code></a>
          scales/multiplies the input audio signal by <em>distanceGain</em> to
          make distant sounds quieter and nearer ones louder.
        </p>
      </section>
      <section>
        <h3 id="Spatialization-sound-cones">
          Sound Cones
        </h3>
        <p>
          The listener and each sound source have an orientation vector
          describing which way they are facing. Each sound source's sound
          projection characteristics are described by an inner and outer "cone"
          describing the sound intensity as a function of the source/listener
          angle from the source's orientation vector. Thus, a sound source
          pointing directly at the listener will be louder than if it is
          pointed off-axis. Sound sources can also be omni-directional.
        </p>
        <p>
          The following algorithm must be used to calculate the gain
          contribution due to the cone effect, given the source (the
          <a><code>PannerNode</code></a>) and the listener:
        </p>
        <pre>
function dotProduct(v1, v2) {
  var d = 0;
  for (var i = 0; i &lt; Math.min(v1.length, v2.length); i++)
    d += v1[i] * v2[i];
  return d;
}

function diff(v1, v2) {
  var v = [];
  for (var i = 0; i & lt; Math.min(v1.length, v2.length); i++)
    v[i] = v1[i] - v2[i];
  return v;
}

function coneGain() {
  if (dotProduct(source.orientation, source.orientation) == 0 || ((source.coneInnerAngle ==
      360) &amp;& (source.coneOuterAngle == 360)))
    return 1; // no cone specified - unity gain

  // Normalized source-listener vector
  var sourceToListener = diff(listener.position, source.position);
  sourceToListener.normalize();

  var normalizedSourceOrientation = source.orientation;
  normalizedSourceOrientation.normalize();

  // Angle between the source orientation vector and the source-listener vector
  var dotProduct = dotProduct(sourceToListener, normalizedSourceOrientation);
  var angle = 180 * Math.acos(dotProduct) / Math.PI;
  var absAngle = Math.abs(angle);

  // Divide by 2 here since API is entire angle (not half-angle)
  var absInnerAngle = Math.abs(source.coneInnerAngle) / 2;
  var absOuterAngle = Math.abs(source.coneOuterAngle) / 2;
  var gain = 1;

  if (absAngle &lt;= absInnerAngle) {
    // No attenuation
    gain = 1;
  } else if (absAngle &gt;= absOuterAngle) {
    // Max attenuation
    gain = source.coneOuterGain;
  } else {
    // Between inner and outer cones
    // inner -&gt; outer, x goes from 0 -&gt; 1
    var x = (absAngle - absInnerAngle) / (absOuterAngle - absInnerAngle);
    gain = (1 - x) + source.coneOuterGain * x;
  }

  return gain;
}  
</pre>
      </section>
    </section>
    <section>
      <h2 id="Performance">
        Performance Considerations
      </h2>
      <section class="informative">
        <h3>
          Latency
        </h3>
        <figure>
          <img alt="latency" src="images/latency.png">
          <figcaption>
            Use cases in which the latency can be important
          </figcaption>
        </figure>
        <p>
          For web applications, the time delay between mouse and keyboard
          events (keydown, mousedown, etc.) and a sound being heard is
          important.
        </p>
        <p>
          This time delay is called latency and is caused by several factors
          (input device latency, internal buffering latency, DSP processing
          latency, output device latency, distance of user's ears from
          speakers, etc.), and is cumulative. The larger this latency is, the
          less satisfying the user's experience is going to be. In the extreme,
          it can make musical production or game-play impossible. At moderate
          levels it can affect timing and give the impression of sounds lagging
          behind or the game being non-responsive. For musical applications the
          timing problems affect rhythm. For gaming, the timing problems affect
          precision of gameplay. For interactive applications, it generally
          cheapens the users experience much in the same way that very low
          animation frame-rates do. Depending on the application, a reasonable
          latency can be from as low as 3-6 milliseconds to 25-50 milliseconds.
        </p>
        <p>
          Implementations will generally seek to minimize overall latency.
        </p>
        <p>
          Along with minimizing overall latency, implementations will generally
          seek to minimize the difference between an
          <a><code>AudioContext</code></a>'s <code>currentTime</code> and an
          <a><code>AudioProcessingEvent</code></a>'s <code>playbackTime</code>.
          Deprecation of <a><code>ScriptProcessorNode</code></a> will make this
          consideration less important over time.
        </p>Additionally, some <a>AudioNode</a>s can add latency to some paths
        of the audio graph, notably:
        <ul>
          <li>The <a>AudioWorker</a> can run a script that buffers internally,
          adding delay to the signal path.
          </li>
          <li>The <a>DelayNode</a>, whose role is to add controlled latency
          time.
          </li>
          <li>The <a>BiquadFilterNode</a> and <a>IIRFilterNode</a> filter
          design can delay incoming samples, as a natural consequence of the
          causal filtering process.
          </li>
          <li>The <a>ConvolverNode</a> depending on the impulse, can delay
          incoming samples, as a natural result of the convolution operation.
          </li>
          <li>The <a>DynamicsCompressorNode</a> has a look ahead algorithm that
          causes delay in the signal path.
          </li>
          <li>The <a>MediaStreamAudioSourceNode</a> and
          <a>MediaStreamAudioDestinationNode</a>, depending on the
          implementation, can add buffers internally that add delays.
          </li>
          <li>The <a>ScriptProcessorNode</a> can have buffers between the
          control thread and the rendering thread.
          </li>
          <li>The <a>WaveShaperNode</a>, when oversampling, and depending on
          the oversampling technique, add delays to the signal path.
          </li>
        </ul>
      </section>
      <section>
        <h3>
          Audio Buffer Copying
        </h3>
        <p>
          When an <a href="#acquire-the-content">acquire the content</a>
          operation is performed on an <a>AudioBuffer</a>, the entire operation
          can usually be implemented without copying channel data. In
          particular, the last step should be performed lazily at the next
          <a href=
          "#widl-AudioBuffer-getChannelData-Float32Array-unsigned-long-channel">
          <code>getChannelData</code></a> call. That means a sequence of
          consecutive <a href="#acquire-the-content">acquire the contents</a>
          operations with no intervening <a href=
          "#widl-AudioBuffer-getChannelData-Float32Array-unsigned-long-channel">
          <code>getChannelData</code></a> (e.g. multiple
          <a><code>AudioBufferSourceNode</code></a>s playing the same
          <a><code>AudioBuffer</code></a>) can be implemented with no
          allocations or copying.
        </p>
        <p>
          Implementations can perform an additional optimization: if <a href=
          "#widl-AudioBuffer-getChannelData-Float32Array-unsigned-long-channel">
          getChannelData</a> is called on an <a>AudioBuffer</a>, fresh
          <code>ArrayBuffer</code>s have not yet been allocated, but all
          invokers of previous <a href="#acquire-the-content">acquire the
          content</a> operations on an <a>AudioBuffer</a> have stopped using
          the <a>AudioBuffer</a>'s data, the raw data buffers can be recycled
          for use with new <a>AudioBuffer</a>s, avoiding any reallocation or
          copying of the channel data.
        </p>
      </section>
      <section class="informative">
        <h3>
          AudioParam Transitions
        </h3>
        <p>
          While no automatic smoothing is done when directly setting the
          <a href="#widl-AudioParam-value"><code>value</code></a> attribute of
          an <a><code>AudioParam</code></a>, for certain parameters, smooth
          transition are preferable to directly setting the value.
        </p>
        <p>
          Using the <a href=
          "#widl-AudioParam-setTargetAtTime-AudioParam-float-target-double-startTime-float-timeConstant">
          <code>setTargetAtTime</code></a> method with a low
          <code>timeConstant</code> allows authors to perform a smooth
          transition.
        </p>
      </section>
      <section>
        <h3>
          Audio Glitching
        </h3>
        <p>
          Audio glitches are caused by an interruption of the normal continuous
          audio stream, resulting in loud clicks and pops. It is considered to
          be a catastrophic failure of a multi-media system and must be
          avoided. It can be caused by problems with the threads responsible
          for delivering the audio stream to the hardware, such as scheduling
          latencies caused by threads not having the proper priority and
          time-constraints. It can also be caused by the audio DSP trying to do
          more work than is possible in real-time given the CPU's speed.
        </p>
      </section>
      <section>
        <h3 id="JavaScriptPerformance">
          JavaScript Issues with Real-Time Processing and Synthesis:
        </h3>While processing audio in JavaScript, it is extremely challenging
        to get reliable, glitch-free audio while achieving a reasonably
        low-latency, especially under heavy processor load.
        <ul>
          <li>JavaScript is very much slower than heavily optimized C++ code
          and is not able to take advantage of SSE optimizations and
          multi-threading which is critical for getting good performance on
          today's processors. Optimized native code can be on the order of
          twenty times faster for processing FFTs as compared with JavaScript.
          It is not efficient enough for heavy-duty processing of audio such as
          convolution and 3D spatialization of large numbers of audio sources.
          </li>
          <li>setInterval() and XHR handling will steal time from the audio
          processing. In a reasonably complex game, some JavaScript resources
          will be needed for game physics and graphics. This creates challenges
          because audio rendering is deadline driven (to avoid glitches and get
          low enough latency).
          </li>
          <li>JavaScript does not run in a real-time processing thread and thus
          can be pre-empted by many other threads running on the system.
          </li>
          <li>Garbage Collection (and autorelease pools on Mac OS X) can cause
          unpredictable delay on a JavaScript thread.
          </li>
          <li>Multiple JavaScript contexts can be running on the main thread,
          stealing time from the context doing the processing.
          </li>
          <li>Other code (other than JavaScript) such as page rendering runs on
          the main thread.
          </li>
          <li>Locks can be taken and memory is allocated on the JavaScript
          thread. This can cause additional thread preemption.
          </li>
        </ul>The problems are even more difficult with today's generation of
        mobile devices which have processors with relatively poor performance
        and power consumption / battery-life issues.
      </section>
    </section>
    <section class="informative">
      <h2 id="Security-Privacy-Considerations">
        Security and Privacy Considerations
      </h2>
      <p>
        The W3C TAG is developing a <a href=
        "https://w3ctag.github.io/security-questionnaire/">Self-Review
        Questionnaire: Security and Privacy</a> for editors of specifications
        to informatively answer.
      </p>
      <p>
        Per the <a href=
        "https://w3ctag.github.io/security-questionnaire/#questions">Questions
        to Consider</a>
      </p>
      <ol class="seclist">
        <li>
          <p>
            Does this specification deal with personally-identifiable
            information?
          </p>
          <!-- assuming the audio context does not expose detailed information 
         on audio device inputs and outputs; revisit this question after the 
         fingerprinting analysis is done -->
          <p>
            No.
          </p>
        </li>
        <li>
          <p>
            Does this specification deal with high-value data?
          </p>
          <p>
            No. Credit card information and the like is not used in Web Audio.
            It is possible to use Web Audio to process or analyze voice data,
            which might be a privacy concern, but access to the user's
            microphone is permission-based via getUserMedia.
          </p>
        </li>
        <li>
          <p>
            Does this specification introduce new state for an origin that
            persists across browsing sessions?
          </p>
          <p>
            No. AudioWorker does not persist across browsing sessions.
            <em>right?</em>
          </p>
        </li>
        <li>
          <p>
            Does this specification expose persistent, cross-origin state to
            the web?
          </p>
          <p>
            <em>Not sure. If audio sample data is loaded cross-origin, it
            exposes state (whether that sample data resolves or not) to the
            script origin.</em>
          </p>
        </li>
        <li>
          <p>
            Does this specification expose any other data to an origin that it
            doesnâ€™t currently have access to?
          </p>
          <p>
            Yes. When giving various information on available
            <a><code>AudioNode</code></a>s, the Web Audio API potentially
            exposes information on characteristic features of the client (such
            as audio hardware sample-rate) to any page that makes use of the
            <a><code>AudioNode</code></a> interface. Additionally, timing
            information can be collected through the
            <a><code>AnalyserNode</code></a> or
            <a><code>ScriptProcessorNode</code></a> interface. The information
            could subsequently be used to create a fingerprint of the client.
          </p>
        </li>
        <li>
          <p>
            Does this specification enable new script execution/loading
            mechanisms?
          </p>
          <p>
            No. However, it does use the worker script execution method,
            defined in that specification.
          </p>
        </li>
        <li>
          <p>
            Does this specification allow an origin access to a userâ€™s
            location?
          </p>
          <p>
            No.
          </p>
        </li>
        <li>
          <p>
            Does this specification allow an origin access to sensors on a
            userâ€™s device?
          </p>
          <p>
            Not directly. Currently audio input is not specified in this
            document, but it will involve gaining access to the client
            machine's audio input or microphone. This will require asking the
            user for permission in an appropriate way, probably via the
            <a href="https://w3c.github.io/mediacapture-main/#dom-mediadevices-getusermedia">
            getUserMedia() API</a>.
          </p>
        </li>
        <li>
          <p>
            Does this specification allow an origin access to aspects of a
            userâ€™s local computing environment?
          </p>
          <p>
            <em>Not sure. Does it allow probing of supported sample rates?
            Supported audio codecs? We should mention denial of service attack
            by consuming CPU cycles.</em>
          </p>
        </li>
        <li>
          <p>
            Does this specification allow an origin access to other devices?
          </p>
          <p>
            No.
          </p>
        </li>
        <li>
          <p>
            Does this specification allow an origin some measure of control
            over a user agentâ€™s native UI?
          </p>
          <p>
            No?. <em>Though it could be used to emulate system sounds to make
            an attack seem more like a local system event?</em>
          </p>
        </li>
        <li>
          <p>
            Does this specification expose temporary identifiers to the web?
          </p>
          <p>
            No.
          </p>
        </li>
        <li>
          <p>
            Does this specification distinguish between behavior in first-party
            and third-party contexts?
          </p>
          <p>
            No.
          </p>
        </li>
        <li>
          <p>
            How should this specification work in the context of a user agentâ€™s
            "incognito" mode?
          </p>
          <p>
            No differently.
          </p>
        </li>
        <li>
          <p>
            Does this specification persist data to a userâ€™s local device?
          </p>
          <p>
            <em>Maybe? Cached impulses or audio sample data stored
            locally?</em>
          </p>
        </li>
        <li>
          <p>
            Does this specification have a "Security Considerations" and
            "Privacy Considerations" section?
          </p>
          <p>
            Yes.
          </p>
        </li>
        <li>
          <p>
            Does this specification allow downgrading default security
            characteristics?
          </p>
          <p>
            No.
          </p>
        </li>
      </ol>
    </section>
    <section>
      <h2 id="requirements">
        Requirements and Use Cases
      </h2>
      <p>
        Please see [[webaudio-usecases]].
      </p>
    </section>
    <section>
      <h2>
        Acknowledgements
      </h2>
      <p>
        This specification is the collective work of the W3C <a href=
        "http://www.w3.org/2011/audio/">Audio Working Group</a>.
      </p>
      <p>
        Members of the Working Group are (at the time of writing, and by
        alphabetical order):<br>
        Adenot, Paul (Mozilla Foundation) - Specification Co-editor; Akhgari,
        Ehsan (Mozilla Foundation); Berkovitz, Joe (Hal Leonard/Noteflight) â€“
        WG Chair; Bossart, Pierre (Intel Corporation); Carlson, Eric (Apple,
        Inc.); Choi, Hongchan (Google, Inc.); Geelnard, Marcus (Opera
        Software); Goode, Adam (Google, Inc.); Gregan, Matthew (Mozilla
        Foundation); Hofmann, Bill (Dolby Laboratories); JÃ¤genstedt, Philip
        (Opera Software); Kalliokoski, Jussi (Invited Expert); Lilley, Chris
        (W3C Staff); Lowis, Chris (Invited Expert. WG co-chair from December
        2012 to September 2013, affiliated with British Broadcasting
        Corporation); Mandyam, Giridhar (Qualcomm Innovation Center, Inc);
        Noble, Jer (Apple, Inc.); O'Callahan, Robert(Mozilla Foundation);
        Onumonu, Anthony (British Broadcasting Corporation); Paradis, Matthew
        (British Broadcasting Corporation); Raman, T.V. (Google, Inc.);
        Schepers, Doug (W3C/MIT); Shires, Glen (Google, Inc.); Smith, Michael
        (W3C/Keio); Thereaux, Olivier (British Broadcasting Corporation); Toy,
        Raymond (Google, Inc.); Verdie, Jean-Charles (MStar Semiconductor,
        Inc.); Wilson, Chris (Google,Inc.) - Specification Co-editor; ZERGAOUI,
        Mohamed (INNOVIMAX)
      </p>
      <p>
        Former members of the Working Group and contributors to the
        specification include:<br>
        Caceres, Marcos (Invited Expert); Cardoso, Gabriel (INRIA); Chen, Bin
        (Baidu, Inc.); MacDonald, Alistair (W3C Invited Experts) â€” WG co-chair
        from March 2011 to July 2012; Michel, Thierry (W3C/ERCIM); Rogers,
        Chris (Google, Inc.) â€“ Specification Editor until August 2013; Wei,
        James (Intel Corporation);
      </p>
    </section>
    <section>
      <h2 id="ChangeLog">
        Web Audio API Change Log
      </h2>
      <p>
        See <a href="changelog.html">changelog.html</a>.
      </p>
    </section>
  </body>
</html>
